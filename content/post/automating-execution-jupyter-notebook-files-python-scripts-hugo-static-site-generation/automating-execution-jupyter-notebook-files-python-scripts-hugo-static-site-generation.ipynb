{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307fa821",
   "metadata": {},
   "source": [
    "# Automating Execution of Jupyter Notebook Files, Python Scripts, and Hugo Static Site Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ce63e",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a6dbc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T20:30:53.865202Z",
     "iopub.status.busy": "2025-06-29T20:30:53.864381Z",
     "iopub.status.idle": "2025-06-29T20:30:54.858891Z",
     "shell.execute_reply": "2025-06-29T20:30:54.858500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter, MultipleLocator\n",
    "\n",
    "# Data Sources\n",
    "import yfinance as yf\n",
    "\n",
    "# Statistical Analysis\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3911f",
   "metadata": {},
   "source": [
    "## Add Directories To Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaed08fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T20:30:54.860744Z",
     "iopub.status.busy": "2025-06-29T20:30:54.860493Z",
     "iopub.status.idle": "2025-06-29T20:30:54.865456Z",
     "shell.execute_reply": "2025-06-29T20:30:54.865120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: /usr/lib/python313.zip\n",
      "1: /usr/lib/python3.13\n",
      "2: /usr/lib/python3.13/lib-dynload\n",
      "3: \n",
      "4: /home/jared/python-virtual-envs/general_313/lib/python3.13/site-packages\n",
      "5: /home/jared/Cloud_Storage/Dropbox/Websites/jaredszajkowski.github.io/src\n",
      "6: /home/jared/Cloud_Storage/Dropbox/Quant_Finance_Research\n",
      "7: /home/jared/Cloud_Storage/Dropbox/Quant_Finance_Research/src\n"
     ]
    }
   ],
   "source": [
    "# Add the source subdirectory to the system path to allow import config from settings.py\n",
    "current_directory = Path(os.getcwd())\n",
    "website_base_directory = current_directory.parent.parent.parent\n",
    "src_directory = website_base_directory / \"src\"\n",
    "sys.path.append(str(src_directory)) if str(src_directory) not in sys.path else None\n",
    "\n",
    "# Import settings.py\n",
    "from settings import config\n",
    "\n",
    "# Add configured directories from config to path\n",
    "SOURCE_DIR = config(\"SOURCE_DIR\")\n",
    "sys.path.append(str(Path(SOURCE_DIR))) if str(Path(SOURCE_DIR)) not in sys.path else None\n",
    "\n",
    "QUANT_FINANCE_RESEARCH_BASE_DIR = config(\"QUANT_FINANCE_RESEARCH_BASE_DIR\")\n",
    "sys.path.append(str(Path(QUANT_FINANCE_RESEARCH_BASE_DIR))) if str(Path(QUANT_FINANCE_RESEARCH_BASE_DIR)) not in sys.path else None\n",
    "\n",
    "QUANT_FINANCE_RESEARCH_SOURCE_DIR = config(\"QUANT_FINANCE_RESEARCH_SOURCE_DIR\")\n",
    "sys.path.append(str(Path(QUANT_FINANCE_RESEARCH_SOURCE_DIR))) if str(Path(QUANT_FINANCE_RESEARCH_SOURCE_DIR)) not in sys.path else None\n",
    "\n",
    "# Add other configured directories\n",
    "BASE_DIR = config(\"BASE_DIR\")\n",
    "CONTENT_DIR = config(\"CONTENT_DIR\")\n",
    "POSTS_DIR = config(\"POSTS_DIR\")\n",
    "PAGES_DIR = config(\"PAGES_DIR\")\n",
    "PUBLIC_DIR = config(\"PUBLIC_DIR\")\n",
    "SOURCE_DIR = config(\"SOURCE_DIR\")\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "DATA_MANUAL_DIR = config(\"DATA_MANUAL_DIR\")\n",
    "\n",
    "# Print system path\n",
    "for i, path in enumerate(sys.path):\n",
    "    print(f\"{i}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc2cb6",
   "metadata": {},
   "source": [
    "## Track Index Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273bf49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T20:30:54.866816Z",
     "iopub.status.busy": "2025-06-29T20:30:54.866576Z",
     "iopub.status.idle": "2025-06-29T20:30:54.871124Z",
     "shell.execute_reply": "2025-06-29T20:30:54.870773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create file to track markdown dependencies\n",
    "dep_file = Path(\"index_dep.txt\")\n",
    "dep_file.write_text(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b2cef",
   "metadata": {},
   "source": [
    "## Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090330c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T20:30:54.872632Z",
     "iopub.status.busy": "2025-06-29T20:30:54.872426Z",
     "iopub.status.idle": "2025-06-29T20:30:54.875104Z",
     "shell.execute_reply": "2025-06-29T20:30:54.874704Z"
    }
   },
   "outputs": [],
   "source": [
    "from export_track_md_deps import export_track_md_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069fc09",
   "metadata": {},
   "source": [
    "## dodo.py Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f6a554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T20:30:54.876848Z",
     "iopub.status.busy": "2025-06-29T20:30:54.876616Z",
     "iopub.status.idle": "2025-06-29T20:30:54.879649Z",
     "shell.execute_reply": "2025-06-29T20:30:54.879211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported and tracked: 01_Imports.md\n"
     ]
    }
   ],
   "source": [
    "# Copy this <!-- INSERT_01_Import_HERE --> to index_temp.md\n",
    "export_track_md_deps(dep_file=dep_file, md_filename=\"01_Imports.md\", content=\n",
    "\"\"\"\n",
    "```python\n",
    "#######################################\n",
    "## Import Libraries\n",
    "#######################################\n",
    "\n",
    "import sys\n",
    "\n",
    "## Make sure the src folder is in the path\n",
    "sys.path.insert(1, \"./src/\")\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "from datetime import datetime\n",
    "from os import environ, getcwd, path\n",
    "from pathlib import Path\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5d3050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T20:30:54.881888Z",
     "iopub.status.busy": "2025-06-29T20:30:54.881405Z",
     "iopub.status.idle": "2025-06-29T20:30:54.885013Z",
     "shell.execute_reply": "2025-06-29T20:30:54.884585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported and tracked: 02_Print_Green.md\n"
     ]
    }
   ],
   "source": [
    "# Copy this <!-- INSERT_02_Print_Green_HERE --> to index_temp.md\n",
    "export_track_md_deps(dep_file=dep_file, md_filename=\"02_Print_Green.md\", content=\n",
    "\"\"\"\n",
    "```python\n",
    "# Code from lines 29-75 referenced from the UChicago\n",
    "# FINM 32900 - Full-Stack Quantitative Finance course\n",
    "# Credit to Jeremy Bejarano\n",
    "# https://github.com/jmbejara\n",
    "\n",
    "## Custom reporter: Print PyDoit Text in Green\n",
    "# This is helpful because some tasks write to sterr and pollute the output in\n",
    "# the console. I don't want to mute this output, because this can sometimes\n",
    "# cause issues when, for example, LaTeX hangs on an error and requires\n",
    "# presses on the keyboard before continuing. However, I want to be able\n",
    "# to easily see the task lines printed by PyDoit. I want them to stand out\n",
    "# from among all the other lines printed to the console.\n",
    "\n",
    "from doit.reporter import ConsoleReporter\n",
    "from settings import config\n",
    "\n",
    "#######################################\n",
    "## Slurm Configuration\n",
    "#######################################\n",
    "\n",
    "try:\n",
    "    in_slurm = environ[\"SLURM_JOB_ID\"] is not None\n",
    "except:\n",
    "    in_slurm = False\n",
    "\n",
    "class GreenReporter(ConsoleReporter):\n",
    "    def write(self, stuff, **kwargs):\n",
    "        doit_mark = stuff.split(\" \")[0].ljust(2)\n",
    "        task = \" \".join(stuff.split(\" \")[1:]).strip() + \"\\n\"\n",
    "        output = (\n",
    "            Fore.GREEN\n",
    "            + doit_mark\n",
    "            + f\" {path.basename(getcwd())}: \"\n",
    "            + task\n",
    "            + Style.RESET_ALL\n",
    "        )\n",
    "        self.outstream.write(output)\n",
    "\n",
    "if not in_slurm:\n",
    "    DOIT_CONFIG = {\n",
    "        \"reporter\": GreenReporter,\n",
    "        # other config here...\n",
    "        # \"cleanforget\": True, # Doit will forget about tasks that have been cleaned.\n",
    "        \"backend\": \"sqlite3\",\n",
    "        \"dep_file\": \"./.doit-db.sqlite\",\n",
    "    }\n",
    "else:\n",
    "    DOIT_CONFIG = {\n",
    "        \"backend\": \"sqlite3\", \n",
    "        \"dep_file\": \"./.doit-db.sqlite\"\n",
    "    }\n",
    "init(autoreset=True)\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fbb2ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T20:30:54.886440Z",
     "iopub.status.busy": "2025-06-29T20:30:54.886296Z",
     "iopub.status.idle": "2025-06-29T20:30:54.889085Z",
     "shell.execute_reply": "2025-06-29T20:30:54.888732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported and tracked: 03_Directory_Variables.md\n"
     ]
    }
   ],
   "source": [
    "# Copy this <!-- INSERT_03_Directory_Variables_HERE --> to index_temp.md\n",
    "export_track_md_deps(dep_file=dep_file, md_filename=\"03_Directory_Variables.md\", content=\n",
    "\"\"\"\n",
    "```python\n",
    "#######################################\n",
    "## Set directory variables\n",
    "#######################################\n",
    "\n",
    "BASE_DIR = config(\"BASE_DIR\")\n",
    "CONTENT_DIR = config(\"CONTENT_DIR\")\n",
    "POSTS_DIR = config(\"POSTS_DIR\")\n",
    "PAGES_DIR = config(\"PAGES_DIR\")\n",
    "PUBLIC_DIR = config(\"PUBLIC_DIR\")\n",
    "SOURCE_DIR = config(\"SOURCE_DIR\")\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "DATA_MANUAL_DIR = config(\"DATA_MANUAL_DIR\")\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c9dfd",
   "metadata": {},
   "source": [
    "## Complete dodo.py File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae66b2",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"Execute with `doit` in the terminal.\"\"\"\n",
    "\n",
    "#######################################\n",
    "## Import Libraries\n",
    "#######################################\n",
    "\n",
    "import sys\n",
    "\n",
    "## Make sure the src folder is in the path\n",
    "sys.path.insert(1, \"./src/\")\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "from datetime import datetime\n",
    "from os import environ, getcwd, path\n",
    "from pathlib import Path\n",
    "\n",
    "# Code from lines 29-75 referenced from the UChicago\n",
    "# FINM 32900 - Full-Stack Quantitative Finance course\n",
    "# Credit to Jeremy Bejarano\n",
    "# https://github.com/jmbejara\n",
    "\n",
    "## Custom reporter: Print PyDoit Text in Green\n",
    "# This is helpful because some tasks write to sterr and pollute the output in\n",
    "# the console. I don't want to mute this output, because this can sometimes\n",
    "# cause issues when, for example, LaTeX hangs on an error and requires\n",
    "# presses on the keyboard before continuing. However, I want to be able\n",
    "# to easily see the task lines printed by PyDoit. I want them to stand out\n",
    "# from among all the other lines printed to the console.\n",
    "\n",
    "from doit.reporter import ConsoleReporter\n",
    "from settings import config\n",
    "\n",
    "#######################################\n",
    "## Slurm Configuration\n",
    "#######################################\n",
    "\n",
    "try:\n",
    "    in_slurm = environ[\"SLURM_JOB_ID\"] is not None\n",
    "except:\n",
    "    in_slurm = False\n",
    "\n",
    "class GreenReporter(ConsoleReporter):\n",
    "    def write(self, stuff, **kwargs):\n",
    "        doit_mark = stuff.split(\" \")[0].ljust(2)\n",
    "        task = \" \".join(stuff.split(\" \")[1:]).strip() + \"\\n\"\n",
    "        output = (\n",
    "            Fore.GREEN\n",
    "            + doit_mark\n",
    "            + f\" {path.basename(getcwd())}: \"\n",
    "            + task\n",
    "            + Style.RESET_ALL\n",
    "        )\n",
    "        self.outstream.write(output)\n",
    "\n",
    "if not in_slurm:\n",
    "    DOIT_CONFIG = {\n",
    "        \"reporter\": GreenReporter,\n",
    "        # other config here...\n",
    "        # \"cleanforget\": True, # Doit will forget about tasks that have been cleaned.\n",
    "        \"backend\": \"sqlite3\",\n",
    "        \"dep_file\": \"./.doit-db.sqlite\",\n",
    "    }\n",
    "else:\n",
    "    DOIT_CONFIG = {\n",
    "        \"backend\": \"sqlite3\", \n",
    "        \"dep_file\": \"./.doit-db.sqlite\"\n",
    "    }\n",
    "init(autoreset=True)\n",
    "\n",
    "#######################################\n",
    "## Set directory variables\n",
    "#######################################\n",
    "\n",
    "BASE_DIR = config(\"BASE_DIR\")\n",
    "CONTENT_DIR = config(\"CONTENT_DIR\")\n",
    "POSTS_DIR = config(\"POSTS_DIR\")\n",
    "PAGES_DIR = config(\"PAGES_DIR\")\n",
    "PUBLIC_DIR = config(\"PUBLIC_DIR\")\n",
    "SOURCE_DIR = config(\"SOURCE_DIR\")\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "DATA_MANUAL_DIR = config(\"DATA_MANUAL_DIR\")\n",
    "\n",
    "#######################################\n",
    "## Helper functions\n",
    "#######################################\n",
    "\n",
    "def copy_file(origin_path, destination_path, mkdir=True):\n",
    "    \"\"\"Create a Python action for copying a file.\"\"\"\n",
    "\n",
    "    def _copy_file():\n",
    "        origin = Path(origin_path)\n",
    "        dest = Path(destination_path)\n",
    "        if mkdir:\n",
    "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(origin, dest)\n",
    "\n",
    "    return _copy_file\n",
    "\n",
    "def extract_front_matter(index_path):\n",
    "    \"\"\"Extract front matter as a dict from a Hugo index.md file.\"\"\"\n",
    "    text = index_path.read_text()\n",
    "    match = re.search(r\"(?s)^---(.*?)---\", text)\n",
    "    if match:\n",
    "        return yaml.safe_load(match.group(1))\n",
    "    return {}\n",
    "\n",
    "def notebook_source_hash(notebook_path):\n",
    "    \"\"\"Compute a SHA-256 hash of the notebook's code and markdown cells. This includes all whitespace and comments.\"\"\"\n",
    "    import nbformat\n",
    "    import hashlib\n",
    "\n",
    "    with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    relevant_cells = [\n",
    "        cell[\"source\"]\n",
    "        for cell in nb.cells\n",
    "        if cell.cell_type in {\"code\", \"markdown\"}\n",
    "    ]\n",
    "    full_content = \"\\n\".join(relevant_cells)\n",
    "    return hashlib.sha256(full_content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def clean_pdf_export_pngs(subdir, notebook_name):\n",
    "    \"\"\"Remove .png files created by nbconvert during PDF export.\"\"\"\n",
    "    pattern = f\"{notebook_name}_*_*.png\"\n",
    "    deleted = False\n",
    "    for file in subdir.glob(pattern):\n",
    "        print(f\"🧹 Removing nbconvert temp image: {file}\")\n",
    "        file.unlink()\n",
    "        deleted = True\n",
    "    if not deleted:\n",
    "        print(f\"✅ No temp PNGs to remove for {notebook_name}\")\n",
    "\n",
    "#######################################\n",
    "## PyDoit tasks\n",
    "#######################################\n",
    "\n",
    "def task_config():\n",
    "    \"\"\"Create empty directories for content, page, post, and public if they don't exist\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"ipython ./src/settings.py\"],\n",
    "        \"file_dep\": [\"./src/settings.py\"],\n",
    "        \"targets\": [CONTENT_DIR, PAGES_DIR, POSTS_DIR, PUBLIC_DIR],\n",
    "        \"verbosity\": 2,\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n",
    "\n",
    "def task_list_posts_subdirs():\n",
    "    \"\"\"Create a list of the subdirectories of the posts directory\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"python ./src/list_posts_subdirs.py\"],\n",
    "        \"file_dep\": [\"./src/settings.py\"],\n",
    "        # \"targets\": [POSTS_DIR],\n",
    "        \"verbosity\": 2,\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n",
    "\n",
    "def task_run_post_notebooks():\n",
    "    \"\"\"Execute notebooks that match their subdirectory names and only when code or markdown content has changed\"\"\"\n",
    "    for subdir in POSTS_DIR.iterdir():\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "\n",
    "        notebook_path = subdir / f\"{subdir.name}.ipynb\"\n",
    "        if not notebook_path.exists():\n",
    "            continue  # ✅ Skip subdirs with no matching notebook\n",
    "\n",
    "        hash_file = subdir / f\"{subdir.name}.last_source_hash\"\n",
    "        log_file = subdir / f\"{subdir.name}.log\"\n",
    "        \n",
    "        def source_has_changed(path=notebook_path, hash_path=hash_file, log_path=log_file):\n",
    "            current_hash = notebook_source_hash(path)\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            if hash_path.exists():\n",
    "                old_hash = hash_path.read_text().strip()\n",
    "                if current_hash != old_hash:\n",
    "                    print(f\"🔁 Change detected in {path.name}\")\n",
    "                    return False  # needs re-run\n",
    "\n",
    "                # ✅ No change → log as skipped\n",
    "                with log_path.open(\"a\") as log:\n",
    "                    log.write(f\"[{timestamp}] ⏩ Skipped (no changes): {path.name}\\n\")\n",
    "                print(f\"⏩ No change in hash for {path.name}\")\n",
    "                return True\n",
    "\n",
    "            # 🆕 No previous hash → must run\n",
    "            print(f\"🆕 No previous hash found for {path.name}\")\n",
    "            return False\n",
    "        \n",
    "        def run_and_log(path=notebook_path, hash_path=hash_file, log_path=log_file):\n",
    "            start_time = time.time()\n",
    "            subprocess.run([\n",
    "                \"jupyter\", \"nbconvert\",\n",
    "                \"--execute\",\n",
    "                \"--to\", \"notebook\",\n",
    "                \"--inplace\",\n",
    "                \"--log-level=ERROR\",\n",
    "                str(path)\n",
    "            ], check=True)\n",
    "            elapsed = round(time.time() - start_time, 2)\n",
    "\n",
    "            new_hash = notebook_source_hash(path)\n",
    "            hash_path.write_text(new_hash)\n",
    "            print(f\"✅ Saved new hash for {path.name}\")\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            log_msg = f\"[{timestamp}] ✅ Executed {path.name} in {elapsed}s\\n\"\n",
    "            with log_path.open(\"a\") as f:\n",
    "                f.write(log_msg)\n",
    "\n",
    "            print(log_msg.strip())\n",
    "\n",
    "        yield {\n",
    "            \"name\": subdir.name,\n",
    "            \"actions\": [run_and_log],\n",
    "            \"file_dep\": [notebook_path],\n",
    "            \"uptodate\": [source_has_changed],\n",
    "            \"verbosity\": 2,\n",
    "        }\n",
    "\n",
    "def task_export_post_notebooks():\n",
    "    \"\"\"Export executed notebooks to HTML and PDF, and clean temp PNGs\"\"\"\n",
    "    for subdir in POSTS_DIR.iterdir():\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "\n",
    "        notebook_name = subdir.name\n",
    "        notebook_path = subdir / f\"{notebook_name}.ipynb\"\n",
    "        html_output = subdir / f\"{notebook_name}.html\"\n",
    "        pdf_output = subdir / f\"{notebook_name}.pdf\"\n",
    "\n",
    "        if not notebook_path.exists():\n",
    "            continue\n",
    "\n",
    "        yield {\n",
    "            \"name\": notebook_name,\n",
    "            \"actions\": [\n",
    "                f\"jupyter nbconvert --to=html --log-level=WARN --output={html_output} {notebook_path}\",\n",
    "                f\"jupyter nbconvert --to=pdf --log-level=WARN --output={pdf_output} {notebook_path}\",\n",
    "                (clean_pdf_export_pngs, [subdir, notebook_name])\n",
    "            ],\n",
    "            \"file_dep\": [notebook_path],\n",
    "            \"targets\": [html_output, pdf_output],\n",
    "            \"verbosity\": 2,\n",
    "            \"clean\": [],  # Don't clean these files by default.\n",
    "        }\n",
    "\n",
    "def task_build_post_indices():\n",
    "    \"\"\"Run build_index.py in each post subdirectory to generate index.md\"\"\"\n",
    "    script_path = SOURCE_DIR / \"build_index.py\"\n",
    "\n",
    "    for subdir in POSTS_DIR.iterdir():\n",
    "        if subdir.is_dir() and (subdir / \"index_temp.md\").exists():\n",
    "            def run_script(subdir=subdir):\n",
    "                subprocess.run(\n",
    "                    [\"python\", str(script_path)],\n",
    "                    cwd=subdir,\n",
    "                    check=True\n",
    "                )\n",
    "\n",
    "            yield {\n",
    "                \"name\": subdir.name,\n",
    "                \"actions\": [run_script],\n",
    "                \"file_dep\": [\n",
    "                    subdir / \"index_temp.md\",\n",
    "                    subdir / \"index_dep.txt\",\n",
    "                    script_path,\n",
    "                ],\n",
    "                \"targets\": [subdir / \"index.md\"],\n",
    "                \"verbosity\": 2,\n",
    "                \"clean\": [],  # Don't clean these files by default.\n",
    "            }\n",
    "\n",
    "def task_clean_public():\n",
    "    \"\"\"Remove the Hugo public directory before rebuilding the site.\"\"\"\n",
    "    def remove_public():\n",
    "        if PUBLIC_DIR.exists():\n",
    "            shutil.rmtree(PUBLIC_DIR)\n",
    "            print(f\"🧹 Deleted {PUBLIC_DIR}\")\n",
    "        else:\n",
    "            print(f\"ℹ️  {PUBLIC_DIR} does not exist, nothing to delete.\")\n",
    "    return {\n",
    "        \"actions\": [remove_public],\n",
    "        \"verbosity\": 2,\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n",
    "\n",
    "def task_build_site():\n",
    "    \"\"\"Build the Hugo static site\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"hugo\"],\n",
    "        \"task_dep\": [\"clean_public\"],\n",
    "        \"verbosity\": 2,\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n",
    "\n",
    "def task_copy_notebook_exports():\n",
    "    \"\"\"Copy notebook HTML exports into the correct Hugo public/ date-based folders\"\"\"\n",
    "    for subdir in POSTS_DIR.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            html_file = subdir / f\"{subdir.name}.html\"\n",
    "            index_md = subdir / \"index.md\"\n",
    "\n",
    "            if not html_file.exists() or not index_md.exists():\n",
    "                continue\n",
    "\n",
    "            # Extract slug and date from front matter\n",
    "            front_matter = extract_front_matter(index_md)\n",
    "            slug = front_matter.get(\"slug\", subdir.name)\n",
    "            date_str = front_matter.get(\"date\")\n",
    "            if not date_str:\n",
    "                continue\n",
    "\n",
    "            # Format path like: public/YYYY/MM/DD/slug/\n",
    "            date_obj = datetime.fromisoformat(date_str)\n",
    "            public_path = PUBLIC_DIR / f\"{date_obj:%Y/%m/%d}\" / slug\n",
    "            target_path = public_path / f\"{slug}.html\"\n",
    "\n",
    "            def copy_html(src=html_file, dest=target_path):\n",
    "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(src, dest)\n",
    "                print(f\"✅ Copied {src} → {dest}\")\n",
    "\n",
    "            yield {\n",
    "                \"name\": subdir.name,\n",
    "                \"actions\": [copy_html],\n",
    "                \"file_dep\": [html_file, index_md],\n",
    "                \"targets\": [target_path],\n",
    "                \"task_dep\": [\"build_site\"],\n",
    "                \"verbosity\": 2,\n",
    "                \"clean\": [],  # Don't clean these files by default.\n",
    "            }\n",
    "\n",
    "def task_create_schwab_callback():\n",
    "    \"\"\"Create a Schwab callback URL by creating /public/schwab_callback/index.html and placing the html code in it\"\"\"\n",
    "    def create_callback():\n",
    "        callback_path = PUBLIC_DIR / \"schwab_callback\" / \"index.html\"\n",
    "        callback_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        html = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <title>Schwab OAuth Code</title>\n",
    "    <script>\n",
    "        const params = new URLSearchParams(window.location.search);\n",
    "        const code = params.get(\"code\");\n",
    "        document.write(\"<h1>Authorization Code:</h1><p>\" + code + \"</p>\");\n",
    "    </script>\n",
    "</head>\n",
    "<body></body>\n",
    "</html>\"\"\"\n",
    "        with open(callback_path, \"w\") as f:\n",
    "            f.write(html)\n",
    "        print(f\"✅ Created Schwab callback page at {callback_path}\")\n",
    "\n",
    "    return {\n",
    "        \"actions\": [create_callback],\n",
    "        \"task_dep\": [\"copy_notebook_exports\", \"clean_public\"],\n",
    "        \"verbosity\": 2,\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n",
    "\n",
    "def task_deploy_site():\n",
    "    \"\"\"Prompt for a commit message and push to GitHub\"\"\"\n",
    "    def commit_and_push():\n",
    "        message = input(\"What is the commit message? \")\n",
    "        if not message.strip():\n",
    "            print(\"❌ Commit message cannot be empty.\")\n",
    "            return 1  # signal failure\n",
    "        import subprocess\n",
    "        subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
    "        subprocess.run([\"git\", \"commit\", \"-am\", message], check=True)\n",
    "        subprocess.run([\"git\", \"push\"], check=True)\n",
    "        print(\"✅ Pushed to GitHub.\")\n",
    "\n",
    "    return {\n",
    "        \"actions\": [commit_and_push],\n",
    "        \"task_dep\": [\"create_schwab_callback\"],\n",
    "        \"verbosity\": 2,\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n",
    "\n",
    "# def task_build_all():\n",
    "#     return {\n",
    "#         \"actions\": None,\n",
    "#         \"task_dep\": [\n",
    "#             \"run_post_notebooks\",\n",
    "#             \"export_post_notebooks\",\n",
    "#             \"build_post_indices\",\n",
    "#             \"clean_public\",\n",
    "#             \"build_site\",\n",
    "#             \"copy_notebook_exports\",\n",
    "#             \"create_schwab_callback\",\n",
    "#             \"deploy_site\",\n",
    "#         ]\n",
    "#     }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c99458",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
