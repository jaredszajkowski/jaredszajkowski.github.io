[{"content":"Introduction From the CBOE VIX website:\n\u0026ldquo;Cboe Global Markets revolutionized investing with the creation of the Cboe Volatility Index® (VIX® Index), the first benchmark index to measure the market’s expectation of future volatility. The VIX Index is based on options of the S\u0026amp;P 500® Index, considered the leading indicator of the broad U.S. stock market. The VIX Index is recognized as the world’s premier gauge of U.S. equity market volatility.\u0026rdquo;\nIn this tutorial, we will investigate finding a signal to use as a basis to trade the VIX.\nVIX Data I don\u0026rsquo;t have access to data for the VIX through Nasdaq Data Link, but for our purposes Yahoo Finance is sufficient.\nUsing the yfinance python module, we can pull what we need and quicky dump it to excel to retain it for future use.\nPython Functions First, a couple of useful functions:\nPull Data From Yahoo Finance Here\u0026rsquo;s the code for the function to pull the data and dump to Excel:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # This function pulls data from Yahoo finance def yf_data_updater(fund): # Download data from YF df_comp = yf.download(fund) # Drop the column level with the ticker symbol df_comp.columns = df_comp.columns.droplevel(1) # Reset index df_comp = df_comp.reset_index() # Remove the \u0026#34;Price\u0026#34; header from the index df_comp.columns.name = None # Reset date column df_comp[\u0026#39;Date\u0026#39;] = df_comp[\u0026#39;Date\u0026#39;].dt.tz_localize(None) # Set \u0026#39;Date\u0026#39; column as index df_comp = df_comp.set_index(\u0026#39;Date\u0026#39;, drop=True) # Drop data from last day because it\u0026#39;s not accrate until end of day df_comp = df_comp.drop(df_comp.index[-1]) # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df_comp.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) print(f\u0026#34;The first and last date of data for {fund} is: \u0026#34;) print(df_comp[:1]) print(df_comp[-1:]) print(f\u0026#34;Data updater complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Set Number Of Decimal Places 1 2 3 # Set number of decimal places in pandas def dp(decimal_places): pd.set_option(\u0026#39;display.float_format\u0026#39;, lambda x: f\u0026#39;%.{decimal_places}f\u0026#39; % x) Import Data From CSV / XLSX 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def load_data(file): # Import CSV try: df = pd.read_csv(file) except: pass # Import excel try: df = pd.read_excel(file, sheet_name=\u0026#39;data\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) except: pass return df Return Information About A Dataframe 1 2 3 4 5 6 7 8 9 # The `df_info` function returns some useful information about a dataframe, such as the columns, data types, and size. def df_info(df): print(\u0026#39;There are \u0026#39;, df.shape[0], \u0026#39; rows and \u0026#39;, df.shape[1], \u0026#39; columns\u0026#39;) print(\u0026#39;The columns and data types are:\u0026#39;) print(df.dtypes) print(\u0026#39;The first 4 rows are:\u0026#39;) display(df.head(4)) print(\u0026#39;The last 4 rows are:\u0026#39;) display(df.tail(4)) Data Overview Acquire Data First, let\u0026rsquo;s get the data:\n1 yf_data_updater(\u0026#39;^VIX\u0026#39;) Load Data Then set our decimal places to something reasonable (like 2):\n1 dp(2) Now that we have the data, let\u0026rsquo;s load it up and take a look.\n1 2 3 4 5 6 7 8 9 10 11 # VIX vix = load_data(\u0026#39;^VIX.xlsx\u0026#39;) # Set \u0026#39;Date\u0026#39; column as datetime vix[\u0026#39;Date\u0026#39;] = pd.to_datetime(vix[\u0026#39;Date\u0026#39;]) # Drop \u0026#39;Volume\u0026#39; vix.drop(columns = {\u0026#39;Volume\u0026#39;}, inplace = True) # Set Date as index vix.set_index(\u0026#39;Date\u0026#39;, inplace = True) Check For Missing Values \u0026amp; Forward Fill Any Missing Values 1 2 3 4 5 # Check to see if there are any NaN values vix[vix[\u0026#39;High\u0026#39;].isna()] # Forward fill to clean up missing data vix[\u0026#39;High\u0026#39;] = vix[\u0026#39;High\u0026#39;].ffill() DataFrame Info Now, running:\n1 df_info(vix) Gives us the following:\nInteresting Statistics Some interesting statistics jump out at use when we look at the mean, standard deviation, min, and max values:\n1 2 3 4 5 6 7 8 9 10 11 12 13 vix_stats = vix.describe() vix_stats.loc[\u0026#39;mean + 1 std\u0026#39;] = {\u0026#39;Open\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Open\u0026#39;] + vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Open\u0026#39;], \u0026#39;High\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;High\u0026#39;] + vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;High\u0026#39;], \u0026#39;Low\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Low\u0026#39;] + vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Low\u0026#39;], \u0026#39;Close\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Close\u0026#39;] + vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Close\u0026#39;]} vix_stats.loc[\u0026#39;mean + 2 std\u0026#39;] = {\u0026#39;Open\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Open\u0026#39;] + 2 * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Open\u0026#39;], \u0026#39;High\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;High\u0026#39;] + 2 * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;High\u0026#39;], \u0026#39;Low\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Low\u0026#39;] + 2 * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Low\u0026#39;], \u0026#39;Close\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Close\u0026#39;] + 2 * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Close\u0026#39;]} vix_stats.loc[\u0026#39;mean - 1 std\u0026#39;] = {\u0026#39;Open\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Open\u0026#39;] - vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Open\u0026#39;], \u0026#39;High\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;High\u0026#39;] - vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;High\u0026#39;], \u0026#39;Low\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Low\u0026#39;] - vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Low\u0026#39;], \u0026#39;Close\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Close\u0026#39;] - vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Close\u0026#39;]} And the levels for each decile:\n1 2 deciles = vix.quantile(np.arange(0, 1.1, 0.1)) display(deciles) A quick histogram gives us the distribution for the entire dataset:\nNow, let\u0026rsquo;s add the levels for the mean, mean + 1 standard deviation, mean - 1 standard deviation, and mean + 2 standard deviations:\nHistorical VIX Plot Here\u0026rsquo;s two plots for the dataset. The first covers 1990 - 2009, and the second 2010 - 2024. This is the daily high level.\nFrom this plot, we can see the following:\nThe VIX has really only jumped above 50 several times (GFC, COVID, recently in August of 2024) The highest levels (\u0026gt; 80) occured only during the GFC \u0026amp; COVID Interestingly, the VIX did not ever get above 50 during the .com bubble Investigating A Signal Next, we will consider the idea of a spike level in the VIX and how we might use a spike level to generate a signal. These elevated levels usually occur during market sell-off events or longer term drawdowns in the S\u0026amp;P 500. Sometimes the VIX reverts to recent levels after a spike, but other times levels remain elevated for weeks or even months.\nSpike Level We will start the 10 day simple moving average (SMA) of the daily high level to get an idea of what is happening recently with the VIX. We\u0026rsquo;ll then pick an arbitrary spike level (25% above the 10 day SMA), and our signal is generated if the VIX hits a level that is above the spike threshold.\nThe idea is that the 10 day SMA will smooth out the recent short term volatility in the VIX, and any gradual increases in the VIX are not interpreted as spike events.\nWe also will generate the 20 and 50 day SMAs for reference, and again to see what is happening with the level of the VIX over slightly longer timeframes.\nHere\u0026rsquo;s the code for the above:\n1 2 3 4 5 6 7 spike_level = 1.25 vix[\u0026#39;SMA_10\u0026#39;] = vix[\u0026#39;High\u0026#39;].rolling(10).mean() vix[\u0026#39;SMA_10_Shift\u0026#39;] = vix[\u0026#39;SMA_10\u0026#39;].shift(1) vix[\u0026#39;Spike_Level\u0026#39;] = vix[\u0026#39;SMA_10_Shift\u0026#39;] * spike_level vix[\u0026#39;Spike\u0026#39;] = vix[\u0026#39;High\u0026#39;] \u0026gt;= vix[\u0026#39;Spike_Level\u0026#39;] vix[\u0026#39;SMA_20\u0026#39;] = vix[\u0026#39;High\u0026#39;].rolling(20).mean() vix[\u0026#39;SMA_50\u0026#39;] = vix[\u0026#39;High\u0026#39;].rolling(50).mean() Now, let\u0026rsquo;s look at the first\nReferences https://www.cboe.com/tradable_products/vix/ https://github.com/ranaroussi/yfinance\nCode The jupyter notebook with the functions and all other code is available here.\n","date":"2025-01-07T00:00:01Z","image":"https://www.jaredszajkowski.com/2025/01/07/investigating-a-vix-trading-signal/cover.jpg","permalink":"https://www.jaredszajkowski.com/2025/01/07/investigating-a-vix-trading-signal/","title":"Investigating A VIX Trading Signal"},{"content":"Python Module Management As an Arch Linux user, the push is to utilize pacman and related tools to manage dependencies and package updates (including Python modules). In fact, the wiki itself explicitly states this (see 2.1), and the default Arch installation of Python disables python-pip.\nUnfortunately, there are limited resources put into maintaining packages for modules and only the most common and popular modules are maintained, and they are updated promptly as is consistent within the Arch ecosystem.\nCreating A Virtual Environment After recently delving into crypto and the web3 Python module, the Coinbase API, and others, I\u0026rsquo;ve found the need to install Python modules from Pypi, the Python package index. This is the most exhaustive location to find modules, including the latest updates and version history.\nUsing python-pip necessitated the use of virtual environments, which made me reconsider the idea of not maintaining Python modules (or maintaining very few) through pacman at all.\nI chose to place the virtual environments at ~/python-virtual-envs/ and within that directory have one called general and other called wrds. The wrds environment is specific to the Wharton Research Data Services which requires (for some reason) an older package of nympy.\nThe \u0026ldquo;general\u0026rdquo; environment covers everything else. I created it with the usual command:\n$ python -m venv ~/python-virtual-envs/general Once created, it can be activated (either in a terminal or an IDE such as VS Code) by executing the following in the terminal:\n$ source ~/python-virtual-envs/general/bin/activate Using python-pip After the virtual environment is created and activated, modules can be installed by using python-pip, such as:\n$ pip install \u0026lt;package-name\u0026gt; If you want to view all installed modules, run:\n$ pip list Or the outdated modules:\n$ pip list --outdated And updated at a later point in time with:\n$ pip install --upgrade \u0026lt;package-name\u0026gt; Maintaining Across Multiple Systems To avoid having to redundantly install modules on different systems, after I make a change to the virtual environment I zip the entire ~/python-virtual-envs/ directory and upload the zip file to Dropbox. This takes less than a minute, and if I am working on a different system can simply extract the archive and have a completely up-to-date and current virtual environment to work in.\nReferences https://docs.python.org/3/library/venv.html https://pypi.org/ https://note.nkmk.me/en/python-pip-usage/ https://wiki.archlinux.org/title/Python\n","date":"2024-12-02T00:00:01Z","image":"https://www.jaredszajkowski.com/2024/12/02/using-python-virtual-environments/cover.jpg","permalink":"https://www.jaredszajkowski.com/2024/12/02/using-python-virtual-environments/","title":"Using Python Virtual Environments"},{"content":"Post Updates Update 12/5/2024: Updated code for summary stats function, various code comments, and corrected grammatical errors.\nIntroduction Harry Browne was an influencial politician, financial advisor, and author who lived from 1933 to 2006 and published 12 books. Wikipedia has an in-depth biography on him.\nWithin the world of finance and investing, one of his best known works is Fail-Safe Investing: Lifelong Financial Security in 30 Minutes. In it, he introduces the idea of the \u0026ldquo;Permanent Portfolio\u0026rdquo;, an investment strategy that uses only four assets and is very simple to implement.\nIn this post, we will investigate Browne\u0026rsquo;s suggested portfolio, including performance across various market cycles and economic regimes.\nBrowne\u0026rsquo;s Portfolio Requirements In Fail-Safe Investing, under rule #11, Browne lays out the requirements for a \u0026ldquo;bulletproof portfolio\u0026rdquo; that will \u0026ldquo;assure that your wealth will survive any event - including events that would be devastating to any one investment. In other words, this portfolio should protect you no matter what the future brings.\u0026rdquo;\nHis requirements for the portfolio consist of the followng:\nSafety: Protection again any economic future, including \u0026ldquo;inflation, recession, or even depression\u0026rdquo; Stability: Performance should be consistent so that you will not need to make any changes and will not experience significant drawdowns Simplicity: Easy to implement and take very little time to maintain He then describes the four \u0026ldquo;broad movements\u0026rdquo; of the economy:\nProsperity: The economy is growing, business is doing well, interest rates are usually low Inflation: The cost of goods and services is rising Tight money or recession: The money supply is shrinking, economic activity is slowing Deflation: Prices are declining and the value of money is increasing The Permanent Portfolio Browne then matches an asset class to each of the economic conditions above:\nProsperity -\u0026gt; Stocks (due to prosperity) and long term bonds (when interest rates fall) Inflation -\u0026gt; Gold Deflation -\u0026gt; Long term bonds (when interest rates fall) Tight money -\u0026gt; Cash He completes the Permanent Portfolio by stipulating the following:\nStart with a base allocation of 25% to each of the asset classes (stocks, bonds, gold, cash) Rebalance back to the base allocation annually, or when \u0026ldquo;any of the four investments has become worth less than 15%, or more than 35%, of the portfolio\u0026rsquo;s overall value\u0026rdquo;Note: Browne does not specify when the portfolio should be rebalanced; therefore, we will make an assumption of a January 1st rebalance. Data For this exercise, we will use the following asset classes:\nStocks: S\u0026amp;P 500 (SPXT_S\u0026amp;P 500 Total Return Index) Bonds: 10 Year US Treasuries (SPBDU10T_S\u0026amp;P US Treasury Bond 7-10 Year Total Return Index) Gold: Gold Spot Price (XAU_Gold USD Spot) Cash: USD With the exception of cash, all data is sourced from Bloomberg.\nWe could use ETFs, but the available price history for the ETFs is much shorter than the indices above. If we wanted to use ETFs, the following would work:\nStocks: IVV - iShares Core S\u0026amp;P 500 ETF Bonds: IEF - iShares 7-10 Year Treasury Bond ETF Gold: GLD - SPDR Gold Shares ETF Cash: USD Python Functions First, a couple of useful python functions to help with the analysis.\nClean Bloomberg Data Export This is discussed here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # This function takes an excel export from Bloomberg and # removes all excess data leaving date and close columns # Imports import pandas as pd # Function definition def bb_data_updater(fund): # File name variable file = fund + \u0026#34;.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) # Export data to excel file = fund + \u0026#34;_Clean.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Set Number Of Decimal Places 1 2 3 4 # Set number of decimal places in pandas def dp(decimal_places): pd.set_option(\u0026#39;display.float_format\u0026#39;, lambda x: f\u0026#39;%.{decimal_places}f\u0026#39; % x) Return Information About A Dataframe 1 2 3 4 5 6 7 8 9 10 11 # The `df_info` function returns some useful information about # a dataframe, such as the columns, data types, and size. def df_info(df): print(\u0026#39;There are \u0026#39;, df.shape[0], \u0026#39; rows and \u0026#39;, df.shape[1], \u0026#39; columns\u0026#39;) print(\u0026#39;The columns and data types are:\u0026#39;) print(df.dtypes) print(\u0026#39;The first 4 rows are:\u0026#39;) display(df.head(4)) print(\u0026#39;The last 4 rows are:\u0026#39;) display(df.tail(4)) Import Data From CSV / XLSX 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def load_data(file): # Import CSV try: df = pd.read_csv(file) except: pass # Import excel try: df = pd.read_excel(file, sheet_name=\u0026#39;data\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) except: pass return df Portfolio Strategy This is the function that executes the strategy. The function takes in the following variables and produces a dataframe with the results:\nfund_list: This is a list of the funds (in this case asset classes) to be used starting_cash: Starting capital amount for the strategy cash_contrib: Daily cash contribution close_prices_df; Dataframe with close prices for each asset class rebal_month: Month that the annual rebalancing should take place rebal_day: Day of the month that the annual rebalancing should take place 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def strategy(fund_list, starting_cash, cash_contrib, close_prices_df, rebal_month, rebal_day, rebal_per_high, rebal_per_low): num_funds = len(fund_list) df = close_prices_df.copy() df.reset_index(inplace = True) # Date to be used for annual rebalance target_month = rebal_month target_day = rebal_day # Create a dataframe with dates from the specific month rebal_date = df[df[\u0026#39;Date\u0026#39;].dt.month == target_month] # Specify the date or the next closest rebal_date = rebal_date[rebal_date[\u0026#39;Date\u0026#39;].dt.day \u0026gt;= target_day] # Group by year and take the first entry for each year rebal_dates_by_year = rebal_date.groupby(rebal_date[\u0026#39;Date\u0026#39;].dt.year).first().reset_index(drop=True) \u0026#39;\u0026#39;\u0026#39; Column order for the dataframe: df[fund + \u0026#34;_BA_Shares\u0026#34;] df[fund + \u0026#34;_BA_$_Invested\u0026#34;] df[fund + \u0026#34;_BA_Port_%\u0026#34;] df[\u0026#39;Total_BA_$_Invested\u0026#39;] df[\u0026#39;Contribution\u0026#39;] df[\u0026#39;Rebalance\u0026#39;] df[fund + \u0026#34;_AA_Shares\u0026#34;] df[fund + \u0026#34;_AA_$_Invested\u0026#34;] df[fund + \u0026#34;_AA_Port_%\u0026#34;] df[\u0026#39;Total_AA_$_Invested\u0026#39;] \u0026#39;\u0026#39;\u0026#39; # Calculate the columns and initial values for before action (BA) shares, $ invested, and port % for fund in fund_list: df[fund + \u0026#34;_BA_Shares\u0026#34;] = starting_cash / num_funds / df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_BA_$_Invested\u0026#34;] = df[fund + \u0026#34;_BA_Shares\u0026#34;] * df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_BA_Port_%\u0026#34;] = 0.25 # Set column values initially df[\u0026#39;Total_BA_$_Invested\u0026#39;] = starting_cash df[\u0026#39;Contribution\u0026#39;] = 0 # df[\u0026#39;Contribution\u0026#39;] = cash_contrib df[\u0026#39;Rebalance\u0026#39;] = \u0026#34;No\u0026#34; # Set columns and values initially for after action (AA) shares, $ invested, and port % for fund in fund_list: df[fund + \u0026#34;_AA_Shares\u0026#34;] = starting_cash / num_funds / df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_AA_$_Invested\u0026#34;] = df[fund + \u0026#34;_AA_Shares\u0026#34;] * df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_AA_Port_%\u0026#34;] = 0.25 # Set column value for after action (AA) total $ invested df[\u0026#39;Total_AA_$_Invested\u0026#39;] = starting_cash # Iterate through the dataframe and execute the strategy for index, row in df.iterrows(): # Ensure there\u0026#39;s a previous row to reference by checking the index value if index \u0026gt; 0: # Initialize variable Total_BA_Invested = 0 # Calculate before action (BA) shares and $ invested values for fund in fund_list: df.at[index, fund + \u0026#34;_BA_Shares\u0026#34;] = df.at[index - 1, fund + \u0026#34;_AA_Shares\u0026#34;] df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] = df.at[index, fund + \u0026#34;_BA_Shares\u0026#34;] * row[fund + \u0026#34;_Close\u0026#34;] # Sum the asset values to find the total Total_BA_Invested = Total_BA_Invested + df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] # Calculate before action (BA) port % values for fund in fund_list: df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] = df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] / Total_BA_Invested # Set column for before action (BA) total $ invested df.at[index, \u0026#39;Total_BA_$_Invested\u0026#39;] = Total_BA_Invested # Initialize variables rebalance = \u0026#34;No\u0026#34; date = row[\u0026#39;Date\u0026#39;] # Check for a specific date annually # Simple if statement to check if date_to_check is in jan_28_or_after_each_year if date in rebal_dates_by_year[\u0026#39;Date\u0026#39;].values: rebalance = \u0026#34;Yes\u0026#34; else: pass # Check to see if any asset has portfolio percentage of greater than 35% or less than 15% and if so set variable for fund in fund_list: if df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] \u0026gt; rebal_per_high or df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] \u0026lt; rebal_per_low: rebalance = \u0026#34;Yes\u0026#34; else: pass # If rebalance is required, rebalance back to 25% for each asset, else just divide contribution evenly across assets if rebalance == \u0026#34;Yes\u0026#34;: df.at[index, \u0026#39;Rebalance\u0026#39;] = rebalance for fund in fund_list: df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] = (Total_BA_Invested + df.at[index, \u0026#39;Contribution\u0026#39;]) * 0.25 else: df.at[index, \u0026#39;Rebalance\u0026#39;] = rebalance for fund in fund_list: df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] = df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] + df.at[index, \u0026#39;Contribution\u0026#39;] * 0.25 # Initialize variable Total_AA_Invested = 0 # Set column values for after action (AA) shares and port % for fund in fund_list: df.at[index, fund + \u0026#34;_AA_Shares\u0026#34;] = df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] / row[fund + \u0026#34;_Close\u0026#34;] # Sum the asset values to find the total Total_AA_Invested = Total_AA_Invested + df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] # Calculate after action (AA) port % values for fund in fund_list: df.at[index, fund + \u0026#34;_AA_Port_%\u0026#34;] = df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] / Total_AA_Invested # Set column for after action (AA) total $ invested df.at[index, \u0026#39;Total_AA_$_Invested\u0026#39;] = Total_AA_Invested # If this is the first row else: pass df[\u0026#39;Return\u0026#39;] = df[\u0026#39;Total_AA_$_Invested\u0026#39;].pct_change() df[\u0026#39;Cumulative_Return\u0026#39;] = (1 + df[\u0026#39;Return\u0026#39;]).cumprod() plan_name = \u0026#39;_\u0026#39;.join(fund_list) file = plan_name + \u0026#34;_Strategy.xlsx\u0026#34; location = file df.to_excel(location, sheet_name=\u0026#39;data\u0026#39;) print(f\u0026#34;Strategy complete for {plan_name}.\u0026#34;) return df Summary Stats 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # Stats for entire data set def summary_stats(fund_list, df, period): if period == \u0026#39;Monthly\u0026#39;: timeframe = 12 # months elif period == \u0026#39;Weekly\u0026#39;: timeframe = 52 # weeks elif period == \u0026#39;Daily\u0026#39;: timeframe = 365 # days elif period == \u0026#39;Hourly\u0026#39;: timeframe = 8760 # hours else: return print(\u0026#34;Error, check inputs\u0026#34;) df_stats = pd.DataFrame(df.mean(axis=0) * timeframe) # annualized # df_stats = pd.DataFrame((1 + df.mean(axis=0)) ** timeframe - 1) # annualized, this is this true annualized return but we will simply use the mean df_stats.columns = [\u0026#39;Annualized Mean\u0026#39;] df_stats[\u0026#39;Annualized Volatility\u0026#39;] = df.std() * np.sqrt(timeframe) # annualized df_stats[\u0026#39;Annualized Sharpe Ratio\u0026#39;] = df_stats[\u0026#39;Annualized Mean\u0026#39;] / df_stats[\u0026#39;Annualized Volatility\u0026#39;] df_cagr = (1 + df[\u0026#39;Return\u0026#39;]).cumprod() cagr = (df_cagr[-1] / 1) ** (1/(len(df_cagr) / timeframe)) - 1 df_stats[\u0026#39;CAGR\u0026#39;] = cagr df_stats[period + \u0026#39; Max Return\u0026#39;] = df.max() df_stats[period + \u0026#39; Max Return (Date)\u0026#39;] = df.idxmax().values[0] df_stats[period + \u0026#39; Min Return\u0026#39;] = df.min() df_stats[period + \u0026#39; Min Return (Date)\u0026#39;] = df.idxmin().values[0] wealth_index = 1000*(1+df).cumprod() previous_peaks = wealth_index.cummax() drawdowns = (wealth_index - previous_peaks)/previous_peaks df_stats[\u0026#39;Max Drawdown\u0026#39;] = drawdowns.min() df_stats[\u0026#39;Peak\u0026#39;] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns] df_stats[\u0026#39;Bottom\u0026#39;] = drawdowns.idxmin() recovery_date = [] for col in wealth_index.columns: prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max() recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T recovery_date.append(recovery_wealth[recovery_wealth[col] \u0026gt;= prev_max].index.min()) df_stats[\u0026#39;Recovery Date\u0026#39;] = recovery_date plan_name = \u0026#39;_\u0026#39;.join(fund_list) file = plan_name + \u0026#34;_Summary_Stats.xlsx\u0026#34; location = file df_stats.to_excel(location, sheet_name=\u0026#39;data\u0026#39;) print(f\u0026#34;Summary stats complete for {plan_name}.\u0026#34;) return df_stats Plot Cumulative Return 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def plot_cumulative_return(strat_df): # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, strat_df[\u0026#39;Cumulative_Return\u0026#39;], label = \u0026#39;Strategy Cumulative Return\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;green\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 0.5 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.ylabel(\u0026#39;Cumulative Return\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(0, 7.5) # Set title, etc. plt.title(\u0026#39;Cumulative Return\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;03_Cumulative_Return.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Plot Portfolio Values 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def plot_values(strat_df): # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, strat_df[\u0026#39;Total_AA_$_Invested\u0026#39;], label=\u0026#39;Total Portfolio Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;black\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Stocks_AA_$_Invested\u0026#39;], label=\u0026#39;Stocks Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;orange\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Bonds_AA_$_Invested\u0026#39;], label=\u0026#39;Bond Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;yellow\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Gold_AA_$_Invested\u0026#39;], label=\u0026#39;Gold Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;blue\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Cash_AA_$_Invested\u0026#39;], label=\u0026#39;Cash Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;brown\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 5000 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:,.0f}\u0026#39;.format(x))) # Adding commas to y-axis labels plt.ylabel(\u0026#39;Total Value ($)\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(0, 75000) # Set title, etc. plt.title(\u0026#39;Total Values For Stocks, Bonds, Gold, and Cash Positions and Portfolio\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;04_Portfolio_Values.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Plot Portfolio Drawdown 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def plot_drawdown(strat_df): rolling_max = strat_df[\u0026#39;Total_AA_$_Invested\u0026#39;].cummax() drawdown = (strat_df[\u0026#39;Total_AA_$_Invested\u0026#39;] - rolling_max) / rolling_max * 100 # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, drawdown, label=\u0026#39;Drawdown\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;red\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 1 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) # plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:,.0f}\u0026#39;.format(x))) # Adding commas to y-axis labels plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:.0f}\u0026#39;.format(x))) # Adding 0 decimal places to y-axis labels plt.ylabel(\u0026#39;Drawdown (%)\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(-20, 0) # Set title, etc. plt.title(\u0026#39;Portfolio Drawdown\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;05_Portfolio_Drawdown.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Plot Portfolio Asset Weights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def plot_asset_weights(strat_df): # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, strat_df[\u0026#39;Stocks_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Stocks Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;orange\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Bonds_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Bonds Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;yellow\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Gold_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Gold Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;blue\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Cash_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Cash Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;brown\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 2 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) # plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:,.0f}\u0026#39;.format(x))) # Adding commas to y-axis labels plt.ylabel(\u0026#39;Asset Weight (%)\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(14, 36) # Set title, etc. plt.title(\u0026#39;Portfolio Asset Weights For Stocks, Bonds, Gold, and Cash Positions\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;07_Portfolio_Weights.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Data Overview Import Data As previously mentioned, the data for this exercise comes primarily from Bloomberg. We\u0026rsquo;ll start with loading the data first for bonds:\n1 2 3 4 5 6 7 8 9 10 # Bonds dataframe bb_data_updater(\u0026#39;SPBDU10T_S\u0026amp;P US Treasury Bond 7-10 Year Total Return Index\u0026#39;) bonds_data = load_data(\u0026#39;SPBDU10T_S\u0026amp;P US Treasury Bond 7-10 Year Total Return Index_Clean.xlsx\u0026#39;) bonds_data[\u0026#39;Date\u0026#39;] = pd.to_datetime(bonds_data[\u0026#39;Date\u0026#39;]) bonds_data.set_index(\u0026#39;Date\u0026#39;, inplace = True) bonds_data = bonds_data[(bonds_data.index \u0026gt;= \u0026#39;1990-01-01\u0026#39;) \u0026amp; (bonds_data.index \u0026lt;= \u0026#39;2023-12-31\u0026#39;)] bonds_data.rename(columns={\u0026#39;Close\u0026#39;:\u0026#39;Bonds_Close\u0026#39;}, inplace=True) bonds_data[\u0026#39;Bonds_Daily_Return\u0026#39;] = bonds_data[\u0026#39;Bonds_Close\u0026#39;].pct_change() bonds_data[\u0026#39;Bonds_Total_Return\u0026#39;] = (1 + bonds_data[\u0026#39;Bonds_Daily_Return\u0026#39;]).cumprod() bonds_data The following is the output:\nThen for stocks:\n1 2 3 4 5 6 7 8 9 10 # Stocks dataframe bb_data_updater(\u0026#39;SPXT_S\u0026amp;P 500 Total Return Index\u0026#39;) stocks_data = load_data(\u0026#39;SPXT_S\u0026amp;P 500 Total Return Index_Clean.xlsx\u0026#39;) stocks_data[\u0026#39;Date\u0026#39;] = pd.to_datetime(stocks_data[\u0026#39;Date\u0026#39;]) stocks_data.set_index(\u0026#39;Date\u0026#39;, inplace = True) stocks_data = stocks_data[(stocks_data.index \u0026gt;= \u0026#39;1990-01-01\u0026#39;) \u0026amp; (stocks_data.index \u0026lt;= \u0026#39;2023-12-31\u0026#39;)] stocks_data.rename(columns={\u0026#39;Close\u0026#39;:\u0026#39;Stocks_Close\u0026#39;}, inplace=True) stocks_data[\u0026#39;Stocks_Daily_Return\u0026#39;] = stocks_data[\u0026#39;Stocks_Close\u0026#39;].pct_change() stocks_data[\u0026#39;Stocks_Total_Return\u0026#39;] = (1 + stocks_data[\u0026#39;Stocks_Daily_Return\u0026#39;]).cumprod() stocks_data And finally, gold:\n1 2 3 4 5 6 7 8 9 10 # Gold dataframe bb_data_updater(\u0026#39;XAU_Gold USD Spot\u0026#39;) gold_data = load_data(\u0026#39;XAU_Gold USD Spot_Clean.xlsx\u0026#39;) gold_data[\u0026#39;Date\u0026#39;] = pd.to_datetime(gold_data[\u0026#39;Date\u0026#39;]) gold_data.set_index(\u0026#39;Date\u0026#39;, inplace = True) gold_data = gold_data[(gold_data.index \u0026gt;= \u0026#39;1990-01-01\u0026#39;) \u0026amp; (gold_data.index \u0026lt;= \u0026#39;2023-12-31\u0026#39;)] gold_data.rename(columns={\u0026#39;Close\u0026#39;:\u0026#39;Gold_Close\u0026#39;}, inplace=True) gold_data[\u0026#39;Gold_Daily_Return\u0026#39;] = gold_data[\u0026#39;Gold_Close\u0026#39;].pct_change() gold_data[\u0026#39;Gold_Total_Return\u0026#39;] = (1 + gold_data[\u0026#39;Gold_Daily_Return\u0026#39;]).cumprod() gold_data Combine Data We\u0026rsquo;ll now combine the dataframes for the timeseries data from each of the asset classes, as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Merge the stock data and bond data into a single DataFrame using their indices (dates) perm_port = pd.merge(stocks_data[\u0026#39;Stocks_Close\u0026#39;], bonds_data[\u0026#39;Bonds_Close\u0026#39;], left_index=True, right_index=True) # Add gold data to the portfolio DataFrame by merging it with the existing data on indices (dates) perm_port = pd.merge(perm_port, gold_data[\u0026#39;Gold_Close\u0026#39;], left_index=True, right_index=True) # Add a column for cash with a constant value of 1 (assumes the value of cash remains constant at $1 over time) perm_port[\u0026#39;Cash_Close\u0026#39;] = 1 # Remove any rows with missing values (NaN) to ensure clean data for further analysis perm_port.dropna(inplace=True) # Display the finalized portfolio DataFrame perm_port DataFrame Info Now, running:\n1 df_info(perm_port) Gives us the following:\nWe can see that we have close data for all 4 asset classes from the beginning of 1990 to the end of 2023.\nExecute Strategy Using an annual rebalance date of January 1, we\u0026rsquo;ll now execute the strategy with the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # List of funds to be used fund_list = [\u0026#39;Stocks\u0026#39;, \u0026#39;Bonds\u0026#39;, \u0026#39;Gold\u0026#39;, \u0026#39;Cash\u0026#39;] # Starting cash contribution starting_cash = 10000 # Monthly cash contribution cash_contrib = 0 strat = strategy(fund_list, starting_cash, cash_contrib, perm_port, 1, 1, 0.35, 0.15).set_index(\u0026#39;Date\u0026#39;) sum_stats = summary_stats(fund_list, strat[[\u0026#39;Return\u0026#39;]], \u0026#39;Daily\u0026#39;) strat_pre_1999 = strat[strat.index \u0026lt; \u0026#39;2000-01-01\u0026#39;] sum_stats_pre_1999 = summary_stats(fund_list, strat_pre_1999[[\u0026#39;Return\u0026#39;]], \u0026#39;Daily\u0026#39;) strat_post_1999 = strat[strat.index \u0026gt;= \u0026#39;2000-01-01\u0026#39;] sum_stats_post_1999 = summary_stats(fund_list, strat_post_1999[[\u0026#39;Return\u0026#39;]], \u0026#39;Daily\u0026#39;) strat_post_2009 = strat[strat.index \u0026gt;= \u0026#39;2010-01-01\u0026#39;] sum_stats_post_2009 = summary_stats(fund_list, strat_post_2009[[\u0026#39;Return\u0026#39;]], \u0026#39;Daily\u0026#39;) Strategy Statistics 1 2 3 4 5 6 7 8 9 all_sum_stats = pd.concat([sum_stats]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;1990 - 2023\u0026#39;}) all_sum_stats = pd.concat([all_sum_stats, sum_stats_pre_1999]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;Pre 1999\u0026#39;}) all_sum_stats = pd.concat([all_sum_stats, sum_stats_post_1999]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;Post 1999\u0026#39;}) all_sum_stats = pd.concat([all_sum_stats, sum_stats_post_2009]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;Post 2009\u0026#39;}) all_sum_stats Since the strategy, summary statistics, and annual returns are all exported as excel files, they can be found at the following locations:\nStocks_Bonds_Gold_Cash_Strategy.xlsx Stocks_Bonds_Gold_Cash_Summary_Stats.xlsx Stocks_Bonds_Gold_Cash_Annual_Returns.xlsx\nHere\u0026rsquo;s the summary stats for the example above including the various timeframes:\nHere we have a mean annualized return of 8.3%, volatility of 7.2%, a CAGR of 8.4% and a Sharpe ratio of 1.15. And this with a max drawdown of just over 15%. Not bad, Mr. Browne!\nSince the book was published in 1999, let\u0026rsquo;s look specifically at the summary stats for below and after 1999.\nThe mean annualized return is approximately 0.7% lower for the pre 1999 vs post 1999 data, as is the CAGR. The volatility is higher for the post 1999 data which leads to a difference in the Sharpe ratio.\nHere\u0026rsquo;s the annual returns:\nGenerate Plots 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 plot_cumulative_return(strat) plot_values(strat) plot_drawdown(strat) plot_asset_weights(strat) # Create dataframe for the annual returns strat_annual_returns = strat[\u0026#39;Cumulative_Return\u0026#39;].resample(\u0026#39;Y\u0026#39;).last().pct_change().dropna() strat_annual_returns_df = strat_annual_returns.to_frame() strat_annual_returns_df[\u0026#39;Year\u0026#39;] = strat_annual_returns_df.index.year # Add a \u0026#39;Year\u0026#39; column with just the year strat_annual_returns_df.reset_index(drop=True, inplace=True) # Reset the index to remove the datetime index # Now the DataFrame will have \u0026#39;Year\u0026#39; and \u0026#39;Cumulative_Return\u0026#39; columns strat_annual_returns_df = strat_annual_returns_df[[\u0026#39;Year\u0026#39;, \u0026#39;Cumulative_Return\u0026#39;]] # Keep only \u0026#39;Year\u0026#39; and \u0026#39;Cumulative_Return\u0026#39; columns strat_annual_returns_df.rename(columns = {\u0026#39;Cumulative_Return\u0026#39;:\u0026#39;Return\u0026#39;}, inplace=True) strat_annual_returns_df.set_index(\u0026#39;Year\u0026#39;, inplace=True) display(strat_annual_returns_df) plan_name = \u0026#39;_\u0026#39;.join(fund_list) file = plan_name + \u0026#34;_Annual_Returns.xlsx\u0026#34; location = file strat_annual_returns_df.to_excel(location, sheet_name=\u0026#39;data\u0026#39;) plot_annual_returns(strat_annual_returns_df) Here are several relevant plots:\nCumulative Return Portfolio Values (Total, Stocks, Bonds, Gold, and Cash) Here we can see the annual rebalancing taking effect with the values of the different asset classes. This can also be seen more clearly below.\nPortfolio Drawdown From this plot, we can see that the maximum drawdown came during the GFC; the drawdown during COVID was (interestingly) less than 10%.\nPortfolio Asset Weights The annual rebalancing appears to work effectively by selling assets that have increased in value and buying assets that have decreased in value over the previous year. Also note that there is only one instance when the weight of an asset fell to 15%. This occured for stocks during the GFC.\nPortfolio Annual Returns It\u0026rsquo;s interesting to see that there really aren\u0026rsquo;t any significant up or down years. Instead, it\u0026rsquo;s a steady climb without much volatility.\nSummary Overall, this is an interesting case study and Browne\u0026rsquo;s idea behind the Permanent Portfolio is certainly compelling. There might be more investigation to be done with respect to the following:\nInvestigate the extent to which the rebalancing date effects the portfolio performance Vary the weights of the asset classes to see if there is a meanful change in the results Experiment with leverage (i.e., simulating 1.2x leverage with a portfolio with weights of 30, 30, 30, 10 for stocks, bonds, gold, cash respectively.) References Fail-Safe Investing: Lifelong Financial Security in 30 Minutes, by Harry Browne\nCode The jupyter notebook with the functions and all other code is available here.\n","date":"2024-11-04T00:00:01Z","image":"https://www.jaredszajkowski.com/2024/11/04/does-harry-brownes-permanent-portfolio-withstand-the-test-of-time/cover.jpg","permalink":"https://www.jaredszajkowski.com/2024/11/04/does-harry-brownes-permanent-portfolio-withstand-the-test-of-time/","title":"Does Harry Browne's permanent portfolio withstand the test of time?"},{"content":"Introduction While there are numerous backup solutions available for Linux, many require extensive configuration and maintenance, and restoring from the backup is not always simple. Incremental backups are ideal because they maintain snapshots of the files and allow for access to previous versions of files.\nLinux Journal recently published an article on various backup solutions, and I thought I\u0026rsquo;d provide my incremental backup script that uses rsync and cp.\nIncremental backup script This script provides an incremental backup solution and only requires rsync and cp to be installed on the system.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash # Define the directories to backup and their destination directories source_dir1=\u0026#34;/source1\u0026#34; backup_dir1=\u0026#34;/backup1/\u0026#34; source_dir2=\u0026#34;/source2\u0026#34; backup_dir2=\u0026#34;/backup2/\u0026#34; # Define excluded directories excluded_dir1=\u0026#34;leave/out/\u0026#34; excluded_dir2=\u0026#34;dont/want/\u0026#34; excluded_dir3=\u0026#34;exclude/this/\u0026#34; # Function to run a backup run_backup() { source_dir=$1 backup_dir=$2 # Check if the source directory exists if [ ! -d \u0026#34;$source_dir\u0026#34; ]; then echo \u0026#34;Error: Source directory not found\u0026#34; exit 1 fi # Input year and date echo \u0026#34;What is today\u0026#39;s year:\u0026#34; read backup_year echo \u0026#34;What is today\u0026#39;s date:\u0026#34; read backup_date # Check if the backup directory exists and run backup if [ -d \u0026#34;$backup_dir\u0026#34; ]; then echo \u0026#34;Backup directory found, backing up $source_dir\u0026#34; rsync -av --delete --exclude \u0026#34;$excluded_dir1\u0026#34; --exclude \u0026#34;$excluded_dir2\u0026#34; --exclude \u0026#34;$excluded_dir3\u0026#34; $source_dir $backup_dir/Monthly/ cp -al $backup_dir/Monthly/ $backup_dir/$backup_year/$backup_date/ else echo \u0026#34;Error: Backup directory not found\u0026#34; exit 1 fi } # Run backups run_backup $source_dir1 $backup_dir1 run_backup $source_dir2 $backup_dir2 # Output confirmation echo \u0026#34;Backup complete\u0026#34; Let\u0026rsquo;s break this down line by line.\nSource and backup directories First, we need to define the source and backup directories, and any directories from the source that are to be excluded from the backup:\n1 2 3 4 5 6 7 8 9 10 11 # Define the directories to backup and their destination directories source_dir1=\u0026#34;/source1\u0026#34; backup_dir1=\u0026#34;/backup1/\u0026#34; source_dir2=\u0026#34;/source2\u0026#34; backup_dir2=\u0026#34;/backup2/\u0026#34; # Define excluded directories excluded_dir1=\u0026#34;leave/out/\u0026#34; excluded_dir2=\u0026#34;dont/want/\u0026#34; excluded_dir3=\u0026#34;exclude/this/\u0026#34; You can add as many directories as you want here.\nBackup function Then we have the backup function. This performs the following:\nTakes an input of the source and backup directories (defined above) Checks to see if the source directory exists Prompts for a year Prompts for a date Checks to make sure the backup destination directory exists Executes the backup 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Function to run a backup run_backup() { source_dir=$1 backup_dir=$2 # Check if the source directory exists if [ ! -d \u0026#34;$source_dir\u0026#34; ]; then echo \u0026#34;Error: Source directory not found\u0026#34; exit 1 fi # Input year and date echo \u0026#34;What is today\u0026#39;s year:\u0026#34; read backup_year echo \u0026#34;What is today\u0026#39;s date:\u0026#34; read backup_date # Check if the backup directory exists and run backup if [ -d \u0026#34;$backup_dir\u0026#34; ]; then echo \u0026#34;Backup directory found, backing up $source_dir\u0026#34; rsync -av --delete --exclude \u0026#34;$excluded_dir1\u0026#34; --exclude \u0026#34;$excluded_dir2\u0026#34; --exclude \u0026#34;$excluded_dir3\u0026#34; $source_dir $backup_dir/Monthly/ cp -al $backup_dir/Monthly/ $backup_dir/$backup_year/$backup_date/ else echo \u0026#34;Error: Backup directory not found\u0026#34; exit 1 fi } rsync is used to compare the files in the source to the Monthly backup directory and then update or delete files accordingly.\nOnce the files are copied over via rsync, then the cp command is used to link the files in the Monthly directory to the year/date/ diorectory. As the files change in the Monthly directory, then the link also changes. This method saves disk space because files are not copied over and over again. Any files that do not change are simply linked within the filesystem. The links take up a trivial amount of disk space, and the filesystem handles all of the heavy lifting associated with tracking which files are linked and where on the filesystem. There is no database, log, etc. required to track the individual files and/or their versions.\nRunning backups Finally, run the backups and confirm complete:\n1 2 3 4 5 6 # Run backups run_backup $source_dir1 $backup_dir1 run_backup $source_dir2 $backup_dir2 # Output confirmation echo \u0026#34;Backup complete\u0026#34; Results This script provides an incremental backup record organized by year and date:\nAccessing older backups is straightforward - simply navigate to the desired directory within the filesystem.\nDeleting old backups Deleting or removing old and out-of-date backups is as simple as deleting the directories. The filesystem links and files that are not linked elsewhere are removed from the filesystem, freeing up the disk space.\nReferences https://rsync.samba.org/ https://github.com/WayneD/rsync https://www.gnu.org/software/coreutils/ https://www.man7.org/linux/man-pages/man1/cp.1.html\n","date":"2024-01-12T00:00:01Z","image":"https://www.jaredszajkowski.com/2024/01/12/simple-incremental-bash-backup-script/cover.jpg","permalink":"https://www.jaredszajkowski.com/2024/01/12/simple-incremental-bash-backup-script/","title":"Simple Incremental Bash Backup Script"},{"content":"Introduction In this tutorial, we will write a python function that pulls data from Nasdaq Data Link through the tables API, adds relevant columns that are not present in the raw data, updates columns to allow for ease of use, and leaves the data in a format where it can then be used in time series analysis.\nNasdaq Data Link is a provider of numerous different types of financial data from many different asset classes. It provides API\u0026rsquo;s that allow access from Python, R, Excel, and other methods. It is available to institutional investors as well as individual retail investors.\nNasdaq Data Link Initial Data Retrieval We will use the data for AAPL for this example. This will give us a data set that requires some thought as to how the splits need to be handled as well as the dividends.\nWe\u0026rsquo;ll start with pulling the initial data set, with the first 10 rows shown as follows from the pandas dataframe:\nAnd the last 10 rows:\nFrom left to right, we have the following columns:\nRow number: 0 indexed, gives us the total number of rows/dates of data Ticker: The ticker symbol for our data Date: In the format YYYY-MM-DD Open: Daily open High: Daily high Low: Daily low Close: Daily close Volume: Volume of shares traded Dividend: Dividend paid on that date Split: Split executed on that date Adjusted Open: Daily open price adusted for all splits and dividends Adjusted High: Daily high price adusted for all splits and dividends Adjusted Low: Daily low price adusted for all splits and dividends Adjusted Close: Daily close price adusted for all splits and dividends Adjusted Volume: Daily volume price adusted for all splits Data questions The above information is a good starting point, but what if we are looking for the following answers?\nThe data shows a split value for every day, but we know the stock didn\u0026rsquo;t split every day. What does this represent? What is the total cumulative split ratio? What is the split ratio at different points in time? What is the adjusted share price without including the dividends? This would be needed for any time series analysis. What is the dividend dollar value based on an adjusted share price? What would the share price be if the stock hadn\u0026rsquo;t split? We\u0026rsquo;ll add columns and modify as necessary to answer the above questions and more.\nAssumptions The remainder of this tutorial assumes the following:\nYou have the Nasdaq Data Link library installed You have the pandas library installed You have the OpenPyXL library installed Python function to modify the data The following function will perform the desired modifications:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # This function pulls the data for the specific fund from from Nasdaq Data # Link and adds many missing columns # Imports import nasdaqdatalink import pandas as pd import numpy as np # Add API key for reference to allow access to unrestricted data nasdaqdatalink.ApiConfig.api_key = \u0026#39;your_key\u0026#39; # Function definition def ndl_data_updater(fund): # Command to pull data # If start date and end date are not specified the entire data set is included df = nasdaqdatalink.get_table(\u0026#39;QUOTEMEDIA/PRICES\u0026#39;, ticker = fund, paginate=True) # Sort columns by date ascending df.sort_values(\u0026#39;date\u0026#39;, ascending = True, inplace = True) # Rename date column df.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Date\u0026#39;}, inplace = True) # Set index to date column df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Replace all split values of 1.0 with NaN df[\u0026#39;split\u0026#39;] = df[\u0026#39;split\u0026#39;].replace(1.0, np.nan) # Create a new data frame with split values only df_splits = df.drop(columns = {\u0026#39;ticker\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;, \u0026#39;dividend\u0026#39;, \u0026#39;adj_open\u0026#39;, \u0026#39;adj_high\u0026#39;, \u0026#39;adj_low\u0026#39;, \u0026#39;adj_close\u0026#39;, \u0026#39;adj_volume\u0026#39;}).dropna() # Create a new column for cumulative split df_splits[\u0026#39;Cum_Split\u0026#39;] = df_splits[\u0026#39;split\u0026#39;].cumprod() # Drop original split column before combining dataframes df_splits.drop(columns = {\u0026#39;split\u0026#39;}, inplace = True) # Merge df and df_split dataframes df_comp = pd.merge(df, df_splits, on=\u0026#39;Date\u0026#39;, how=\u0026#39;outer\u0026#39;) # Forward fill for all cumulative split values df_comp[\u0026#39;Cum_Split\u0026#39;].fillna(method = \u0026#39;ffill\u0026#39;, inplace = True) # Replace all split and cumulative split values of NaN with 1.0 to have complete split values df_comp[\u0026#39;split\u0026#39;] = df_comp[\u0026#39;split\u0026#39;].replace(np.nan, 1.0) df_comp[\u0026#39;Cum_Split\u0026#39;] = df_comp[\u0026#39;Cum_Split\u0026#39;].replace(np.nan, 1.0) # Calculate the non adjusted prices based on the splits only df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] = df_comp[\u0026#39;open\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] = df_comp[\u0026#39;high\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] = df_comp[\u0026#39;low\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] = df_comp[\u0026#39;close\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] # Calculate the adjusted prices based on the splits df_comp[\u0026#39;Open\u0026#39;] = df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;High\u0026#39;] = df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Low\u0026#39;] = df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Close\u0026#39;] = df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend\u0026#39;] = df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend_Pct_Orig\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] / df_comp[\u0026#39;close\u0026#39;] df_comp[\u0026#39;Dividend_Pct_Adj\u0026#39;] = df_comp[\u0026#39;Dividend\u0026#39;] / df_comp[\u0026#39;Close\u0026#39;] # Export data to excel file = fund + \u0026#34;_NDL.xlsx\u0026#34; df_comp.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df_comp[-1:]) print(f\u0026#34;NDL data updater complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Let\u0026rsquo;s break this down line by line.\nImports First, we need to import the required libraries:\n1 2 3 4 # Imports import nasdaqdatalink import pandas as pd import numpy as np NDL API Key To gain access to anything beyond the free tier, you will need to provide your access key:\n1 2 # Add API key for reference to allow access to unrestricted data nasdaqdatalink.ApiConfig.api_key = \u0026#39;your_key\u0026#39; Download data as a dataframe Moving on to the function definition, we have the command to pull data from NDL. There are two separate APIs - the time series and the tables. The syntax is different, and some data sets are only available as one or the other. We will use the tables API for this tutorial.\n1 2 3 # Command to pull data # If start date and end date are not specified the entire data set is included df = nasdaqdatalink.get_table(\u0026#39;QUOTEMEDIA/PRICES\u0026#39;, ticker = fund, paginate=True) In the example above, the fund is an input parameter to the function.\nThe 'QUOTEMEDIA/PRICES' is the data source that we are accessing.\nThere are many other arguments that we could pass in the above, including specifying columns, period start date, period end date, and others. Nasdaq as a few examples to get you started:\nhttps://docs.data.nasdaq.com/docs/python-tables\nRunning:\ndf.head(10) Gives us:\nSort columns by date Next, we will sort the columns by date ascending. By default, the dataframe is created with the data sorted by descending date, and we want to change that:\n1 2 # Sort columns by date ascending df.sort_values(\u0026#39;date\u0026#39;, ascending = True, inplace = True) The inplace = True argument specifies that the sort function should take effect on the existing dataframe.\nNow, running:\ndf.head(10) Gives us:\nSetting the date as the index Next, we will rename the date column from \u0026lsquo;date\u0026rsquo; to \u0026lsquo;Date\u0026rsquo;, and set the index to be the Date column:\n1 2 3 4 5 # Rename date column df.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Date\u0026#39;}, inplace = True) # Set index to date column df.set_index(\u0026#39;Date\u0026#39;, inplace = True) Now, running:\ndf.head(10) Gives us:\nCalculating splits The next sections deal with the split column. So far we have only seen a split value of 1.0 in the data, but we\u0026rsquo;ve only looked at the first 10 and last 10 rows. Are there any other values? Let\u0026rsquo;s check by running:\n1 df_not_1_split = df[df[\u0026#39;split\u0026#39;] != 1.0] And checking the first 10 rows:\ndf_not_1_split.head(10) Gives us:\nSo we now know that the stock did in fact split several times. Next, we will replace all of the 1.0 split values - because they are really meaningless - and then create a new dataframe to deal with the splits.\n1 2 # Replace all split values of 1.0 with NaN df[\u0026#39;split\u0026#39;] = df[\u0026#39;split\u0026#39;].replace(1.0, np.nan) This gives us:\nWe will now create a dataframe with only the split values:\n1 2 3 4 # Create a new data frame with split values only df_splits = df.drop(columns = {\u0026#39;ticker\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;, \u0026#39;dividend\u0026#39;, \u0026#39;adj_open\u0026#39;, \u0026#39;adj_high\u0026#39;, \u0026#39;adj_low\u0026#39;, \u0026#39;adj_close\u0026#39;, \u0026#39;adj_volume\u0026#39;}).dropna() Which gives us:\nCreating a column for the cumulative split will provide an accurate perspective on the stock price. We can do that with the following:\n1 2 # Create a new column for cumulative split df_splits[\u0026#39;Cum_Split\u0026#39;] = df_splits[\u0026#39;split\u0026#39;].cumprod() Which gives us:\nWe will then drop the original split column before combining the split data frame with the original data frame, as follows:\n1 2 # Drop original split column before combining dataframes df_splits.drop(columns = {\u0026#39;split\u0026#39;}, inplace = True) Which gives us:\nCombining dataframes Now we will merge the df_split dataframe with the original df dataframe so that the cumulative split column is part of the original dataframe. We will call this data frame df_comp:\n1 2 # Merge df and df_split dataframes df_comp = pd.merge(df, df_splits, on=\u0026#39;Date\u0026#39;, how=\u0026#39;outer\u0026#39;) We are using the merge function of pandas, which includes arguments for the names of both dataframes to be merged, the column to match between the dataframes, and the parameter for the type of merge to be performed. The outer argument specifies that all rows from both dataframes will be included, and any missing values will be filled in with NaN if there is no matching data. This ensures that all data from both dataframes is retained.\nRunning:\ndf_comp.head(10) Gives us:\nForward filling cumulative split values From here, we want to fill in the rest of the split and Cum_Split values. This is done using the forward fill function, which for all cells that have a value of NaN will fill in the previous valid value until another value is encountered. Here\u0026rsquo;s the code:\n1 2 # Forward fill for all cumulative split values df_comp[\u0026#39;Cum_Split\u0026#39;].fillna(method = \u0026#39;ffill\u0026#39;, inplace = True) Running:\ndf_comp.head(10) Gives us:\nAt first glance, it doesn\u0026rsquo;t look like anything changed. That\u0026rsquo;s because there wasn\u0026rsquo;t any ffill action taken on the initial values until pandas encountered a valid value to then forward fill. However, checking the last 10 rows:\ndf_comp.tail(10) Gives us:\nWhich is the result that we were expecting. But, what about the first rows from 12/12/1980 to 6/15/1987? We can fill those split and Cum_Split values with the following code:\n1 2 3 # Replace all split and cumulative split values of NaN with 1.0 to have complete split values df_comp[\u0026#39;split\u0026#39;] = df_comp[\u0026#39;split\u0026#39;].replace(np.nan, 1.0) df_comp[\u0026#39;Cum_Split\u0026#39;] = df_comp[\u0026#39;Cum_Split\u0026#39;].replace(np.nan, 1.0) Now, checking the first 10 rows:\ndf_comp.head(10) Gives us:\nWith this data, we now know for every day in the data set the following pieces of information:\nIf the stock split on that day What the total split ratio is up to and including that day Calculating adjusted and non-adjusted prices From here, we can complete our dataset by calculating the adjusted and non-adjusted prices using the cumulative split ratios from above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Calculate the non adjusted prices based on the splits only df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] = df_comp[\u0026#39;open\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] = df_comp[\u0026#39;high\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] = df_comp[\u0026#39;low\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] = df_comp[\u0026#39;close\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] # Calculate the adjusted prices based on the splits df_comp[\u0026#39;Open\u0026#39;] = df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;High\u0026#39;] = df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Low\u0026#39;] = df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Close\u0026#39;] = df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend\u0026#39;] = df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend_Pct_Orig\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] / df_comp[\u0026#39;close\u0026#39;] df_comp[\u0026#39;Dividend_Pct_Adj\u0026#39;] = df_comp[\u0026#39;Dividend\u0026#39;] / df_comp[\u0026#39;Close\u0026#39;] Included above is the adjusted dividends values. For any time series analysis, not only are the adjusted prices needed, but so are the adusted dividends. Remember, we already have the adjusted total return prices - those come directly from NDL.\nExport data Next, we want to export the data to an excel file, for easy viewing and reference later:\n1 2 3 # Export data to excel file = fund + \u0026#34;_NDL.xlsx\u0026#34; df_comp.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) And verify the output is as expected:\nOutput confirmation Finally, we want to print a confirmation that the process succeeded along withe last date we have for data:\n1 2 3 4 5 # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df_comp[-1:]) print(f\u0026#34;NDL data updater complete for {fund} data\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) And confirming the output:\nReferences https://docs.data.nasdaq.com/docs https://docs.data.nasdaq.com/docs/tables-1 https://docs.data.nasdaq.com/docs/time-series https://docs.data.nasdaq.com/docs/python\n","date":"2023-12-24T00:00:01Z","image":"https://www.jaredszajkowski.com/2023/12/24/nasdaq-data-link-tables-api-data-retrieval/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/12/24/nasdaq-data-link-tables-api-data-retrieval/","title":"Nasdaq Data Link Tables API Data Retrieval"},{"content":"Introduction As a follow up to this post about using yt-dlp with Zoom, I found the need to also download videos hosted by Panopto for offline viewing.\nSimilar to the Zoom tutorial, this also requires you to have a \u0026ldquo;cookies\u0026rdquo; text file, which needs to contain the cookies export in the Netscape HTTP format of the Panopto hosting site after logging in.\nHere\u0026rsquo;s the steps to setting this up:\nInstall cookie editor Install the cookie editor extension. I personnally use it with Microsoft Edge, but there are similar extensions for Chrome, Firefox, etc.\nModify export format Change the preferred cookie export format to Netscape HTTP Cookie File in the extension options. It is necessary to export in this format, otherwise yt-dlp will not be able to read the cookies.txt file correctly.\nLog in to Panopto Log in to Panopto in your browser. Be sure to remain logged in while exporting the cookies under step 4.\nExport cookies The export button is at the top fo the window. It copies the cookies to your clipboard, which then need to be pasted into a text file (I have my saved as cookies.txt), which yt-dlp will then read when it executes.\nCreate bash script Save the following code to a text file (my bash script file name is yt-dlp-panopto.sh):\n1 2 3 4 5 6 #!/bin/bash echo What is the link? read link yt-dlp --cookies /path/to/cookies/file/cookies.txt -o \u0026#34;%(title)s-%(id)s.%(ext)s\u0026#34; --write-subs $link Change permissions Modify the permissions of the bash script to allow execution:\n$ chmod +x yt-dlp-panopto.sh Execute the script Execute the bash script with ./yt-dlp-panopto.sh, copy and paste the link to the video that you would like to save, and it should download the video and the subtitles. If there are not any subtitles present in the stream, then it will notify you and only download the video stream.\nReferences References for yt-dlp (and used for above):\nhttps://ostechnix.com/yt-dlp-tutorial/\n","date":"2023-12-10T00:00:00Z","image":"https://www.jaredszajkowski.com/2023/12/10/using-yt-dlp-with-panopto/banner_1.svg","permalink":"https://www.jaredszajkowski.com/2023/12/10/using-yt-dlp-with-panopto/","title":"Using yt-dlp With Panopto"},{"content":"Introduction In this tutorial, we will write a python function that imports an excel export from Bloomberg, removes ancillary rows and columns, and leaves the data in a format where it can then be used in time series analysis.\nExample of a Bloomberg excel export We will use the SPX index data in this example. Exporting the data from Bloomberg using the excel Bloomberg add-on yields data in the following format:\nData modifications The above format isn\u0026rsquo;t horrible, but we want to perform the following modifications:\nRemove the first six rows of the data Convert the 7th row to become column headings Rename column 2 to \u0026ldquo;Close\u0026rdquo; to represent the closing price Remove column 3, as we are not concerned about volume Export to excel and make the name of the excel worksheet \u0026ldquo;data\u0026rdquo; Assumptions The remainder of this tutorial assumes the following:\nYour excel file is named \u0026ldquo;SPX_Index.xlsx\u0026rdquo; The worksheet in the excel file is named \u0026ldquo;Worksheet\u0026rdquo; You have the pandas library installed You have the OpenPyXL library installed Python function to modify the data The following function will perform the modifications mentioned above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # This function takes an excel export from Bloomberg and # removes all excess data leaving date and close columns # Imports import pandas as pd # Function definition def bb_data_updater(fund): # File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Let\u0026rsquo;s break this down line by line.\nImports First, we need to import pandas:\n1 import pandas as pd Import excel data file Then import the excel file as a pandas dataframe:\n1 2 3 4 5 # File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) Running:\ndf.head(10) Gives us:\nSet column headings Next, set the column heading:\n1 2 # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] Now, running:\ndf.head(10) Gives us:\nRemove index heading Next, remove the column heading from the index column:\n1 2 # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) Note: The axis=1 argument here specifies the column index.\nNow, running:\ndf.head(10) Gives us:\nDrop rows Next, we want to remove the first 6 rows that have unneeded data:\n1 2 # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) Note: When dropping rows, the range to drop begins with row 0 and continues up to - but not including - row 6.\nNow, running:\ndf.head(10) Gives us:\nSet index Next, we want to set the date column as the index:\n1 2 # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) Now, running:\ndf.head(10) Gives us:\nDrop the \u0026ldquo;PX_VOLUME\u0026rdquo; column Next, we want to drop the volume column:\n1 2 3 4 5 # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass For some data records, the volume column does not exist. Therefore, we try, and if it fails with a KeyError, then we assume the \u0026ldquo;PX_VOLUME\u0026rdquo; column does not exist, and just pass to move on.\nNow, running:\ndf.head(10) Gives us:\nRename the \u0026ldquo;PX_LAST\u0026rdquo; column Next, we want to rename the \u0026ldquo;PX_LAST\u0026rdquo; column as \u0026ldquo;Close\u0026rdquo;:\n1 2 # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) Now, running:\ndf.head(10) Gives us:\nSort data Next, we want to sort the data starting with the oldest date:\n1 2 # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) Now, running:\ndf.head(10) Gives us:\nExport data Next, we want to export the data to an excel file, for easy viewing and reference later:\n1 2 3 # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) And verify the output is as expected:\nOutput confirmation Finally, we want to print a confirmation that the process succeeded along withe last date we have for data:\n1 2 3 4 5 # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) And confirming the output:\nReferences https://www.bloomberg.com/professional/support/software-updates/\n","date":"2023-11-15T00:00:01Z","image":"https://www.jaredszajkowski.com/2023/11/15/cleaning-a-bloomberg-data-excel-export/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/11/15/cleaning-a-bloomberg-data-excel-export/","title":"Cleaning A Bloomberg Data Excel Export"},{"content":"Introduction Here are my notes for some of the more commonly used git commands along with initial setup for git in Linux.\nInstallation To begin, install as follows for Arch Linux:\n# pacman -Syu git Or\n$ yay git Pacman will include all required depencies.\nInitial configuration First, set your name and email address:\n$ git config --global user.name \u0026quot;Firstname Lastname\u0026quot; $ git config --global user.email \u0026quot;email@address.com\u0026quot; Then, set your preferred text editor (if you have one). I use nano:\n$ git config --global core.editor \u0026quot;nano\u0026quot; You can verify the updates with:\n$ git config --global core.editor Alternatively, you can edit the git configuration directly with:\n$ git config --global --edit Store credentials In 2021, GitHub disabled authentication via password and now requires authentication with a token. The following command sets up the credential helper, where it will store your token in ~/.git-credentials:\n$ git config --global credential.helper store After you log in during a git push with your username and token, the username or email address and token will be stored in the above location.\nNote: The token is stored in plain text, so use caution if that is a concern.\nCloning repositories Repositories can be cloned with the following:\n$ git clone https://github.com/\u0026lt;username\u0026gt;/\u0026lt;repository\u0026gt;.git Updating repositories The local record of a repository can be updated with the following command:\n$ cd \u0026lt;repository\u0026gt;/ $ git pull Adding, committing, and pushing Any files or directories that have been added, modified, or removed can be add to the list of changes to be pushed with the following command:\n$ git add . Then committed (staged in preparation to push) with the following command:\n$ git commit -am \u0026quot;Add your commit message here\u0026quot; Note: Without add, commit will handle any changes to files that have been modified or deleted, but will not incorporate any files that have been created.\nThen finally pushed:\n$ git push If, for some reason, you would like to reset a commit:\n$ git reset These commands can be chained together with the AND operator:\n$ git add . \u0026amp;\u0026amp; git commit -am \u0026quot;Add your commit message here\u0026quot; \u0026amp;\u0026amp; git push Stashing changes If you forget to update a repository before making changes, you can \u0026ldquo;stash\u0026rdquo; those changes and then re-apply them after running git pull.\nFirst, stash the changes:\n$ git stash Then, update the local record of the repository:\n$ git pull Finally, re-apply the changes you previously made:\n$ git stash apply This has proven to be very useful for me when I forget to update a repository before making edits to the code.\nReferences References for git (and used for above):\nhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup#_first_time https://git-scm.com/book/en/v2/Appendix-C%3A-Git-Commands-Setup-and-Config https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage#_credential_caching https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning https://www.geeksforgeeks.org/difference-between-chaining-operators-in-linux/\n","date":"2023-10-16T00:00:00Z","image":"https://www.jaredszajkowski.com/2023/10/16/git-quick-start-guide/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/10/16/git-quick-start-guide/","title":"Git Quick Start Guide"},{"content":"Introduction If anyone uses Zoom to record and later wants to access recordings of video calls, here\u0026rsquo;s a simple linux bash script to download the video file and acompanying subtitles. For a long time I used zoomdl, but it is no longer under active development, and I began running into various issues about a year ago.\nThis tutorial requires you to have a \u0026ldquo;cookies\u0026rdquo; text file, which needs to contain the cookies export in the Netscape HTTP format of the Zoom cookies after logging in.\nHere\u0026rsquo;s the steps to setting this up:\nInstall cookie editor Install the cookie editor extension. I personnally use it with Microsoft Edge, but there are similar extensions for Chrome, Firefox, etc.\nModify export format Change the preferred cookie export format to Netscape HTTP Cookie File in the extension options. It is necessary to export in this format, otherwise yt-dlp will not be able to read the cookies.txt file correctly.\nLog in to Zoom Log in to Zoom in your browser. Be sure to remain logged in while exporting the cookies under step 4.\nExport cookies The export button is at the top fo the window. It copies the cookies to your clipboard, which then need to be pasted into a text file (I have my saved as cookies.txt), which yt-dlp will then read when it executes.\nCreate bash script Save the following code to a text file (my bash script file name is yt-dlp-zoom.sh):\n1 2 3 4 5 6 #!/bin/bash echo What is the link? read link yt-dlp --referer \u0026#34;https://zoom.us/\u0026#34; --cookies /path/to/cookies/file/cookies.txt -o \u0026#34;%(title)s-%(id)s.%(ext)s\u0026#34; --write-subs $link Change permissions Modify the permissions of the bash script to allow execution:\n$ chmod +x yt-dlp-zoom.sh Execute the script Execute the bash script with ./yt-dlp-zoom.sh, copy and paste the link to the video that you would like to save, and it should download the video and the subtitles.\nReferences References for yt-dlp (and used for above):\nhttps://ostechnix.com/yt-dlp-tutorial/\n","date":"2023-10-01T00:00:00Z","image":"https://www.jaredszajkowski.com/2023/10/01/using-yt-dlp-with-zoom/banner_1.svg","permalink":"https://www.jaredszajkowski.com/2023/10/01/using-yt-dlp-with-zoom/","title":"Using yt-dlp With Zoom"},{"content":"Introduction This is the basic framework that I use to install Arch Linux, with a few changes catered to the Lenovo ThinkPad E15 Gen 2. I have found that this is a decent mid range laptop, excellent linux compatibility, great keyboard, and overall provides a good value.\nGetting started This tutorial assumes the following:\nYou are booting from a USB drive with the Arch install ISO. Wireless or wired network is detected and drivers are configured automatically. You want drive encrytion on your root partition, but not on your boot/efi/swap partitions. Verify UEFI boot mode The following command should show directory without error:\n# ls /sys/firmware/efi/efivars Configure wireless network The following command will drop you into the iwd daemon:\n# iwctl From there:\n# device list # station *device* scan # station *device* get-networks # station *device* connect *SSID* Verify internet connectivity # ping archlinux.org Update system clock # timedatectl set-ntp true # timedatectl status Disks, partition table \u0026amp; partitions The following assumes that your NVME drive is found as /dev/nvme0n1. Partitions will then be /dev/nvme0n1p1 and so on.\nWipe disk List disks:\n# fdisk -l Wipe all file system records:\n# wipefs -a /dev/nvme0n1 Create new partition table Open nvme0n1 with gdisk:\n# gdisk /dev/nvme0n1 Create GPT partition table with option \u0026ldquo;o\u0026rdquo;.\nCreate EFI partition Create new EFI partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #1 Default starting sector +550M Change partition type to EFI System (ef00) Create boot partition Create new boot partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #2 Default starting sector +550M Leave default type of 8300 Create swap partition The old rule of thumb used to be that a swap partition should be the same size as the amount of memory in the system, but given the typical amount of memory in modern systems this is obviously no longer necessary. For my system with 16 or 32 GB of memory, a swap of 8 GB is rarely even used.\nCreate new Swap partition w/ 8GB with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #3 Default starting sector +8G Change to linux swap (8200) Create root partition Create new root partition w/ remaining disk space with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #4 Default starting sector Remaining space Linux LUKS type 8309 And then exit gdisk.\nWrite file systems EFI partition Write file system to new EFI System partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p1 # mkfs.fat -F32 /dev/nvme0n1p1 Boot partition Then boot partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p2 # mkfs.ext2 /dev/nvme0n1p2 Root partition Prepare root partition w/ LUKS:\n# cryptsetup -y -v luksFormat --type luks2 /dev/nvme0n1p4 # cryptsetup luksDump /dev/nvme0n1p4 # cryptsetup open /dev/nvme0n1p4 archcryptroot # mkfs.ext4 /dev/mapper/archcryptroot # mount /dev/mapper/archcryptroot /mnt I use archcryptroot for the name of my encrypted volume, but change as necessary.\nSwap partition Then swap:\n# mkswap /dev/nvme0n1p3 # swapon /dev/nvme0n1p3 Create mount points # mkdir /mnt/boot # mount /dev/nvme0n1p2 /mnt/boot # mkdir /mnt/boot/efi # mount /dev/nvme0n1p1 /mnt/boot/efi System install Install base packages # pacstrap /mnt base base-devel linux linux-firmware grub-efi-x86_64 efibootmgr Generate fstab # genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Enter new system # arch-chroot /mnt /bin/bash Set clock # ln -sf /usr/share/zoneinfo/America/Chicago /etc/localtime # hwclock –systohc Generate locale In /etc/locale.gen uncomment only: en_US.UTF-8 UTF-8\n# locale-gen In /etc/locale.conf, you should only have this line: LANG=en_US.UTF-8\n# nano /etc/locale.conf Set hostname \u0026amp; update hosts # echo linuxmachine \u0026gt; /etc/hostname Update /etc/hosts with the following:\n127.0.0.1 localhost ::1 localhost 127.0.1.1 linuxmachine.localdomain linuxmachine Set root password # passwd Update /etc/mkinitcpio.conf \u0026amp; generate initrd image Edit /etc/mkinitcpio.conf with the following:\nHOOKS=(base udev autodetect modconf block keymap encrypt resume filesystems keyboard fsck) Then run:\n# mkinitcpio -p linux Install grub # grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=ArchLinux Edit /etc/default/grub so it includes a statement like this:\nGRUB_CMDLINE_LINUX=\u0026quot;cryptdevice=/dev/nvme0n1p4:archcryptroot resume=/dev/nvme0n1p3\u0026quot; Generate final grub configuration:\n# grub-mkconfig -o /boot/grub/grub.cfg Exit \u0026amp; reboot # exit # umount -R /mnt # swapoff -a # reboot To be continued.\n","date":"2023-09-29T00:00:01Z","image":"https://www.jaredszajkowski.com/2023/09/29/arch-linux-install/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/09/29/arch-linux-install/","title":"Arch Linux Install"},{"content":"Hello World Welcome to my website. This is meant to serve as a place for me to publish various posts from my explorations into Arch Linux, data science, quant finance, and other topics.\nThe theme has been adopted from the Hugo Theme Stack produced by Jimmy Cai.\nThis is the only theme that I have found that checks all of the following boxes:\nTheme for the static site generator Hugo Includes modules for archives Includes tags and topics/categories Includes built-in search functionality Simple interface that is easily navigable Highly extensible including modules for image galleries, posts, comment capabilities, etc. It is hosted on GitHub pages. I followed the install instructions that the theme author provided, including using GitHub codespace for editing in the cloud. There are only a few details that I ran into that he did not mention.\nDon\u0026rsquo;t forget to run Hugo to build the site. This creates the public directory, which is where the static site files are located. Make sure to update the branch to be gh-pages under Settings -\u0026gt; Pages -\u0026gt; Build and deployment -\u0026gt; Branch in GitHub. Make sure to remove the public directory from the .gitignore file. Otherwise GitHub will ignore the public directory and your site will show the README.md instead of the Hugo site. The site can be updated either through codespace, or locally as long as Hugo and it\u0026rsquo;s required dependencies have been installed.\nUpdating and pushing changes The simple command after making any changes and to push those updates is as follows:\n$ hugo \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -am \u0026quot;Updating site\u0026quot; \u0026amp;\u0026amp; git push This can be put in a bash script to make it easier. Save the following as git-update.sh:\n1 2 3 4 5 6 #!/bin/bash echo What is the commit message? read message hugo \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -am \u0026#34;$message\u0026#34; \u0026amp;\u0026amp; git push Change permissions:\n$ chmod +x git-update.sh And then execute:\n$ ./git-update.sh References Here\u0026rsquo;s the full list of resources I referenced for deploying Hugo with GitHub pages:\nhttps://www.o11ycloud.com/posts/gh_hugo/ https://github.com/CaiJimmy/hugo-theme-stack https://medium.com/@magstherdev/github-pages-hugo-86ae6bcbadd\n","date":"2023-09-26T00:00:00Z","image":"https://www.jaredszajkowski.com/2023/09/26/hello-world/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/09/26/hello-world/","title":"Hello World"}]