[{"content":"Introduction In this post, I\u0026rsquo;ll cover the implementation of doit to automate the execution of Jupyter notebook files, Python scripts, and building the Hugo static site. Many of the concepts covered below were introduced recently in FINM 32900 - Full-Stack Quantitative Finance. This course emphasized the \u0026ldquo;full stack\u0026rdquo; approach, including the following:\nUse of GitHub Virtual environments Environment variables Use of various data sources (particularly WRDS) Processing/cleaning data GitHub actions Publishing data Restricting access to GitHub hosted sites Motivation The primary motivation for automation came from several realizations:\nSetting directory variables would avoid any issues with managing where the static files were stored locally I wanted to be able to pull updated data, execute Jupyter notebooks, and update the posts within my Hugo site without a manual intervention and processes I like to include the html and PDF exports of the Jupyter notebooks, which required copying the exports to the \u0026ldquo;Public\u0026rdquo; folder of the website I needed a system to build the \u0026ldquo;index.md\u0026rdquo; files that are present in each post directory, and automatically include Python code and functions (again, without copying/pasting or manual processes) dodo.py The dodo.py file in the primary directory is referenced by doit and includes all imports, functions, environment variables, etc. as required to execute the desired code. My dodo.py is broken down as follows:\nImports The inital imports are as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ####################################### ## Import Libraries ####################################### import sys ## Make sure the src folder is in the path sys.path.insert(1, \u0026#34;./src/\u0026#34;) import re import shutil import subprocess import time import yaml from colorama import Fore, Style, init from datetime import datetime from os import environ, getcwd, path from pathlib import Path This first adds the /src/ subdirectory to the path (required later on), and then imports any other required modules. I prefer to sort all imports alphabetically for easy reference and readability.\nPrint PyDoit Text in Green Next, I use the following code to help differentiate the various outputs in the termial when executing doit:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # Code from lines 29-75 referenced from the UChicago # FINM 32900 - Full-Stack Quantitative Finance course # Credit to Jeremy Bejarano # https://github.com/jmbejara ## Custom reporter: Print PyDoit Text in Green # This is helpful because some tasks write to sterr and pollute the output in # the console. I don\u0026#39;t want to mute this output, because this can sometimes # cause issues when, for example, LaTeX hangs on an error and requires # presses on the keyboard before continuing. However, I want to be able # to easily see the task lines printed by PyDoit. I want them to stand out # from among all the other lines printed to the console. from doit.reporter import ConsoleReporter from settings import config ####################################### ## Slurm Configuration ####################################### try: in_slurm = environ[\u0026#34;SLURM_JOB_ID\u0026#34;] is not None except: in_slurm = False class GreenReporter(ConsoleReporter): def write(self, stuff, **kwargs): doit_mark = stuff.split(\u0026#34; \u0026#34;)[0].ljust(2) task = \u0026#34; \u0026#34;.join(stuff.split(\u0026#34; \u0026#34;)[1:]).strip() + \u0026#34; \u0026#34; output = ( Fore.GREEN + doit_mark + f\u0026#34; {path.basename(getcwd())}: \u0026#34; + task + Style.RESET_ALL ) self.outstream.write(output) if not in_slurm: DOIT_CONFIG = { \u0026#34;reporter\u0026#34;: GreenReporter, # other config here... # \u0026#34;cleanforget\u0026#34;: True, # Doit will forget about tasks that have been cleaned. \u0026#34;backend\u0026#34;: \u0026#34;sqlite3\u0026#34;, \u0026#34;dep_file\u0026#34;: \u0026#34;./.doit-db.sqlite\u0026#34;, } else: DOIT_CONFIG = { \u0026#34;backend\u0026#34;: \u0026#34;sqlite3\u0026#34;, \u0026#34;dep_file\u0026#34;: \u0026#34;./.doit-db.sqlite\u0026#34; } init(autoreset=True) Set Directory Variables Next, I establish the variables that reference some of the more important directories and subdirectories in the project:\n1 2 3 4 5 6 7 8 9 10 11 12 ####################################### ## Set directory variables ####################################### BASE_DIR = config(\u0026#34;BASE_DIR\u0026#34;) CONTENT_DIR = config(\u0026#34;CONTENT_DIR\u0026#34;) POSTS_DIR = config(\u0026#34;POSTS_DIR\u0026#34;) PAGES_DIR = config(\u0026#34;PAGES_DIR\u0026#34;) PUBLIC_DIR = config(\u0026#34;PUBLIC_DIR\u0026#34;) SOURCE_DIR = config(\u0026#34;SOURCE_DIR\u0026#34;) DATA_DIR = config(\u0026#34;DATA_DIR\u0026#34;) DATA_MANUAL_DIR = config(\u0026#34;DATA_MANUAL_DIR\u0026#34;) These directory variables are set from the settings.py file in the /src/ directory. Setting these directory variables allows me to reference them at any point later on in the dodo.py file.\nHelper Functions The following are several helper functions that are referenced in the tasks. These are somewhat self explanatory, and are used by the task functions in the next section below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ####################################### ## Helper functions ####################################### def copy_file(origin_path, destination_path, mkdir=True): \u0026#34;\u0026#34;\u0026#34;Create a Python action for copying a file.\u0026#34;\u0026#34;\u0026#34; def _copy_file(): origin = Path(origin_path) dest = Path(destination_path) if mkdir: dest.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(origin, dest) return _copy_file def extract_front_matter(index_path): \u0026#34;\u0026#34;\u0026#34;Extract front matter as a dict from a Hugo index.md file.\u0026#34;\u0026#34;\u0026#34; text = index_path.read_text() match = re.search(r\u0026#34;(?s)^---(.*?)---\u0026#34;, text) if match: return yaml.safe_load(match.group(1)) return {} def notebook_source_hash(notebook_path): \u0026#34;\u0026#34;\u0026#34;Compute a SHA-256 hash of the notebook\u0026#39;s code and markdown cells. This includes all whitespace and comments.\u0026#34;\u0026#34;\u0026#34; import nbformat import hashlib with open(notebook_path, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: nb = nbformat.read(f, as_version=4) relevant_cells = [ cell[\u0026#34;source\u0026#34;] for cell in nb.cells if cell.cell_type in {\u0026#34;code\u0026#34;, \u0026#34;markdown\u0026#34;} ] full_content = \u0026#34;\\n\u0026#34;.join(relevant_cells) return hashlib.sha256(full_content.encode(\u0026#34;utf-8\u0026#34;)).hexdigest() def clean_pdf_export_pngs(subdir, notebook_name): \u0026#34;\u0026#34;\u0026#34;Remove .png files created by nbconvert during PDF export.\u0026#34;\u0026#34;\u0026#34; pattern = f\u0026#34;{notebook_name}_*_*.png\u0026#34; deleted = False for file in subdir.glob(pattern): print(f\u0026#34;🧹 Removing nbconvert temp image: {file}\u0026#34;) file.unlink() deleted = True if not deleted: print(f\u0026#34;✅ No temp PNGs to remove for {notebook_name}\u0026#34;) Tasks Next, we will look at the individual tasks that are being executed by doit.\nThe config task creates the base directories for the Hugo site:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ####################################### ## PyDoit tasks ####################################### def task_config(): \u0026#34;\u0026#34;\u0026#34;Create empty directories for content, page, post, and public if they don\u0026#39;t exist\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [\u0026#34;ipython ./src/settings.py\u0026#34;], \u0026#34;file_dep\u0026#34;: [\u0026#34;./src/settings.py\u0026#34;], \u0026#34;targets\u0026#34;: [CONTENT_DIR, PAGES_DIR, POSTS_DIR, PUBLIC_DIR], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_list_posts_subdirs function is not really necessary, but was used as an initial starting point for when I began building the dodo.py file:\n1 2 3 4 5 6 7 8 9 def task_list_posts_subdirs(): \u0026#34;\u0026#34;\u0026#34;Create a list of the subdirectories of the posts directory\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [\u0026#34;python ./src/list_posts_subdirs.py\u0026#34;], \u0026#34;file_dep\u0026#34;: [\u0026#34;./src/settings.py\u0026#34;], # \u0026#34;targets\u0026#34;: [POSTS_DIR], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_run_post_notebooks function performs the following actions:\nFinds all of the \u0026ldquo;post\u0026rdquo; subdirectories In each \u0026ldquo;post\u0026rdquo; directory, it executes the jupyter notebook file (if found) that has the same name as the post The hash of the non-markdown cells in the notebook is also checked, and if the hash has not changed since the last run, then it skips executing the notebook After the notebook is executed (or not), the log is updated with the date and action 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def task_run_post_notebooks(): \u0026#34;\u0026#34;\u0026#34;Execute notebooks that match their subdirectory names and only when code or markdown content has changed\u0026#34;\u0026#34;\u0026#34; for subdir in POSTS_DIR.iterdir(): if not subdir.is_dir(): continue notebook_path = subdir / f\u0026#34;{subdir.name}.ipynb\u0026#34; if not notebook_path.exists(): continue # ✅ Skip subdirs with no matching notebook hash_file = subdir / f\u0026#34;{subdir.name}.last_source_hash\u0026#34; log_file = subdir / f\u0026#34;{subdir.name}.log\u0026#34; def source_has_changed(path=notebook_path, hash_path=hash_file, log_path=log_file): current_hash = notebook_source_hash(path) timestamp = datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) if hash_path.exists(): old_hash = hash_path.read_text().strip() if current_hash != old_hash: print(f\u0026#34;🔁 Change detected in {path.name}\u0026#34;) return False # needs re-run # ✅ No change → log as skipped with log_path.open(\u0026#34;a\u0026#34;) as log: log.write(f\u0026#34;[{timestamp}] ⏩ Skipped (no changes): {path.name}\\n\u0026#34;) print(f\u0026#34;⏩ No change in hash for {path.name}\u0026#34;) return True # 🆕 No previous hash → must run print(f\u0026#34;🆕 No previous hash found for {path.name}\u0026#34;) return False def run_and_log(path=notebook_path, hash_path=hash_file, log_path=log_file): start_time = time.time() subprocess.run([ \u0026#34;jupyter\u0026#34;, \u0026#34;nbconvert\u0026#34;, \u0026#34;--execute\u0026#34;, \u0026#34;--to\u0026#34;, \u0026#34;notebook\u0026#34;, \u0026#34;--inplace\u0026#34;, \u0026#34;--log-level=ERROR\u0026#34;, str(path) ], check=True) elapsed = round(time.time() - start_time, 2) new_hash = notebook_source_hash(path) hash_path.write_text(new_hash) print(f\u0026#34;✅ Saved new hash for {path.name}\u0026#34;) timestamp = datetime.now().strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) log_msg = f\u0026#34;[{timestamp}] ✅ Executed {path.name} in {elapsed}s\\n\u0026#34; with log_path.open(\u0026#34;a\u0026#34;) as f: f.write(log_msg) print(log_msg.strip()) yield { \u0026#34;name\u0026#34;: subdir.name, \u0026#34;actions\u0026#34;: [run_and_log], \u0026#34;file_dep\u0026#34;: [notebook_path], \u0026#34;uptodate\u0026#34;: [source_has_changed], \u0026#34;verbosity\u0026#34;: 2, } Next, the task_export_post_notebooks function exports the executed jupyter notebook to both HTML and PDF formats.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def task_export_post_notebooks(): \u0026#34;\u0026#34;\u0026#34;Export executed notebooks to HTML and PDF, and clean temp PNGs\u0026#34;\u0026#34;\u0026#34; for subdir in POSTS_DIR.iterdir(): if not subdir.is_dir(): continue notebook_name = subdir.name notebook_path = subdir / f\u0026#34;{notebook_name}.ipynb\u0026#34; html_output = subdir / f\u0026#34;{notebook_name}.html\u0026#34; pdf_output = subdir / f\u0026#34;{notebook_name}.pdf\u0026#34; if not notebook_path.exists(): continue yield { \u0026#34;name\u0026#34;: notebook_name, \u0026#34;actions\u0026#34;: [ f\u0026#34;jupyter nbconvert --to=html --log-level=WARN --output={html_output} {notebook_path}\u0026#34;, f\u0026#34;jupyter nbconvert --to=pdf --log-level=WARN --output={pdf_output} {notebook_path}\u0026#34;, (clean_pdf_export_pngs, [subdir, notebook_name]) ], \u0026#34;file_dep\u0026#34;: [notebook_path], \u0026#34;targets\u0026#34;: [html_output, pdf_output], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_build_post_indices builds each index.md file within each \u0026ldquo;post\u0026rdquo; directory. It looks for an index_temp.md and an index_dep.txt file, which contains the dependencies required to build the index.md file. The dependencies are established within the jupyter notebook for each post, and the index_dep.txt file is also updated when the notebook is executed.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def task_build_post_indices(): \u0026#34;\u0026#34;\u0026#34;Run build_index.py in each post subdirectory to generate index.md\u0026#34;\u0026#34;\u0026#34; script_path = SOURCE_DIR / \u0026#34;build_index.py\u0026#34; for subdir in POSTS_DIR.iterdir(): if subdir.is_dir() and (subdir / \u0026#34;index_temp.md\u0026#34;).exists(): def run_script(subdir=subdir): subprocess.run( [\u0026#34;python\u0026#34;, str(script_path)], cwd=subdir, check=True ) yield { \u0026#34;name\u0026#34;: subdir.name, \u0026#34;actions\u0026#34;: [run_script], \u0026#34;file_dep\u0026#34;: [ subdir / \u0026#34;index_temp.md\u0026#34;, subdir / \u0026#34;index_dep.txt\u0026#34;, script_path, ], \u0026#34;targets\u0026#34;: [subdir / \u0026#34;index.md\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } Here\u0026rsquo;s an example of when the index_dep.txt file is generated, and then updated with a markdown file dependency is generated with the export_track_md_deps function:\n1 2 3 4 5 6 7 8 9 10 11 12 # Create file to track markdown dependencies dep_file = Path(\u0026#34;index_dep.txt\u0026#34;) dep_file.write_text(\u0026#34;\u0026#34;) # Copy this \u0026lt;!-- INSERT_01_VIX_Stats_By_Year_HERE --\u0026gt; to index_temp.md export_track_md_deps(dep_file=dep_file, md_filename=\u0026#34;01_VIX_Stats_By_Year.md\u0026#34;, content=vix_stats_by_year.to_markdown(floatfmt=\u0026#34;.2f\u0026#34;)) # Copy this \u0026lt;!-- INSERT_02_VVIX_DF_Info_HERE --\u0026gt; to index_temp.md export_track_md_deps(dep_file=dep_file, md_filename=\u0026#34;02_VVIX_DF_Info.md\u0026#34;, content=df_info_markdown(vix)) # Copy this \u0026lt;!-- INSERT_11_Net_Profit_Percent_HERE --\u0026gt; to index_temp.md export_track_md_deps(dep_file=dep_file, md_filename=\u0026#34;11_Net_Profit_Percent.md\u0026#34;, content=net_profit_percent_str) Moving on, the task_clean_public removes the public directory within the static site. This is necessary to clean out any erroneous files or directories that are changed when the site is rebuilt.\n1 2 3 4 5 6 7 8 9 10 11 12 13 def task_clean_public(): \u0026#34;\u0026#34;\u0026#34;Remove the Hugo public directory before rebuilding the site.\u0026#34;\u0026#34;\u0026#34; def remove_public(): if PUBLIC_DIR.exists(): shutil.rmtree(PUBLIC_DIR) print(f\u0026#34;🧹 Deleted {PUBLIC_DIR}\u0026#34;) else: print(f\u0026#34;ℹ️ {PUBLIC_DIR} does not exist, nothing to delete.\u0026#34;) return { \u0026#34;actions\u0026#34;: [remove_public], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_build_site builds the Hugo static site.\n1 2 3 4 5 6 7 8 def task_build_site(): \u0026#34;\u0026#34;\u0026#34;Build the Hugo static site\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;actions\u0026#34;: [\u0026#34;hugo\u0026#34;], \u0026#34;task_dep\u0026#34;: [\u0026#34;clean_public\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_copy_notebook_exports copies the HTML and PDF exports generated above to the public folder. This is necessary due to how Hugo handles HTML and PDF files and excludes those when generating the static site public directories and files.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def task_copy_notebook_exports(): \u0026#34;\u0026#34;\u0026#34;Copy notebook HTML exports into the correct Hugo public/ date-based folders\u0026#34;\u0026#34;\u0026#34; for subdir in POSTS_DIR.iterdir(): if subdir.is_dir(): html_file = subdir / f\u0026#34;{subdir.name}.html\u0026#34; index_md = subdir / \u0026#34;index.md\u0026#34; if not html_file.exists() or not index_md.exists(): continue # Extract slug and date from front matter front_matter = extract_front_matter(index_md) slug = front_matter.get(\u0026#34;slug\u0026#34;, subdir.name) date_str = front_matter.get(\u0026#34;date\u0026#34;) if not date_str: continue # Format path like: public/YYYY/MM/DD/slug/ date_obj = datetime.fromisoformat(date_str) public_path = PUBLIC_DIR / f\u0026#34;{date_obj:%Y/%m/%d}\u0026#34; / slug target_path = public_path / f\u0026#34;{slug}.html\u0026#34; def copy_html(src=html_file, dest=target_path): dest.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(src, dest) print(f\u0026#34;✅ Copied {src} → {dest}\u0026#34;) yield { \u0026#34;name\u0026#34;: subdir.name, \u0026#34;actions\u0026#34;: [copy_html], \u0026#34;file_dep\u0026#34;: [html_file, index_md], \u0026#34;targets\u0026#34;: [target_path], \u0026#34;task_dep\u0026#34;: [\u0026#34;build_site\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } The task_create_schwab_callback creates a simple HTML file that will read the authorization code when using oauth with the Schwab API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def task_create_schwab_callback(): \u0026#34;\u0026#34;\u0026#34;Create a Schwab callback URL by creating /public/schwab_callback/index.html and placing the html code in it\u0026#34;\u0026#34;\u0026#34; def create_callback(): callback_path = PUBLIC_DIR / \u0026#34;schwab_callback\u0026#34; / \u0026#34;index.html\u0026#34; callback_path.parent.mkdir(parents=True, exist_ok=True) html = \u0026#34;\u0026#34;\u0026#34;\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Schwab OAuth Code\u0026lt;/title\u0026gt; \u0026lt;script\u0026gt; const params = new URLSearchParams(window.location.search); const code = params.get(\u0026#34;code\u0026#34;); document.write(\u0026#34;\u0026lt;h1\u0026gt;Authorization Code:\u0026lt;/h1\u0026gt;\u0026lt;p\u0026gt;\u0026#34; + code + \u0026#34;\u0026lt;/p\u0026gt;\u0026#34;); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\u0026#34;\u0026#34;\u0026#34; with open(callback_path, \u0026#34;w\u0026#34;) as f: f.write(html) print(f\u0026#34;✅ Created Schwab callback page at {callback_path}\u0026#34;) return { \u0026#34;actions\u0026#34;: [create_callback], \u0026#34;task_dep\u0026#34;: [\u0026#34;copy_notebook_exports\u0026#34;, \u0026#34;clean_public\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } Finally, the task_deploy_site adds any new files, commits the changes, prompts for a message, and pushes the updates to GitHub.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def task_deploy_site(): \u0026#34;\u0026#34;\u0026#34;Prompt for a commit message and push to GitHub\u0026#34;\u0026#34;\u0026#34; def commit_and_push(): message = input(\u0026#34;What is the commit message? \u0026#34;) if not message.strip(): print(\u0026#34;❌ Commit message cannot be empty.\u0026#34;) return 1 # signal failure import subprocess subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;add\u0026#34;, \u0026#34;.\u0026#34;], check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;commit\u0026#34;, \u0026#34;-am\u0026#34;, message], check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;push\u0026#34;], check=True) print(\u0026#34;✅ Pushed to GitHub.\u0026#34;) return { \u0026#34;actions\u0026#34;: [commit_and_push], \u0026#34;task_dep\u0026#34;: [\u0026#34;create_schwab_callback\u0026#34;], \u0026#34;verbosity\u0026#34;: 2, \u0026#34;clean\u0026#34;: [], # Don\u0026#39;t clean these files by default. } As (likely) expected, a good portion of the above code was generated by ChatGPT - somewhere ~ 50%-75%. The balance was generated by myself or modified using the base code provided. Importantly, the general idea of automating the entire process within Hugo and processing the post subdirectories is original (as far as I know).\nFinally, the complete dodo.py and settings.py files are available in the jupyter notebook / HTML / PDF exports linked below.\nExecuting doit To execute doit, simply run:\n$ doit in the terminal after changing to the high level directory.\nAlternatively, you can list the individual tasks with:\n$ doit list and then execute individually, such as:\n$ doit build_post_indices And finally, doit can be forced to execute all tasks with:\n$ doit --always or an individual task with:\n$ doit --always build_post_indices References https://pydoit.org/ https://github.com/jmbejara Code The jupyter notebook with the functions and all other code is available here. The html export of the jupyter notebook is available here. The pdf export of the jupyter notebook is available here.\n","date":"2025-06-29T00:00:01Z","image":"https://www.jaredszajkowski.com/2025/06/29/automating-execution-jupyter-notebook-files-python-scripts-hugo-static-site-generation/cover.jpg","permalink":"https://www.jaredszajkowski.com/2025/06/29/automating-execution-jupyter-notebook-files-python-scripts-hugo-static-site-generation/","title":"Automating Execution of Jupyter Notebook Files, Python Scripts, and Hugo Static Site Generation"},{"content":"Trading History I have began trading based on the ideas from part 2, opening positions during the VIX spikes and closing them as volatility comes back down. The executed trades, closed positions, and open positions listed below are all automated updates from the transaction history exports from Schwab. The exported CSV files are available in the GitHub repository.\nTrades Executed Here are the trades executed to date, with any comments related to execution, market sentiment, reason for opening/closing position, VIX level, etc.\nTrade_Date Action Symbol Quantity Price Fees \u0026amp; Comm Amount Approx_VIX_Level Comments 2024-08-05 00:00:00 Buy to Open VIX 09/18/2024 34.00 P 1 10.95 1.08 1096.08 34.33 nan 2024-08-21 00:00:00 Sell to Close VIX 09/18/2024 34.00 P 1 17.95 1.08 1793.92 16.50 nan 2024-08-05 00:00:00 Buy to Open VIX 10/16/2024 40.00 P 1 16.35 1.08 1636.08 42.71 nan 2024-09-18 00:00:00 Sell to Close VIX 10/16/2024 40.00 P 1 21.54 1.08 2152.92 18.85 nan 2024-08-07 00:00:00 Buy to Open VIX 11/20/2024 25.00 P 2 5.90 2.16 1182.16 27.11 nan 2024-11-04 00:00:00 Sell to Close VIX 11/20/2024 25.00 P 2 6.10 2.16 1217.84 22.43 nan 2024-08-06 00:00:00 Buy to Open VIX 12/18/2024 30.00 P 1 10.25 1.08 1026.08 32.27 nan 2024-11-27 00:00:00 Sell to Close VIX 12/18/2024 30.00 P 1 14.95 1.08 1493.92 14.04 nan 2025-03-04 00:00:00 Buy to Open VIX 04/16/2025 25.00 P 5 5.65 5.40 2830.40 25.75 nan 2025-03-24 00:00:00 Sell to Close VIX 04/16/2025 25.00 P 5 7.00 5.40 3494.60 18.01 nan 2025-03-10 00:00:00 Buy to Open VIX 05/21/2025 26.00 P 5 7.10 5.40 3555.40 27.54 Missed opportunity to close position for 20% profit before vol spike in early April 2025 2025-04-04 00:00:00 Buy to Open VIX 05/21/2025 26.00 P 10 4.10 10.81 4110.81 38.88 Averaged down on existing position 2025-04-24 00:00:00 Sell to Close VIX 05/21/2025 26.00 P 7 3.50 7.57 2442.43 27.37 Sold half of position due to vol spike concerns and theta 2025-05-02 00:00:00 Sell to Close VIX 05/21/2025 26.00 P 4 4.35 4.32 1735.68 22.73 Sold half of remaining position due to vol spike concerns and theta 2025-05-07 00:00:00 Sell to Close VIX 05/21/2025 26.00 P 4 3.55 4.32 1415.68 24.49 Closed position ahead of Fed’s (Powell’s) comments 2025-04-04 00:00:00 Buy to Open VIX 05/21/2025 37.00 P 3 13.20 3.24 3963.24 36.46 nan 2025-05-07 00:00:00 Sell to Close VIX 05/21/2025 37.00 P 3 13.75 3.24 4121.76 24.51 Closed position ahead of Fed’s (Powell’s) comments 2025-04-08 00:00:00 Buy to Open VIX 05/21/2025 50.00 P 2 21.15 2.16 4232.16 nan nan 2025-04-24 00:00:00 Sell to Close VIX 05/21/2025 50.00 P 1 25.30 1.08 2528.92 nan nan 2025-04-25 00:00:00 Sell to Close VIX 05/21/2025 50.00 P 1 25.65 1.08 2563.92 nan nan 2025-04-03 00:00:00 Buy to Open VIX 06/18/2025 27.00 P 8 7.05 8.65 5648.65 27.62 nan 2025-04-08 00:00:00 Buy to Open VIX 06/18/2025 27.00 P 4 4.55 4.32 1824.32 55.44 Averaged down on existing position 2025-05-12 00:00:00 Sell to Close VIX 06/18/2025 27.00 P 6 7.55 6.49 4523.51 19.05 Market up on positive news of lowering tariffs with China; VIX down 15%, VVIX down 10% 2025-05-12 00:00:00 Sell to Close VIX 06/18/2025 27.00 P 6 7.40 6.49 4433.51 19.47 Market up on positive news of lowering tariffs with China; VIX down 15%, VVIX down 10% 2025-04-04 00:00:00 Buy to Open VIX 06/18/2025 36.00 P 3 13.40 3.24 4023.24 36.61 nan 2025-05-12 00:00:00 Sell to Close VIX 06/18/2025 36.00 P 3 16.00 3.24 4796.76 19.14 Market up on positive news of lowering tariffs with China; VIX down 15%, VVIX down 10% 2025-04-07 00:00:00 Buy to Open VIX 06/18/2025 45.00 P 2 18.85 2.16 3772.16 53.65 nan 2025-05-12 00:00:00 Sell to Close VIX 06/18/2025 45.00 P 2 25.00 2.16 4997.84 19.24 Market up on positive news of lowering tariffs with China; VIX down 15%, VVIX down 10% 2025-04-03 00:00:00 Buy to Open VIX 07/16/2025 29.00 P 5 8.55 5.40 4280.40 29.03 nan 2025-05-13 00:00:00 Sell to Close VIX 07/16/2025 29.00 P 3 10.40 3.24 3116.76 17.72 nan 2025-05-13 00:00:00 Sell to Close VIX 07/16/2025 29.00 P 2 10.30 2.16 2057.84 17.68 nan 2025-04-04 00:00:00 Buy to Open VIX 07/16/2025 36.00 P 3 13.80 3.24 4143.24 36.95 nan 2025-05-13 00:00:00 Sell to Close VIX 07/16/2025 36.00 P 1 17.00 1.08 1698.92 17.79 nan 2025-05-13 00:00:00 Sell to Close VIX 07/16/2025 36.00 P 2 16.90 2.16 3377.84 17.72 nan 2025-04-07 00:00:00 Buy to Open VIX 07/16/2025 45.00 P 2 21.55 2.16 4312.16 46.17 nan 2025-05-13 00:00:00 Sell to Close VIX 07/16/2025 45.00 P 2 25.65 2.16 5127.84 17.96 nan 2025-04-07 00:00:00 Buy to Open VIX 08/20/2025 45.00 P 2 21.75 2.16 4352.16 49.07 nan 2025-05-13 00:00:00 Sell to Close VIX 08/20/2025 45.00 P 2 25.40 2.16 5077.84 18.06 nan 2025-06-26 00:00:00 Buy to Open VIX 09/17/2025 20.00 C 10 3.00 10.81 3010.81 16.37 Opened long dated call position; VIX level at 4th historical decile 2025-06-26 00:00:00 Buy to Open VIX 10/22/2025 22.00 C 10 2.94 10.81 2950.81 16.43 Opened long dated call position; VIX level at 4th historical decile 2025-07-17 00:00:00 Buy to Open VIX 10/22/2025 23.00 C 10 2.75 10.81 2760.81 16.86 Continued low volatility, opened long dated call position; VIX level at 4th historical decile Volatility In August 2024 Plot with VIX high/low, trade side, VIX option, and VIX level at trade date/time:\nClosed positions:\nSymbol Amount_Buy Quantity_Buy Amount_Sell Quantity_Sell Realized_PnL Percent_PnL VIX 09/18/2024 34.00 P 1096.08 1 1793.92 1 697.84 0.64 VIX 10/16/2024 40.00 P 1636.08 1 2152.92 1 516.84 0.32 VIX 11/20/2024 25.00 P 1182.16 2 1217.84 2 35.68 0.03 VIX 12/18/2024 30.00 P 1026.08 1 1493.92 1 467.84 0.46 Open positions:\nSymbol Amount_Buy Quantity_Buy Total Opened Position Market Value: $4,940.40 Total Closed Position Market Value: $6,658.60 Net Profit/Loss: $1,718.20 Percent Profit/Loss: 34.78%\nVolatility In March 2025 Plot with VIX high/low, trade side, VIX option, and VIX level at trade date/time:\nClosed positions:\nSymbol Amount_Buy Quantity_Buy Amount_Sell Quantity_Sell Realized_PnL Percent_PnL VIX 04/16/2025 25.00 P 2830.40 5 3494.60 5 664.20 0.23 Open positions:\nSymbol Amount_Buy Quantity_Buy Total Opened Position Market Value: $2,830.40 Total Closed Position Market Value: $3,494.60 Net Profit/Loss: $664.20 Percent Profit/Loss: 23.47%\nVolatility In April 2025 Plot with VIX high/low, trade side, VIX option, and VIX level at trade date/time:\nClosed positions:\nSymbol Amount_Buy Quantity_Buy Amount_Sell Quantity_Sell Realized_PnL Percent_PnL VIX 05/21/2025 26.00 P 7666.21 15 5593.79 15 -2072.42 -0.27 VIX 05/21/2025 37.00 P 3963.24 3 4121.76 3 158.52 0.04 VIX 05/21/2025 50.00 P 4232.16 2 5092.84 2 860.68 0.20 VIX 06/18/2025 27.00 P 7472.97 12 8957.02 12 1484.05 0.20 VIX 06/18/2025 36.00 P 4023.24 3 4796.76 3 773.52 0.19 VIX 06/18/2025 45.00 P 3772.16 2 4997.84 2 1225.68 0.32 VIX 07/16/2025 29.00 P 4280.40 5 5174.60 5 894.20 0.21 VIX 07/16/2025 36.00 P 4143.24 3 5076.76 3 933.52 0.23 VIX 07/16/2025 45.00 P 4312.16 2 5127.84 2 815.68 0.19 VIX 08/20/2025 45.00 P 4352.16 2 5077.84 2 725.68 0.17 Open positions:\nSymbol Amount_Buy Quantity_Buy Total Opened Position Market Value: $48,217.94 Total Closed Position Market Value: $54,017.05 Net Profit/Loss: $5,799.11 Percent Profit/Loss: 12.03%\nLow Volatility In June 2025 Plot with VIX high/low, trade side, VIX option, and VIX level at trade date/time:\nClosed positions:\nSymbol Amount_Buy Quantity_Buy Amount_Sell Quantity_Sell Realized_PnL Percent_PnL Open positions:\nSymbol Amount_Buy Quantity_Buy VIX 09/17/2025 20.00 C 3010.81 10 VIX 10/22/2025 22.00 C 2950.81 10 VIX 10/22/2025 23.00 C 2760.81 10 Total Opened Position Market Value: $0.00 Total Closed Position Market Value: $0.00 Net Profit/Loss: $0.00 Percent Profit/Loss: nan%\nComplete Trade History Total Opened Position Market Value: $55,988.74 Total Closed Position Market Value: $64,170.25 Net Profit/Loss: $8,181.51 Percent Profit/Loss: 14.61%\nReferences https://www.cboe.com/tradable_products/vix/ https://github.com/ranaroussi/yfinance Code Note: The files below are identical to those linked in part 1 and part 2.\nThe jupyter notebook with the functions and all other code is available here. The html export of the jupyter notebook is available here. The pdf export of the jupyter notebook is available here.\n","date":"2025-03-03T00:00:01Z","image":"https://www.jaredszajkowski.com/2025/03/03/investigating-a-vix-trading-signal-part-3-trading/cover.jpg","permalink":"https://www.jaredszajkowski.com/2025/03/03/investigating-a-vix-trading-signal-part-3-trading/","title":"Investigating A VIX Trading Signal, Part 3: Trading"},{"content":"Investigating A Signal Continuing from where we left off in part 1, we will now consider the idea of a spike level in the VIX and how we might use a spike level to generate a signal. These elevated levels usually occur during market sell-off events or longer term drawdowns in the S\u0026amp;P 500. Sometimes the VIX reverts to recent levels after a spike, but other times levels remain elevated for weeks or even months.\nDetermining A Spike Level We will start the 10 day simple moving average (SMA) of the daily high level to get an idea of what is happening recently with the VIX. We\u0026rsquo;ll then pick an arbitrary spike level (25% above the 10 day SMA), and our signal is generated if the VIX hits a level that is above the spike threshold.\nThe idea is that the 10 day SMA will smooth out the recent short term volatility in the VIX, and therefore any gradual increases in the VIX are not interpreted as spike events.\nWe also will generate the 20 and 50 day SMAs for reference, and again to see what is happening with the level of the VIX over slightly longer timeframes.\nHere\u0026rsquo;s the code for the above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # Define the spike multiplier for detecting significant spikes spike_level = 1.25 # ========================= # Simple Moving Averages (SMA) # ========================= # Calculate 10-period SMA of \u0026#39;High\u0026#39; vix[\u0026#39;High_SMA_10\u0026#39;] = vix[\u0026#39;High\u0026#39;].rolling(window=10).mean() # Shift the 10-period SMA by 1 to compare with current \u0026#39;High\u0026#39; vix[\u0026#39;High_SMA_10_Shift\u0026#39;] = vix[\u0026#39;High_SMA_10\u0026#39;].shift(1) # Calculate the spike level based on shifted SMA and spike multiplier vix[\u0026#39;Spike_Level_SMA\u0026#39;] = vix[\u0026#39;High_SMA_10_Shift\u0026#39;] * spike_level # Calculate 20-period SMA of \u0026#39;High\u0026#39; vix[\u0026#39;High_SMA_20\u0026#39;] = vix[\u0026#39;High\u0026#39;].rolling(window=20).mean() # Determine if \u0026#39;High\u0026#39; exceeds the spike level (indicates a spike) vix[\u0026#39;Spike_SMA\u0026#39;] = vix[\u0026#39;High\u0026#39;] \u0026gt;= vix[\u0026#39;Spike_Level_SMA\u0026#39;] # Calculate 50-period SMA of \u0026#39;High\u0026#39; for trend analysis vix[\u0026#39;High_SMA_50\u0026#39;] = vix[\u0026#39;High\u0026#39;].rolling(window=50).mean() # ========================= # Exponential Moving Averages (EMA) # ========================= # Calculate 10-period EMA of \u0026#39;High\u0026#39; vix[\u0026#39;High_EMA_10\u0026#39;] = vix[\u0026#39;High\u0026#39;].ewm(span=10, adjust=False).mean() # Shift the 10-period EMA by 1 to compare with current \u0026#39;High\u0026#39; vix[\u0026#39;High_EMA_10_Shift\u0026#39;] = vix[\u0026#39;High_EMA_10\u0026#39;].shift(1) # Calculate the spike level based on shifted EMA and spike multiplier vix[\u0026#39;Spike_Level_EMA\u0026#39;] = vix[\u0026#39;High_EMA_10_Shift\u0026#39;] * spike_level # Calculate 20-period EMA of \u0026#39;High\u0026#39; vix[\u0026#39;High_EMA_20\u0026#39;] = vix[\u0026#39;High\u0026#39;].ewm(span=20, adjust=False).mean() # Determine if \u0026#39;High\u0026#39; exceeds the spike level (indicates a spike) vix[\u0026#39;Spike_EMA\u0026#39;] = vix[\u0026#39;High\u0026#39;] \u0026gt;= vix[\u0026#39;Spike_Level_EMA\u0026#39;] # Calculate 50-period EMA of \u0026#39;High\u0026#39; for trend analysis vix[\u0026#39;High_EMA_50\u0026#39;] = vix[\u0026#39;High\u0026#39;].ewm(span=50, adjust=False).mean() For this exercise, we will use simple moving averages.\nSpike Counts (Signals) By Year To investigate the number of spike events (or signals) that we receive on a yearly basis, we can run the following:\n1 2 3 4 5 6 7 8 9 10 # Ensure the index is a DatetimeIndex vix.index = pd.to_datetime(vix.index) # Create a new column for the year extracted from the date index vix[\u0026#39;Year\u0026#39;] = vix.index.year # Group by year and the \u0026#34;Spike_SMA\u0026#34; and \u0026#34;Spike_EMA\u0026#34; columns, then count occurrences spike_count_SMA = vix.groupby([\u0026#39;Year\u0026#39;, \u0026#39;Spike_SMA\u0026#39;]).size().unstack(fill_value=0) display(spike_count_SMA) Which gives us the following:\nYear False True 1990 248 5 1991 249 4 1992 250 4 1993 251 2 1994 243 9 1995 252 0 1996 248 6 1997 247 6 1998 243 9 1999 250 2 2000 248 4 2001 240 8 2002 248 4 2003 251 1 2004 250 2 2005 250 2 2006 242 9 2007 239 12 2008 238 15 2009 249 3 2010 239 13 2011 240 12 2012 248 2 2013 249 3 2014 235 17 2015 240 12 2016 234 18 2017 244 7 2018 228 23 2019 241 11 2020 224 29 2021 235 17 2022 239 12 2023 246 4 2024 237 15 2025 123 12 And the plot to aid with visualization. Based on the plot, it seems as though volatility has increased since the early 2000\u0026rsquo;s:\nSpike Counts (Signals) Plots By Year The most recent yearly plots are shown below for when signals are generated. The images for the previous years are linked below.\nSpike/Signals, 1990 Spike/Signals, 1991 Spike/Signals, 1992 Spike/Signals, 1993 Spike/Signals, 1994 Spike/Signals, 1995 Spike/Signals, 1996 Spike/Signals, 1997 Spike/Signals, 1998 Spike/Signals, 1999 Spike/Signals, 2000 Spike/Signals, 2001 Spike/Signals, 2002 Spike/Signals, 2003 Spike/Signals, 2004 Spike/Signals, 2005 Spike/Signals, 2006 Spike/Signals, 2007 Spike/Signals, 2008 Spike/Signals, 2009 Spike/Signals, 2010 Spike/Signals, 2011 Spike/Signals, 2012 Spike/Signals, 2013 Spike/Signals, 2014 Spike/Signals, 2015 Spike/Signals, 2016 Spike/Signals, 2017 Spike/Signals, 2018 Spike/Signals, 2019\n2020 2021 2022 2023 2024 2025 For comparison with the VVIX plot for 2025:\nSpike Counts (Signals) Plots By Decade And here are the plots for the signals generated over the past 3 decades:\n1990 - 1994 1995 - 1999 2000 - 2004 2005 - 2009 2010 - 2014 2015 - 2019 2020 - 2024 2025 - Present For comparison with the VVIX plot for 2025:\nReferences https://www.cboe.com/tradable_products/vix/ https://github.com/ranaroussi/yfinance Code Note: The files below are identical to those linked in part 1.\nThe jupyter notebook with the functions and all other code is available here. The html export of the jupyter notebook is available here. The pdf export of the jupyter notebook is available here.\n","date":"2025-03-02T00:00:01Z","image":"https://www.jaredszajkowski.com/2025/03/02/investigating-a-vix-trading-signal-part-2-finding-a-signal/cover.jpg","permalink":"https://www.jaredszajkowski.com/2025/03/02/investigating-a-vix-trading-signal-part-2-finding-a-signal/","title":"Investigating A VIX Trading Signal, Part 2: Finding A Signal"},{"content":"Introduction From the CBOE VIX website:\n\u0026ldquo;Cboe Global Markets revolutionized investing with the creation of the Cboe Volatility Index® (VIX® Index), the first benchmark index to measure the market’s expectation of future volatility. The VIX Index is based on options of the S\u0026amp;P 500® Index, considered the leading indicator of the broad U.S. stock market. The VIX Index is recognized as the world’s premier gauge of U.S. equity market volatility.\u0026rdquo;\nIn this tutorial, we will investigate finding a signal to use as a basis to trade the VIX.\nVIX Data I don\u0026rsquo;t have access to data for the VIX through Nasdaq Data Link or any other data source, but for our purposes Yahoo Finance is sufficient. Using the yfinance python module, we can pull what we need and quicky dump it to excel to retain it for future use.\nPython Functions Here are the functions needed for this project:\ncalc_vix_trade_pnl: Calculates the profit/loss from VIX options trades. df_info: A simple function to display the information about a DataFrame and the first five rows and last five rows. df_info_markdown: Similar to the df_info function above, except that it coverts the output to markdown. export_track_md_deps: Exports various text outputs to markdown files, which are included in the index.md file created when building the site with Hugo. load_data: Load data from a CSV, Excel, or Pickle file into a pandas DataFrame. pandas_set_decimal_places: Set the number of decimal places displayed for floating-point numbers in pandas. plot_price: Plot the price data from a DataFrame for a specified date range and columns. plot_stats: Generate a scatter plot for the mean OHLC prices. plot_vix_with_trades: Plot the VIX daily high and low prices, along with the VIX spikes, and trades. yf_pull_data: Download daily price data from Yahoo Finance and export it. Data Overview (VIX) Acquire CBOE Volatility Index (VIX) Data First, let\u0026rsquo;s get the data:\n1 2 3 4 5 6 7 8 9 yf_pull_data( base_directory=DATA_DIR, ticker=\u0026#34;^VIX\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) Load Data - VIX Now that we have the data, let\u0026rsquo;s load it up and take a look:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Set decimal places pandas_set_decimal_places(2) # VIX vix = load_data( base_directory=DATA_DIR, ticker=\u0026#34;^VIX\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, ) # Set \u0026#39;Date\u0026#39; column as datetime vix[\u0026#39;Date\u0026#39;] = pd.to_datetime(vix[\u0026#39;Date\u0026#39;]) # Drop \u0026#39;Volume\u0026#39; vix.drop(columns = {\u0026#39;Volume\u0026#39;}, inplace = True) # Set Date as index vix.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Check to see if there are any NaN values vix[vix[\u0026#39;High\u0026#39;].isna()] # Forward fill to clean up missing data vix[\u0026#39;High\u0026#39;] = vix[\u0026#39;High\u0026#39;].ffill() DataFrame Info - VIX Now, running:\n1 df_info(vix) Gives us the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 The columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 8952 entries, 1990-01-02 to 2025-07-18 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Close 8952 non-null float64 1 High 8952 non-null float64 2 Low 8952 non-null float64 3 Open 8952 non-null float64 dtypes: float64(4) memory usage: 349.7 KB The first 5 rows are:\nDate Close High Low Open 1990-01-02 00:00:00 17.24 17.24 17.24 17.24 1990-01-03 00:00:00 18.19 18.19 18.19 18.19 1990-01-04 00:00:00 19.22 19.22 19.22 19.22 1990-01-05 00:00:00 20.11 20.11 20.11 20.11 1990-01-08 00:00:00 20.26 20.26 20.26 20.26 The last 5 rows are:\nDate Close High Low Open 2025-07-14 00:00:00 17.20 17.85 16.91 17.73 2025-07-15 00:00:00 17.38 17.39 16.56 16.89 2025-07-16 00:00:00 17.16 19.48 16.69 17.66 2025-07-17 00:00:00 16.52 17.37 16.48 17.16 2025-07-18 00:00:00 16.41 16.81 16.28 16.43 Statistics - VIX Some interesting statistics jump out at us when we look at the mean, standard deviation, minimum, and maximum values for the full dataset. The following code:\n1 2 3 4 5 6 7 8 9 10 vix_stats = vix.describe() num_std = [-1, 0, 1, 2, 3, 4, 5] for num in num_std: vix_stats.loc[f\u0026#34;mean + {num} std\u0026#34;] = { \u0026#39;Open\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Open\u0026#39;] + num * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Open\u0026#39;], \u0026#39;High\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;High\u0026#39;] + num * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;High\u0026#39;], \u0026#39;Low\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Low\u0026#39;] + num * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Low\u0026#39;], \u0026#39;Close\u0026#39;: vix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Close\u0026#39;] + num * vix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Close\u0026#39;], } display(vix_stats) Gives us:\nClose High Low Open count 8952.00 8952.00 8952.00 8952.00 mean 19.49 20.40 18.82 19.58 std 7.82 8.38 7.38 7.90 min 9.14 9.31 8.56 9.01 25% 13.88 14.55 13.42 13.94 50% 17.65 18.38 17.07 17.70 75% 22.81 23.81 22.13 22.96 max 82.69 89.53 72.76 82.69 mean + -1 std 11.66 12.02 11.44 11.68 mean + 0 std 19.49 20.40 18.82 19.58 mean + 1 std 27.31 28.78 26.20 27.48 mean + 2 std 35.14 37.17 33.57 35.38 mean + 3 std 42.96 45.55 40.95 43.28 mean + 4 std 50.79 53.93 48.33 51.18 mean + 5 std 58.61 62.31 55.71 59.08 We can also run the statistics individually for each year:\n1 2 3 4 5 6 7 8 # Group by year and calculate mean and std for OHLC vix_stats_by_year = vix.groupby(vix.index.year)[[\u0026#34;Open\u0026#34;, \u0026#34;High\u0026#34;, \u0026#34;Low\u0026#34;, \u0026#34;Close\u0026#34;]].agg([\u0026#34;mean\u0026#34;, \u0026#34;std\u0026#34;]) # Flatten the column MultiIndex vix_stats_by_year.columns = [\u0026#39;_\u0026#39;.join(col).strip() for col in vix_stats_by_year.columns.values] vix_stats_by_year.index.name = \u0026#34;Year\u0026#34; display(vix_stats_by_year) Gives us:\nYear Open_mean Open_std Open_min Open_max High_mean High_std High_min High_max Low_mean Low_std Low_min Low_max Close_mean Close_std Close_min Close_max 1990 23.06 4.74 14.72 36.47 23.06 4.74 14.72 36.47 23.06 4.74 14.72 36.47 23.06 4.74 14.72 36.47 1991 18.38 3.68 13.95 36.20 18.38 3.68 13.95 36.20 18.38 3.68 13.95 36.20 18.38 3.68 13.95 36.20 1992 15.23 2.26 10.29 20.67 16.03 2.19 11.90 25.13 14.85 2.14 10.29 19.67 15.45 2.12 11.51 21.02 1993 12.70 1.37 9.18 16.20 13.34 1.40 9.55 18.31 12.25 1.28 8.89 15.77 12.69 1.33 9.31 17.30 1994 13.79 2.06 9.86 23.61 14.58 2.28 10.31 28.30 13.38 1.99 9.59 23.61 13.93 2.07 9.94 23.87 1995 12.27 1.03 10.29 15.79 12.93 1.07 10.95 16.99 11.96 0.98 10.06 14.97 12.39 0.97 10.36 15.74 1996 16.31 1.92 11.24 23.90 16.99 2.12 12.29 27.05 15.94 1.82 11.11 21.43 16.44 1.94 12.00 21.99 1997 22.43 4.33 16.67 45.69 23.11 4.56 18.02 48.64 21.85 3.98 16.36 36.43 22.38 4.14 17.09 38.20 1998 25.68 6.96 16.42 47.95 26.61 7.36 16.50 49.53 24.89 6.58 16.10 45.58 25.60 6.86 16.23 45.74 1999 24.39 2.90 18.05 32.62 25.20 3.01 18.48 33.66 23.75 2.76 17.07 31.13 24.37 2.88 17.42 32.98 2000 23.41 3.43 16.81 33.70 24.10 3.66 17.06 34.31 22.75 3.19 16.28 30.56 23.32 3.41 16.53 33.49 2001 26.04 4.98 19.21 48.93 26.64 5.19 19.37 49.35 25.22 4.61 18.74 42.66 25.75 4.78 18.76 43.74 2002 27.53 7.03 17.23 48.17 28.28 7.25 17.51 48.46 26.60 6.64 17.02 42.05 27.29 6.91 17.40 45.08 2003 22.21 5.31 15.59 35.21 22.61 5.35 16.19 35.66 21.64 5.18 14.66 33.99 21.98 5.24 15.58 34.69 2004 15.59 1.93 11.41 21.06 16.05 2.02 11.64 22.67 15.05 1.79 11.14 20.61 15.48 1.92 11.23 21.58 2005 12.84 1.44 10.23 18.33 13.28 1.59 10.48 18.59 12.39 1.32 9.88 16.41 12.81 1.47 10.23 17.74 2006 12.90 2.18 9.68 23.45 13.33 2.46 10.06 23.81 12.38 1.96 9.39 21.45 12.81 2.25 9.90 23.81 2007 17.59 5.36 9.99 32.68 18.44 5.76 10.26 37.50 16.75 4.95 9.70 30.44 17.54 5.36 9.89 31.09 2008 32.83 16.41 16.30 80.74 34.57 17.83 17.84 89.53 30.96 14.96 15.82 72.76 32.69 16.38 16.30 80.86 2009 31.75 9.20 19.54 52.65 32.78 9.61 19.67 57.36 30.50 8.63 19.25 49.27 31.48 9.08 19.47 56.65 2010 22.73 5.29 15.44 47.66 23.69 5.82 16.00 48.20 21.69 4.61 15.23 40.30 22.55 5.27 15.45 45.79 2011 24.27 8.17 14.31 46.18 25.40 8.78 14.99 48.00 23.15 7.59 14.27 41.51 24.20 8.14 14.62 48.00 2012 17.93 2.60 13.68 26.35 18.59 2.72 14.08 27.73 17.21 2.37 13.30 25.72 17.80 2.54 13.45 26.66 2013 14.29 1.67 11.52 20.87 14.82 1.88 11.75 21.91 13.80 1.51 11.05 19.04 14.23 1.74 11.30 20.49 2014 14.23 2.65 10.40 29.26 14.95 3.02 10.76 31.06 13.61 2.21 10.28 24.64 14.17 2.62 10.32 25.27 2015 16.71 3.99 11.77 31.91 17.79 5.03 12.22 53.29 15.85 3.65 10.88 29.91 16.67 4.34 11.95 40.74 2016 16.01 4.05 11.32 29.01 16.85 4.40 11.49 32.09 15.16 3.66 10.93 26.67 15.83 3.97 11.27 28.14 2017 11.14 1.34 9.23 16.19 11.72 1.54 9.52 17.28 10.64 1.16 8.56 14.97 11.09 1.36 9.14 16.04 2018 16.63 5.01 9.01 37.32 18.03 6.12 9.31 50.30 15.53 4.25 8.92 29.66 16.64 5.09 9.15 37.32 2019 15.57 2.74 11.55 27.54 16.41 3.06 11.79 28.53 14.76 2.38 11.03 24.05 15.39 2.61 11.54 25.45 2020 29.52 12.45 12.20 82.69 31.46 13.89 12.42 85.47 27.50 10.85 11.75 70.37 29.25 12.34 12.10 82.69 2021 19.83 3.47 15.02 35.16 21.12 4.22 15.54 37.51 18.65 2.93 14.10 29.24 19.66 3.62 15.01 37.21 2022 25.98 4.30 16.57 37.50 27.25 4.59 17.81 38.94 24.69 3.91 16.34 33.11 25.62 4.22 16.60 36.45 2023 17.12 3.17 11.96 27.77 17.83 3.58 12.46 30.81 16.36 2.89 11.81 24.00 16.87 3.14 12.07 26.52 2024 15.69 3.14 11.53 33.71 16.65 4.73 12.23 65.73 14.92 2.58 10.62 24.02 15.61 3.36 11.86 38.57 2025 21.05 6.71 14.89 60.13 22.52 8.20 15.16 60.13 19.67 4.97 14.58 38.58 20.74 6.38 14.77 52.33 It is interesting to see how much the mean OHLC values vary by year.\nAnd finally, we can run the statistics individually for each month:\n1 2 3 4 5 6 7 8 # Group by month and calculate mean and std for OHLC vix_stats_by_month = vix.groupby(vix.index.month)[[\u0026#34;Open\u0026#34;, \u0026#34;High\u0026#34;, \u0026#34;Low\u0026#34;, \u0026#34;Close\u0026#34;]].agg([\u0026#34;mean\u0026#34;, \u0026#34;std\u0026#34;]) # Flatten the column MultiIndex vix_stats_by_month.columns = [\u0026#39;_\u0026#39;.join(col).strip() for col in vix_stats_by_month.columns.values] vix_stats_by_month.index.name = \u0026#34;Month\u0026#34; display(vix_stats_by_month) Gives us:\nMonth Open_mean Open_std Open_min Open_max High_mean High_std High_min High_max Low_mean Low_std Low_min Low_max Close_mean Close_std Close_min Close_max 1 19.34 7.21 9.01 51.52 20.13 7.58 9.31 57.36 18.60 6.87 8.92 49.27 19.22 7.17 9.15 56.65 2 19.67 7.22 10.19 52.50 20.51 7.65 10.26 53.16 18.90 6.81 9.70 48.97 19.58 7.13 10.02 52.62 3 20.47 9.63 10.59 82.69 21.39 10.49 11.24 85.47 19.54 8.65 10.53 70.37 20.35 9.56 10.74 82.69 4 19.43 7.48 10.39 60.13 20.24 7.93 10.89 60.59 18.65 6.88 10.22 52.76 19.29 7.28 10.36 57.06 5 18.60 6.04 9.75 47.66 19.40 6.43 10.14 48.20 17.89 5.63 9.56 40.30 18.51 5.96 9.77 45.79 6 18.46 5.75 9.79 44.09 19.15 6.02 10.28 44.44 17.73 5.40 9.37 34.97 18.34 5.68 9.75 40.79 7 17.85 5.70 9.18 48.17 18.56 5.93 9.52 48.46 17.23 5.44 8.84 42.05 17.78 5.62 9.36 44.92 8 19.17 6.74 10.04 45.34 20.12 7.45 10.32 65.73 18.44 6.38 9.52 41.77 19.18 6.87 9.93 48.00 9 20.51 8.32 9.59 48.93 21.35 8.64 9.83 49.35 19.74 7.90 9.36 43.74 20.43 8.20 9.51 46.72 10 21.83 10.28 9.23 79.13 22.83 11.10 9.62 89.53 20.93 9.51 9.11 67.80 21.75 10.24 9.19 80.06 11 20.34 9.65 9.31 80.74 21.04 10.03 9.74 81.48 19.55 9.02 8.56 72.76 20.16 9.52 9.14 80.86 12 19.34 8.26 9.36 66.68 20.09 8.53 9.55 68.60 18.63 7.88 8.89 62.31 19.29 8.16 9.31 68.51 Deciles - VIX Here are the levels for each decile, for the full dataset:\n1 2 vix_deciles = vix.quantile(np.arange(0, 1.1, 0.1)) display(vix_deciles) Gives us:\nClose High Low Open 0.00 9.14 9.31 8.56 9.01 0.10 12.12 12.64 11.72 12.14 0.20 13.27 13.88 12.86 13.32 0.30 14.62 15.30 14.11 14.69 0.40 16.11 16.77 15.58 16.15 0.50 17.65 18.38 17.07 17.70 0.60 19.54 20.38 18.99 19.68 0.70 21.63 22.62 20.98 21.76 0.80 24.29 25.31 23.46 24.37 0.90 28.68 29.97 27.77 28.83 1.00 82.69 89.53 72.76 82.69 Plots - VIX Histogram Distribution - VIX A quick histogram gives us the distribution for the entire dataset, along with the levels for the mean minus 1 standard deviation, mean, mean plus 1 standard deviation, mean plus 2 standard deviations, mean plus 3 standard deviations, and mean plus 4 standard deviations:\nHistorical Data - VIX Here\u0026rsquo;s two plots for the dataset. The first covers 1990 - 2009, and the second 2010 - Present. This is the daily high level:\nFrom these plots, we can see the following:\nThe VIX has really only jumped above 50 several times (GFC, COVID, recently in August of 2024) The highest levels (\u0026gt; 80) occured only during the GFC \u0026amp; COVID Interestingly, the VIX did not ever get above 50 during the .com bubble Stats By Year - VIX Here\u0026rsquo;s the plot for the mean OHLC values for the VIX by year:\nStats By Month - VIX Here\u0026rsquo;s the plot for the mean OHLC values for the VIX by month:\nData Overview (VVIX) Before moving on to generating a signal, let\u0026rsquo;s run the above data overview code again, but this time for the CBOE VVIX. From the CBOE VVIX website:\n\u0026ldquo;Volatility is often called a new asset class, and every asset class deserves its own volatility index. The Cboe VVIX IndexSM represents the expected volatility of the VIX®. VVIX derives the expected 30-day volatility of VIX by applying the VIX algorithm to VIX options.\u0026rdquo;\nLooking at the statistics of the VVIX should give us an idea of the volatility of the VIX.\nAcquire CBOE VVIX Data First, let\u0026rsquo;s get the data:\n1 2 3 4 5 6 7 8 9 yf_pull_data( base_directory=DATA_DIR, ticker=\u0026#34;^VVIX\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) Load Data - VVIX Now that we have the data, let\u0026rsquo;s load it up and take a look:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Set decimal places pandas_set_decimal_places(2) # VVIX vvix = load_data( base_directory=DATA_DIR, ticker=\u0026#34;^VVIX\u0026#34;, source=\u0026#34;Yahoo_Finance\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, ) # Set \u0026#39;Date\u0026#39; column as datetime vvix[\u0026#39;Date\u0026#39;] = pd.to_datetime(vvix[\u0026#39;Date\u0026#39;]) # Drop \u0026#39;Volume\u0026#39; vvix.drop(columns = {\u0026#39;Volume\u0026#39;}, inplace = True) # Set Date as index vvix.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Check to see if there are any NaN values vvix[vvix[\u0026#39;High\u0026#39;].isna()] # Forward fill to clean up missing data vvix[\u0026#39;High\u0026#39;] = vvix[\u0026#39;High\u0026#39;].ffill() DataFrame Info - VVIX Now, running:\n1 df_info(vvix) Gives us the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 The columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 8952 entries, 1990-01-02 to 2025-07-18 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Close 8952 non-null float64 1 High 8952 non-null float64 2 Low 8952 non-null float64 3 Open 8952 non-null float64 dtypes: float64(4) memory usage: 349.7 KB The first 5 rows are:\nDate Close High Low Open 1990-01-02 00:00:00 17.24 17.24 17.24 17.24 1990-01-03 00:00:00 18.19 18.19 18.19 18.19 1990-01-04 00:00:00 19.22 19.22 19.22 19.22 1990-01-05 00:00:00 20.11 20.11 20.11 20.11 1990-01-08 00:00:00 20.26 20.26 20.26 20.26 The last 5 rows are:\nDate Close High Low Open 2025-07-14 00:00:00 17.20 17.85 16.91 17.73 2025-07-15 00:00:00 17.38 17.39 16.56 16.89 2025-07-16 00:00:00 17.16 19.48 16.69 17.66 2025-07-17 00:00:00 16.52 17.37 16.48 17.16 2025-07-18 00:00:00 16.41 16.81 16.28 16.43 Statistics - VVIX Here are the statistics for the VVIX, generated in the same manner as above for the VIX:\n1 2 3 4 5 6 7 8 9 10 vvix_stats = vvix.describe() num_std = [-1, 0, 1, 2, 3, 4, 5] for num in num_std: vvix_stats.loc[f\u0026#34;mean + {num} std\u0026#34;] = { \u0026#39;Open\u0026#39;: vvix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Open\u0026#39;] + num * vvix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Open\u0026#39;], \u0026#39;High\u0026#39;: vvix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;High\u0026#39;] + num * vvix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;High\u0026#39;], \u0026#39;Low\u0026#39;: vvix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Low\u0026#39;] + num * vvix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Low\u0026#39;], \u0026#39;Close\u0026#39;: vvix_stats.loc[\u0026#39;mean\u0026#39;][\u0026#39;Close\u0026#39;] + num * vvix_stats.loc[\u0026#39;std\u0026#39;][\u0026#39;Close\u0026#39;], } display(vvix_stats) Gives us:\nClose High Low Open count 4655.00 4655.00 4655.00 4655.00 mean 93.52 95.57 91.96 93.77 std 16.40 18.01 15.06 16.46 min 59.74 59.74 59.31 59.31 25% 82.39 83.53 81.50 82.59 50% 90.60 92.31 89.41 90.87 75% 102.20 105.01 99.92 102.54 max 207.59 212.22 187.27 212.22 mean + -1 std 77.12 77.56 76.90 77.31 mean + 0 std 93.52 95.57 91.96 93.77 mean + 1 std 109.92 113.58 107.02 110.23 mean + 2 std 126.33 131.59 122.08 126.69 mean + 3 std 142.73 149.60 137.14 143.14 mean + 4 std 159.14 167.60 152.20 159.60 mean + 5 std 175.54 185.61 167.26 176.06 We can also run the statistics individually for each year:\n1 2 3 4 5 6 7 8 # Group by year and calculate mean and std for OHLC vvix_stats_by_year = vvix.groupby(vvix.index.year)[[\u0026#34;Open\u0026#34;, \u0026#34;High\u0026#34;, \u0026#34;Low\u0026#34;, \u0026#34;Close\u0026#34;]].agg([\u0026#34;mean\u0026#34;, \u0026#34;std\u0026#34;]) # Flatten the column MultiIndex vvix_stats_by_year.columns = [\u0026#39;_\u0026#39;.join(col).strip() for col in vvix_stats_by_year.columns.values] vvix_stats_by_year.index.name = \u0026#34;Year\u0026#34; display(vvix_stats_by_year) Gives us:\nYear Open_mean Open_std Open_min Open_max High_mean High_std High_min High_max Low_mean Low_std Low_min Low_max Close_mean Close_std Close_min Close_max 2007 87.68 13.31 63.52 142.99 87.68 13.31 63.52 142.99 87.68 13.31 63.52 142.99 87.68 13.31 63.52 142.99 2008 81.85 15.60 59.74 134.87 81.85 15.60 59.74 134.87 81.85 15.60 59.74 134.87 81.85 15.60 59.74 134.87 2009 79.78 8.63 64.95 104.02 79.78 8.63 64.95 104.02 79.78 8.63 64.95 104.02 79.78 8.63 64.95 104.02 2010 88.36 13.07 64.87 145.12 88.36 13.07 64.87 145.12 88.36 13.07 64.87 145.12 88.36 13.07 64.87 145.12 2011 92.94 10.21 75.94 134.63 92.94 10.21 75.94 134.63 92.94 10.21 75.94 134.63 92.94 10.21 75.94 134.63 2012 94.84 8.38 78.42 117.44 94.84 8.38 78.42 117.44 94.84 8.38 78.42 117.44 94.84 8.38 78.42 117.44 2013 80.52 8.97 62.71 111.43 80.52 8.97 62.71 111.43 80.52 8.97 62.71 111.43 80.52 8.97 62.71 111.43 2014 83.01 14.33 61.76 138.60 83.01 14.33 61.76 138.60 83.01 14.33 61.76 138.60 83.01 14.33 61.76 138.60 2015 95.44 15.59 73.07 212.22 98.47 16.39 76.41 212.22 92.15 13.35 72.20 148.68 94.82 14.75 73.18 168.75 2016 93.36 10.02 77.96 131.95 95.82 10.86 78.86 132.42 90.54 8.99 76.17 115.15 92.80 10.07 76.17 125.13 2017 90.50 8.65 75.09 134.98 92.94 9.64 77.34 135.32 87.85 7.78 71.75 117.29 90.01 8.80 75.64 135.32 2018 102.60 13.22 83.70 176.72 106.27 16.26 85.00 203.73 99.17 11.31 82.60 165.35 102.26 14.04 83.21 180.61 2019 91.28 8.43 75.58 112.75 93.61 8.98 75.95 117.63 88.90 7.86 74.36 111.48 91.03 8.36 74.98 114.40 2020 118.64 19.32 88.39 203.03 121.91 20.88 88.54 209.76 115.05 17.37 85.31 187.27 118.36 19.39 86.87 207.59 2021 115.51 9.37 96.09 151.35 119.29 11.70 98.36 168.78 111.99 8.14 95.92 144.19 115.32 10.20 97.09 157.69 2022 102.58 18.01 76.48 161.09 105.32 19.16 77.93 172.82 99.17 16.81 76.13 153.26 101.81 17.81 77.05 154.38 2023 90.95 8.64 74.43 127.73 93.72 9.98 75.31 137.65 88.01 7.37 72.27 119.64 90.34 8.38 73.88 124.75 2024 92.88 15.06 59.31 169.68 97.32 18.33 74.79 192.49 89.51 13.16 59.31 137.05 92.81 15.60 73.26 173.32 2025 104.44 15.16 83.19 186.33 109.03 17.81 85.82 189.03 100.21 11.84 81.73 146.51 103.60 14.41 81.89 170.92 And finally, we can run the statistics individually for each month:\n1 2 3 4 5 6 7 8 # Group by month and calculate mean and std for OHLC vvix_stats_by_month = vvix.groupby(vvix.index.month)[[\u0026#34;Open\u0026#34;, \u0026#34;High\u0026#34;, \u0026#34;Low\u0026#34;, \u0026#34;Close\u0026#34;]].agg([\u0026#34;mean\u0026#34;, \u0026#34;std\u0026#34;]) # Flatten the column MultiIndex vvix_stats_by_month.columns = [\u0026#39;_\u0026#39;.join(col).strip() for col in vvix_stats_by_month.columns.values] vvix_stats_by_month.index.name = \u0026#34;Year\u0026#34; display(vvix_stats_by_month) Gives us:\nYear Open_mean Open_std Open_min Open_max High_mean High_std High_min High_max Low_mean Low_std Low_min Low_max Close_mean Close_std Close_min Close_max 1 92.46 15.63 64.87 161.09 94.37 17.63 64.87 172.82 90.69 14.23 64.87 153.26 92.23 15.78 64.87 157.69 2 93.49 18.24 65.47 176.72 95.39 20.70 65.47 203.73 91.39 16.43 65.47 165.35 93.13 18.58 65.47 180.61 3 95.30 21.66 66.97 203.03 97.38 23.56 66.97 209.76 92.94 19.51 66.97 187.27 94.89 21.59 66.97 207.59 4 92.18 19.03 59.74 186.33 94.01 20.57 59.74 189.03 90.30 17.21 59.74 152.01 91.88 18.60 59.74 170.92 5 92.25 16.93 61.76 145.18 93.95 17.99 61.76 151.50 90.54 16.14 61.76 145.12 91.79 16.79 61.76 146.28 6 93.16 14.86 63.52 155.48 94.76 16.11 63.52 172.21 91.49 13.79 63.52 140.15 92.98 14.83 63.52 151.60 7 90.06 12.97 67.21 138.42 91.56 14.04 67.21 149.60 88.57 12.09 67.21 133.82 89.93 12.93 67.21 139.54 8 96.83 16.94 68.05 212.22 98.89 18.72 68.05 212.22 94.68 14.86 68.05 148.68 96.61 16.63 68.05 173.32 9 94.71 14.03 67.94 135.17 96.50 15.52 67.94 146.31 92.86 12.50 67.94 128.46 94.40 13.78 67.94 138.93 10 97.74 14.01 64.97 149.60 99.43 15.11 64.97 154.99 96.14 13.35 64.97 144.55 97.52 14.15 64.97 152.01 11 93.53 14.17 63.77 142.68 95.07 15.36 63.77 161.76 91.98 13.39 63.77 140.44 93.28 14.24 63.77 149.74 12 93.35 15.03 59.31 151.35 95.33 16.63 62.71 168.37 91.78 13.70 59.31 144.19 93.46 15.07 62.71 156.10 Deciles - VVIX Here are the levels for each decile, for the full dataset:\n1 2 vvix_deciles = vvix.quantile(np.arange(0, 1.1, 0.1)) display(vvix_deciles) Gives us:\nClose High Low Open 0.00 59.74 59.74 59.31 59.31 0.10 75.88 76.06 75.47 75.82 0.20 80.63 81.48 79.87 80.79 0.30 83.94 85.30 83.05 84.22 0.40 87.21 88.64 86.07 87.53 0.50 90.60 92.31 89.41 90.87 0.60 94.23 96.16 93.01 94.51 0.70 99.09 101.53 97.42 99.38 0.80 106.06 109.40 103.89 106.47 0.90 115.22 118.73 112.42 115.50 1.00 207.59 212.22 187.27 212.22 Plots - VVIX Histogram Distribution - VVIX A quick histogram gives us the distribution for the entire dataset, along with the levels for the mean minus 1 standard deviation, mean, mean plus 1 standard deviation, mean plus 2 standard deviations, mean plus 3 standard deviations, and mean plus 4 standard deviations:\nHistorical Data - VVIX Here\u0026rsquo;s two plots for the dataset. The first covers 2007 - 2016, and the second 2017 - Present. This is the daily high level:\nStats By Year - VVIX Here\u0026rsquo;s the plot for the mean OHLC values for the VVIX by year:\nStats By Month - VVIX Here\u0026rsquo;s the plot for the mean OHLC values for the VVIX by month:\nReferences https://www.cboe.com/tradable_products/vix/ https://github.com/ranaroussi/yfinance Code The jupyter notebook with the functions and all other code is available here. The html export of the jupyter notebook is available here. The pdf export of the jupyter notebook is available here.\n","date":"2025-03-01T00:00:01Z","image":"https://www.jaredszajkowski.com/2025/03/01/investigating-a-vix-trading-signal-part-1-vix-and-vvix/cover.jpg","permalink":"https://www.jaredszajkowski.com/2025/03/01/investigating-a-vix-trading-signal-part-1-vix-and-vvix/","title":"Investigating A VIX Trading Signal, Part 1: VIX And VVIX"},{"content":" Introduction This post intends to provide the code for all of the python functions that I use in my analysis. The goal here is that when writing another post I will simply be able to link to the functions below as opposed to providing the function code in each post.\nFunction Index bb_clean_data: Takes an Excel export from Bloomberg, removes the miscellaneous headings/rows, and returns a DataFrame. build_index: Reads the index_temp.md markdown file, inserts the markdown dependencies where indicated, and then saves the file as index.md. calc_vix_trade_pnl: Calculates the profit/loss from VIX options trades. df_info: A simple function to display the information about a DataFrame and the first five rows and last five rows. df_info_markdown: Similar to the df_info function above, except that it coverts the output to markdown. export_track_md_deps: Exports various text outputs to markdown files, which are included in the index.md file created when building the site with Hugo. load_data: Load data from a CSV, Excel, or Pickle file into a pandas DataFrame. pandas_set_decimal_places: Set the number of decimal places displayed for floating-point numbers in pandas. plot_price: Plot the price data from a DataFrame for a specified date range and columns. plot_stats: Generate a scatter plot for the mean OHLC prices. plot_vix_with_trades: Plot the VIX daily high and low prices, along with the VIX spikes, and trades. strategy_harry_brown_perm_port: Execute the strategy for the Harry Brown permanent portfolio. summary_stats: Generate summary statistics for a series of returns. yf_pull_data: Download daily price data from Yahoo Finance and export it. Python Functions bb_clean_data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 import os import pandas as pd from IPython.display import display def bb_clean_data( base_directory: str, fund_ticker_name: str, source: str, asset_class: str, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; This function takes an excel export from Bloomberg and removes all excess data leaving date and close columns. Parameters: ----------- base_directory : str Root path to store downloaded data. fund : str The fund to clean the data from. source : str Name of the data source (e.g., \u0026#39;Bloomberg\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- df : pd.DataFrame DataFrame containing cleaned data prices. \u0026#34;\u0026#34;\u0026#34; # Set location from where to read existing excel file location = f\u0026#34;{base_directory}/{source}/{asset_class}/Daily/{fund_ticker_name}.xlsx\u0026#34; # Read data from excel try: df = pd.read_excel(location, sheet_name =\u0026#34;Worksheet\u0026#34;, engine=\u0026#34;calamine\u0026#34;) except FileNotFoundError: print(f\u0026#34;File not found...please download the data for {fund_ticker_name}\u0026#34;) # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) # Create directory directory = f\u0026#34;{base_directory}/{source}/{asset_class}/Daily\u0026#34; os.makedirs(directory, exist_ok=True) # Export to excel if excel_export == True: df.to_excel(f\u0026#34;{directory}/{fund_ticker_name}_Clean.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df.to_pickle(f\u0026#34;{directory}/{fund_ticker_name}_Clean.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;The first and last date of data for {fund_ticker_name} is: \u0026#34;) display(df[:1]) display(df[-1:]) print(f\u0026#34;Bloomberg data cleaning complete for {fund_ticker_name}\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) else: pass return df build_index 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 from pathlib import Path def build_index() -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Build a Hugo-compatible index.md by combining Markdown fragments. This function reads a template file (`index_temp.md`) and a list of markdown dependencies from `index_dep.txt`. For each entry in the dependency list, it replaces a corresponding placeholder in the template (formatted as \u0026lt;!-- INSERT_\u0026lt;name\u0026gt;_HERE --\u0026gt;) with the content from the markdown file. If a file is missing, the placeholder is replaced with a warning note. Output: ------- - Writes the final assembled content to `index.md`. Raises: ------- FileNotFoundError: If either `index_temp.md` or `index_dep.txt` does not exist. Example: -------- If `index_dep.txt` contains: 01_intro.md 02_analysis.md And `index_temp.md` contains: \u0026lt;!-- INSERT_01_intro_HERE --\u0026gt; \u0026lt;!-- INSERT_02_analysis_HERE --\u0026gt; The resulting `index.md` will include the contents of the respective markdown files in place of their placeholders. \u0026#34;\u0026#34;\u0026#34; temp_index_path = Path(\u0026#34;index_temp.md\u0026#34;) final_index_path = Path(\u0026#34;index.md\u0026#34;) dependencies_path = Path(\u0026#34;index_dep.txt\u0026#34;) # Read the index template if not temp_index_path.exists(): raise FileNotFoundError(\u0026#34;Missing index_temp.md\u0026#34;) temp_index_content = temp_index_path.read_text() # Read dependency list if not dependencies_path.exists(): raise FileNotFoundError(\u0026#34;Missing index_dep.txt\u0026#34;) with dependencies_path.open(\u0026#34;r\u0026#34;) as f: markdown_files = [line.strip() for line in f if line.strip()] # Replace placeholders for each dependency final_index_content = temp_index_content for md_file in markdown_files: placeholder = f\u0026#34;\u0026lt;!-- INSERT_{Path(md_file).stem}_HERE --\u0026gt;\u0026#34; if Path(md_file).exists(): content = Path(md_file).read_text() final_index_content = final_index_content.replace(placeholder, content) else: print(f\u0026#34;⚠️ Warning: {md_file} not found, skipping placeholder {placeholder}\u0026#34;) final_index_content = final_index_content.replace(placeholder, f\u0026#34;*{md_file} not found*\u0026#34;) # Write final index.md final_index_path.write_text(final_index_content) print(\u0026#34;✅ index.md successfully built!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: build_index() calc_vix_trade_pnl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 import pandas as pd def calc_vix_trade_pnl( transaction_df: pd.DataFrame, exp_start_date: str, exp_end_date: str, trade_start_date: str, trade_end_date: str, ) -\u0026gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str, str, str, str]: \u0026#34;\u0026#34;\u0026#34; Calculate the profit and loss (PnL) of trades based on transaction data. Parameters: ----------- transaction_df : pd.DataFrame DataFrame containing transaction data. exp_start_date : str Start date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the start of the range for the option expiration date. exp_end_date : str End date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the end of the range for the option expiration date. trade_start_date : str Start date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the start of the range for the trade date. trade_end_date : str End date for filtering transactions in \u0026#39;YYYY-MM-DD\u0026#39; format. This is the end of the range for the trade date. Returns: -------- transactions_data : pd.DataFrame Dataframe containing the transactions for the specified timeframe. closed_trades : pd.DataFrame DataFrame containing the closed trades with realized PnL and percent PnL. open_trades : pd.DataFrame DataFrame containing the open trades. net_PnL_percent_str : str String representation of the net profit percentage. net_PnL_str : str String representation of the net profit and loss in dollars. \u0026#34;\u0026#34;\u0026#34; # If start and end dates for trades and expirations are None, use the entire DataFrame if exp_start_date is None and exp_end_date is None and trade_start_date is None and trade_end_date is None: transactions_data = transaction_df # If both start and end dates for trades and expirations are provided then filter by both else: transactions_data = transaction_df[ (transaction_df[\u0026#39;Exp_Date\u0026#39;] \u0026gt;= exp_start_date) \u0026amp; (transaction_df[\u0026#39;Exp_Date\u0026#39;] \u0026lt;= exp_end_date) \u0026amp; (transaction_df[\u0026#39;Trade_Date\u0026#39;] \u0026gt;= trade_start_date) \u0026amp; (transaction_df[\u0026#39;Trade_Date\u0026#39;] \u0026lt;= trade_end_date) ] # Combine the \u0026#39;Action\u0026#39; and \u0026#39;Symbol\u0026#39; columns to create a unique identifier for each transaction transactions_data[\u0026#39;TradeDate_Action_Symbol_VIX\u0026#39;] = ( transactions_data[\u0026#39;Trade_Date\u0026#39;].astype(str) + \u0026#34;, \u0026#34; + transactions_data[\u0026#39;Action\u0026#39;] + \u0026#34;, \u0026#34; + transactions_data[\u0026#39;Symbol\u0026#39;] + \u0026#34;, VIX = \u0026#34; + transactions_data[\u0026#39;Approx_VIX_Level\u0026#39;].astype(str) ) # Split buys and sells and sum the notional amounts transactions_sells = transactions_data[transactions_data[\u0026#39;Action\u0026#39;] == \u0026#39;Sell to Close\u0026#39;] transactions_sells = transactions_sells.groupby([\u0026#39;Symbol\u0026#39;, \u0026#39;Exp_Date\u0026#39;], as_index=False)[[\u0026#39;Amount\u0026#39;, \u0026#39;Quantity\u0026#39;]].sum() transactions_buys = transactions_data[transactions_data[\u0026#39;Action\u0026#39;] == \u0026#39;Buy to Open\u0026#39;] transactions_buys = transactions_buys.groupby([\u0026#39;Symbol\u0026#39;, \u0026#39;Exp_Date\u0026#39;], as_index=False)[[\u0026#39;Amount\u0026#39;, \u0026#39;Quantity\u0026#39;]].sum() # Merge buys and sells dataframes back together merged_transactions = pd.merge(transactions_buys, transactions_sells, on=[\u0026#39;Symbol\u0026#39;, \u0026#39;Exp_Date\u0026#39;], how=\u0026#39;outer\u0026#39;, suffixes=(\u0026#39;_Buy\u0026#39;, \u0026#39;_Sell\u0026#39;)) merged_transactions = merged_transactions.sort_values(by=[\u0026#39;Exp_Date\u0026#39;], ascending=[True]) merged_transactions = merged_transactions.reset_index(drop=True) # Identify the closed positions merged_transactions[\u0026#39;Closed\u0026#39;] = (~merged_transactions[\u0026#39;Amount_Sell\u0026#39;].isna()) \u0026amp; (~merged_transactions[\u0026#39;Amount_Buy\u0026#39;].isna()) \u0026amp; (merged_transactions[\u0026#39;Quantity_Buy\u0026#39;] == merged_transactions[\u0026#39;Quantity_Sell\u0026#39;]) # Create a new dataframe for closed positions closed_trades = merged_transactions[merged_transactions[\u0026#39;Closed\u0026#39;]] closed_trades = closed_trades.reset_index(drop=True) closed_trades[\u0026#39;Realized_PnL\u0026#39;] = closed_trades[\u0026#39;Amount_Sell\u0026#39;] - closed_trades[\u0026#39;Amount_Buy\u0026#39;] closed_trades[\u0026#39;Percent_PnL\u0026#39;] = closed_trades[\u0026#39;Realized_PnL\u0026#39;] / closed_trades[\u0026#39;Amount_Buy\u0026#39;] closed_trades.drop(columns={\u0026#39;Closed\u0026#39;, \u0026#39;Exp_Date\u0026#39;}, inplace=True) closed_trades[\u0026#39;Quantity_Sell\u0026#39;] = closed_trades[\u0026#39;Quantity_Sell\u0026#39;].astype(int) # Calculate the net % PnL and $ PnL net_PnL_percent = closed_trades[\u0026#39;Realized_PnL\u0026#39;].sum() / closed_trades[\u0026#39;Amount_Buy\u0026#39;].sum() net_PnL_percent_str = f\u0026#34;{round(net_PnL_percent * 100, 2)}%\u0026#34; total_opened_pos_mkt_val = closed_trades[\u0026#39;Amount_Buy\u0026#39;].sum() total_opened_pos_mkt_val_str = f\u0026#34;${total_opened_pos_mkt_val:,.2f}\u0026#34; total_closed_pos_mkt_val = closed_trades[\u0026#39;Amount_Sell\u0026#39;].sum() total_closed_pos_mkt_val_str = f\u0026#34;${total_closed_pos_mkt_val:,.2f}\u0026#34; net_PnL = closed_trades[\u0026#39;Realized_PnL\u0026#39;].sum() net_PnL_str = f\u0026#34;${net_PnL:,.2f}\u0026#34; # Create a new dataframe for open positions open_trades = merged_transactions[~merged_transactions[\u0026#39;Closed\u0026#39;]] open_trades = open_trades.reset_index(drop=True) open_trades.drop(columns={\u0026#39;Closed\u0026#39;, \u0026#39;Amount_Sell\u0026#39;, \u0026#39;Quantity_Sell\u0026#39;, \u0026#39;Exp_Date\u0026#39;}, inplace=True) return transactions_data, closed_trades, open_trades, net_PnL_percent_str, net_PnL_str, total_opened_pos_mkt_val_str, total_closed_pos_mkt_val_str df_info 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import pandas as pd from IPython.display import display def df_info( df: pd.DataFrame, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Display summary information about a pandas DataFrame. This function prints: - The DataFrame\u0026#39;s column names, shape, and data types via `df.info()` - The first 5 rows using `df.head()` - The last 5 rows using `df.tail()` It uses `display()` for better output formatting in environments like Jupyter notebooks. Parameters: ----------- df : pd.DataFrame The DataFrame to inspect. Returns: -------- None Example: -------- \u0026gt;\u0026gt;\u0026gt; df_info(my_dataframe) \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;The columns, shape, and data types are:\u0026#34;) print(df.info()) print(\u0026#34;The first 5 rows are:\u0026#34;) display(df.head()) print(\u0026#34;The last 5 rows are:\u0026#34;) display(df.tail()) df_info_markdown 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import io import pandas as pd def df_info_markdown( df: pd.DataFrame ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Generate a Markdown-formatted summary of a pandas DataFrame. This function captures and formats the output of `df.info()`, `df.head()`, and `df.tail()` in Markdown for easy inclusion in reports, documentation, or web-based rendering (e.g., Hugo or Jupyter export workflows). Parameters: ----------- df : pd.DataFrame The DataFrame to summarize. Returns: -------- str A string containing the DataFrame\u0026#39;s info, head, and tail formatted in Markdown. Example: -------- \u0026gt;\u0026gt;\u0026gt; print(df_info_markdown(df)) ```text The columns, shape, and data types are: \u0026lt;output from df.info()\u0026gt; ``` The first 5 rows are: | | col1 | col2 | |---|------|------| | 0 | ... | ... | The last 5 rows are: ... \u0026#34;\u0026#34;\u0026#34; buffer = io.StringIO() # Capture df.info() output df.info(buf=buffer) info_str = buffer.getvalue() # Convert head and tail to Markdown head_str = df.head().to_markdown(floatfmt=\u0026#34;.2f\u0026#34;) tail_str = df.tail().to_markdown(floatfmt=\u0026#34;.2f\u0026#34;) markdown = [ \u0026#34;```text\u0026#34;, \u0026#34;The columns, shape, and data types are:\\n\u0026#34;, info_str, \u0026#34;```\u0026#34;, \u0026#34;\\nThe first 5 rows are:\\n\u0026#34;, head_str, \u0026#34;\\nThe last 5 rows are:\\n\u0026#34;, tail_str ] return \u0026#34;\\n\u0026#34;.join(markdown) export_track_md_deps 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from pathlib import Path def export_track_md_deps( dep_file: Path, md_filename: str, content: str, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Export Markdown content to a file and track it as a dependency. This function writes the provided content to the specified Markdown file and appends the filename to the given dependency file (typically `index_dep.txt`). This is useful in workflows where Markdown fragments are later assembled into a larger document (e.g., a Hugo `index.md`). Parameters: ----------- dep_file : Path Path to the dependency file that tracks Markdown fragment filenames. md_filename : str The name of the Markdown file to export. content : str The Markdown-formatted content to write to the file. Returns: -------- None Example: -------- \u0026gt;\u0026gt;\u0026gt; export_track_md_deps(Path(\u0026#34;index_dep.txt\u0026#34;), \u0026#34;01_intro.md\u0026#34;, \u0026#34;# Introduction\\n...\u0026#34;) ✅ Exported and tracked: 01_intro.md \u0026#34;\u0026#34;\u0026#34; Path(md_filename).write_text(content) with dep_file.open(\u0026#34;a\u0026#34;) as f: f.write(md_filename + \u0026#34;\\n\u0026#34;) print(f\u0026#34;✅ Exported and tracked: {md_filename}\u0026#34;) load_api_keys 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import os from dotenv import load_dotenv from pathlib import Path from settings import config # Get the environment variable file path from the configuration ENV_PATH = config(\u0026#34;ENV_PATH\u0026#34;) def load_api_keys( env_path: Path=ENV_PATH ) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; Load API keys from a .env file. Parameters: ----------- env_path : Path Path to the .env file. Default is the ENV_PATH from settings. Returns: -------- keys : dict Dictionary of API keys. \u0026#34;\u0026#34;\u0026#34; load_dotenv(dotenv_path=env_path) keys = { \u0026#34;INFURA_KEY\u0026#34;: os.getenv(\u0026#34;INFURA_KEY\u0026#34;), \u0026#34;NASDAQ_DATA_LINK_KEY\u0026#34;: os.getenv(\u0026#34;NASDAQ_DATA_LINK_KEY\u0026#34;), \u0026#34;COINBASE_KEY\u0026#34;: os.getenv(\u0026#34;COINBASE_KEY\u0026#34;), \u0026#34;COINBASE_SECRET\u0026#34;: os.getenv(\u0026#34;COINBASE_SECRET\u0026#34;), \u0026#34;SCHWAB_APP_KEY\u0026#34;: os.getenv(\u0026#34;SCHWAB_APP_KEY\u0026#34;), \u0026#34;SCHWAB_SECRET\u0026#34;: os.getenv(\u0026#34;SCHWAB_SECRET\u0026#34;), \u0026#34;SCHWAB_ACCOUNT_NUMBER_1\u0026#34;: os.getenv(\u0026#34;SCHWAB_ACCOUNT_NUMBER_1\u0026#34;), \u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_1\u0026#34;: os.getenv(\u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_1\u0026#34;), \u0026#34;SCHWAB_ACCOUNT_NUMBER_2\u0026#34;: os.getenv(\u0026#34;SCHWAB_ACCOUNT_NUMBER_2\u0026#34;), \u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_2\u0026#34;: os.getenv(\u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_2\u0026#34;), \u0026#34;SCHWAB_ACCOUNT_NUMBER_3\u0026#34;: os.getenv(\u0026#34;SCHWAB_ACCOUNT_NUMBER_3\u0026#34;), \u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_3\u0026#34;: os.getenv(\u0026#34;SCHWAB_ENCRYPTED_ACCOUNT_ID_3\u0026#34;), } # Raise error if any key is missing for k, v in keys.items(): if not v: raise ValueError(f\u0026#34;Missing environment variable: {k}\u0026#34;) return keys load_data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 import pandas as pd from pathlib import Path def load_data( base_directory: str, ticker: str, source: str, asset_class: str, timeframe: str, file_format: str, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Load data from a CSV, Excel, or Pickle file into a pandas DataFrame. This function attempts to read a file first as a CSV, then as an Excel file (specifically looking for a sheet named \u0026#39;data\u0026#39; and using the \u0026#39;calamine\u0026#39; engine). If both attempts fail, a ValueError is raised. Parameters: ----------- base_directory : str Root path to read data file. ticker : str Ticker symbol to read. source : str Name of the data source (e.g., \u0026#39;Yahoo\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). timeframe : str Timeframe for the data (e.g., \u0026#39;Daily\u0026#39;, \u0026#39;Month_End\u0026#39;). file_format : str Format of the file to load (\u0026#39;csv\u0026#39;, \u0026#39;excel\u0026#39;, or \u0026#39;pickle\u0026#39;) Returns: -------- pd.DataFrame The loaded data. Raises: ------- ValueError If the file could not be loaded as either CSV or Excel. Example: -------- \u0026gt;\u0026gt;\u0026gt; df = load_data(DATA_DIR, \u0026#34;^VIX\u0026#34;, \u0026#34;Yahoo_Finance\u0026#34;, \u0026#34;Indices\u0026#34;) \u0026#34;\u0026#34;\u0026#34; if file_format == \u0026#34;csv\u0026#34;: csv_path = Path(base_directory) / source / asset_class / timeframe / f\u0026#34;{ticker}.csv\u0026#34; df = pd.read_csv(csv_path) return df elif file_format == \u0026#34;excel\u0026#34;: xlsx_path = Path(base_directory) / source / asset_class / timeframe / f\u0026#34;{ticker}.xlsx\u0026#34; df = pd.read_excel(xlsx_path, sheet_name=\u0026#34;data\u0026#34;, engine=\u0026#34;calamine\u0026#34;) return df elif file_format == \u0026#34;pickle\u0026#34;: pickle_path = Path(base_directory) / source / asset_class / timeframe / f\u0026#34;{ticker}.pkl\u0026#34; df = pd.read_pickle(pickle_path) return df else: raise ValueError(f\u0026#34;❌ Unsupported file format: {file_format}. Please use \u0026#39;csv\u0026#39;, \u0026#39;excel\u0026#39;, or \u0026#39;pickle\u0026#39;.\u0026#34;) pandas_set_decimal_places 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import pandas as pd def pandas_set_decimal_places( decimal_places: int, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Set the number of decimal places displayed for floating-point numbers in pandas. Parameters: ---------- decimal_places : int The number of decimal places to display for float values in pandas DataFrames and Series. Returns: -------- None Example: -------- \u0026gt;\u0026gt;\u0026gt; dp(3) \u0026gt;\u0026gt;\u0026gt; pd.DataFrame([1.23456789]) 0 0 1.235 \u0026#34;\u0026#34;\u0026#34; pd.set_option(\u0026#39;display.float_format\u0026#39;, lambda x: f\u0026#39;%.{decimal_places}f\u0026#39; % x) plot_price 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 import matplotlib.pyplot as plt import matplotlib.dates as mdates import matplotlib.ticker as mtick import pandas as pd from matplotlib.ticker import FormatStrFormatter, MultipleLocator def plot_price( price_df: pd.DataFrame, plot_start_date: str, plot_end_date: str, plot_columns, title: str, x_label: str, x_format: str, y_label: str, y_format: str, y_tick_spacing: int, grid: bool, legend: bool, export_plot: bool, plot_file_name: str, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Plot the price data from a DataFrame for a specified date range and columns. Parameters: ----------- df : pd.DataFrame DataFrame containing the price data to plot. plot_start_date : str Start date for the plot in \u0026#39;YYYY-MM-DD\u0026#39; format. plot_end_date : str End date for the plot in \u0026#39;YYYY-MM-DD\u0026#39; format. plot_columns : str OR list List of columns to plot from the DataFrame. If none, all columns will be plotted. title : str Title of the plot. x_label : str Label for the x-axis. x_axis_format : str Format for the x-axis date labels. y_label : str Label for the y-axis. y_tick_spacing : int Spacing for the y-axis ticks. grid : bool Whether to display a grid on the plot. legend : bool Whether to display a legend on the plot. export_plot : bool Whether to save the figure as a PNG file. plot_file_name : str File name for saving the figure (if save_fig is True). Returns: -------- None \u0026#34;\u0026#34;\u0026#34; # If start date and end date are None, use the entire DataFrame if plot_start_date is None and plot_end_date is None: df_filtered = price_df # If only end date is specified, filter by end date elif plot_start_date is None and plot_end_date is not None: df_filtered = price_df[(price_df.index \u0026lt;= plot_end_date)] # If only start date is specified, filter by start date elif plot_start_date is not None and plot_end_date is None: df_filtered = price_df[(price_df.index \u0026gt;= plot_start_date)] # If both start date and end date are specified, filter by both else: df_filtered = price_df[(price_df.index \u0026gt;= plot_start_date) \u0026amp; (price_df.index \u0026lt;= plot_end_date)] # Set plot figure size and background color plt.figure(figsize=(12, 6), facecolor=\u0026#34;#F5F5F5\u0026#34;) # Plot data if plot_columns ==\u0026#34;All\u0026#34;: for col in df_filtered.columns: plt.plot(df_filtered.index, df_filtered[col], label=col, linestyle=\u0026#39;-\u0026#39;, linewidth=1.5) else: for col in plot_columns: plt.plot(df_filtered.index, df_filtered[col], label=col, linestyle=\u0026#39;-\u0026#39;, linewidth=1.5) # Format X axis if x_format == \u0026#34;Day\u0026#34;: plt.gca().xaxis.set_major_locator(mdates.DayLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#34;%d %b %Y\u0026#34;)) elif x_format == \u0026#34;Week\u0026#34;: plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#34;%d %b %Y\u0026#34;)) elif x_format == \u0026#34;Month\u0026#34;: plt.gca().xaxis.set_major_locator(mdates.MonthLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#34;%b %Y\u0026#34;)) elif x_format == \u0026#34;Year\u0026#34;: plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#34;%Y\u0026#34;)) else: raise ValueError(f\u0026#34;Unrecognized x_format: {x_format}. Use \u0026#39;Day\u0026#39;, \u0026#39;Week\u0026#39;, \u0026#39;Month\u0026#39;, or \u0026#39;Year\u0026#39;.\u0026#34;) plt.xlabel(x_label, fontsize=10) plt.xticks(rotation=45, fontsize=8) # Format Y axis if y_format == \u0026#34;Decimal\u0026#34;: plt.gca().yaxis.set_major_formatter(FormatStrFormatter(\u0026#34;%.2f\u0026#34;)) elif y_format == \u0026#34;Percentage\u0026#34;: plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0)) elif y_format == \u0026#34;Scientific\u0026#34;: plt.gca().yaxis.set_major_formatter(FormatStrFormatter(\u0026#34;%.2e\u0026#34;)) elif y_format == \u0026#34;log\u0026#34;: plt.yscale(\u0026#34;log\u0026#34;) else: raise ValueError(f\u0026#34;Unrecognized y_format: {y_format}. Use \u0026#39;Decimal\u0026#39;, \u0026#39;Percentage\u0026#39;, or \u0026#39;Scientific\u0026#39;.\u0026#34;) plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.ylabel(y_label, fontsize=10) plt.yticks(fontsize=8) # Format title, layout, grid, and legend plt.title(title, fontsize=12) plt.tight_layout() if grid == True: plt.grid(True, linestyle=\u0026#39;--\u0026#39;, alpha=0.7) if legend == True: plt.legend(fontsize=9) # Save figure and display plot if export_plot == True: plt.savefig(f\u0026#34;{plot_file_name}.png\u0026#34;, dpi=300, bbox_inches=\u0026#34;tight\u0026#34;) # Display the plot plt.show() return None plot_stats 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 import matplotlib.pyplot as plt import pandas as pd from matplotlib.ticker import MultipleLocator def plot_stats( stats_df: pd.DataFrame, plot_columns, title: str, x_label: str, x_rotation: int, x_tick_spacing: int, y_label: str, y_tick_spacing: int, grid: bool, legend: bool, export_plot: bool, plot_file_name: str, ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Plot the price data from a DataFrame for a specified date range and columns. Parameters: ----------- stats_df : pd.DataFrame DataFrame containing the price data to plot. plot_columns : str OR list List of columns to plot from the DataFrame. If none, all columns will be plotted. title : str Title of the plot. x_label : str Label for the x-axis. x_rotation : int Rotation angle for the x-axis date labels. x_tick_spacing : int Spacing for the x-axis ticks. y_label : str Label for the y-axis. y_tick_spacing : int Spacing for the y-axis ticks. grid : bool Whether to display a grid on the plot. legend : bool Whether to display a legend on the plot. export_plot : bool Whether to save the figure as a PNG file. plot_file_name : str File name for saving the figure (if save_fig is True). Returns: -------- None \u0026#34;\u0026#34;\u0026#34; # Set plot figure size and background color plt.figure(figsize=(12, 6), facecolor=\u0026#34;#F5F5F5\u0026#34;) # Plot data if plot_columns == \u0026#34;All\u0026#34;: for col in stats_df.columns: plt.scatter(stats_df.index, stats_df[col], label=col, linestyle=\u0026#39;-\u0026#39;, linewidth=1.5) else: for col in plot_columns: plt.scatter(stats_df.index, stats_df[col], label=col, linestyle=\u0026#39;-\u0026#39;, linewidth=1.5) # Format X axis plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.xlabel(x_label, fontsize=10) plt.xticks(rotation=x_rotation, fontsize=8) # Format Y axis plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.ylabel(y_label, fontsize=10) plt.yticks(fontsize=8) # Format title, layout, grid, and legend plt.title(title, fontsize=12) plt.tight_layout() if grid == True: plt.grid(True, linestyle=\u0026#39;--\u0026#39;, alpha=0.7) if legend == True: plt.legend(fontsize=9) # Save figure and display plot if export_plot == True: plt.savefig(f\u0026#34;{plot_file_name}.png\u0026#34;, dpi=300, bbox_inches=\u0026#34;tight\u0026#34;) # Display the plot plt.show() return None plot_vix_with_trades 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 import matplotlib.dates as mdates import matplotlib.pyplot as plt import pandas as pd from matplotlib.ticker import MultipleLocator def plot_vix_with_trades( vix_price_df: pd.DataFrame, trades_df: pd.DataFrame, plot_start_date: str, plot_end_date: str, x_tick_spacing: int, y_tick_spacing: int, index_number: str, export_plot: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Plot the VIX daily high and low prices, along with the VIX spikes, and trades. Parameters: ----------- vix_price_df : pd.DataFrame Dataframe containing the VIX price data to plot. trades_df : pd.DataFrame Dataframe containing the trades data. plot_start_date : str Start date for the plot in \u0026#39;YYYY-MM-DD\u0026#39; format. plot_end_date : str End date for the plot in \u0026#39;YYYY-MM-DD\u0026#39; format. index_number : str Index number to be used in the file name of the plot export. export_plot : bool Whether to save the figure as a PNG file. Returns: -------- vix_data : pd.DataFrame Dataframe containing the VIX price data for the specified timeframe. \u0026#34;\u0026#34;\u0026#34; # Create temporary dataframe for the specified date range vix_data = vix_price_df[(vix_price_df.index \u0026gt;= plot_start_date) \u0026amp; (vix_price_df.index \u0026lt;= plot_end_date)] # Set plot figure size and background color plt.figure(figsize=(12, 6), facecolor=\u0026#34;#F5F5F5\u0026#34;) # Plot VIX high and low price data plt.plot(vix_data.index, vix_data[\u0026#39;High\u0026#39;], label=\u0026#39;High\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;steelblue\u0026#39;, linewidth=1) plt.plot(vix_data.index, vix_data[\u0026#39;Low\u0026#39;], label=\u0026#39;Low\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;brown\u0026#39;, linewidth=1) # Plot VIX spikes plt.scatter(vix_data[vix_data[\u0026#39;Spike_SMA\u0026#39;] == True].index, vix_data[vix_data[\u0026#39;Spike_SMA\u0026#39;] == True][\u0026#39;High\u0026#39;], label=\u0026#39;Spike (High \u0026gt; 1.25 * 10 Day High SMA)\u0026#39;, color=\u0026#39;black\u0026#39;, s=20) # Plot trades plt.scatter(trades_df[\u0026#39;Trade_Date\u0026#39;], trades_df[\u0026#39;Approx_VIX_Level\u0026#39;], label=\u0026#39;Trades\u0026#39;, color=\u0026#39;red\u0026#39;, s=20) # Annotate each point in trades_df with the corresponding Action_Symbol for _, row in trades_df.iterrows(): plt.text( row[\u0026#39;Trade_Date\u0026#39;] + pd.Timedelta(days=1), row[\u0026#39;Approx_VIX_Level\u0026#39;] + 0.1, row[\u0026#39;TradeDate_Action_Symbol_VIX\u0026#39;], fontsize=9 ) # Format X axis plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=x_tick_spacing)) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#34;%Y-%m-%d\u0026#34;)) plt.xlabel(\u0026#34;Date\u0026#34;, fontsize=10) plt.xticks(rotation=45, fontsize=8) # Format Y axis plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.ylabel(\u0026#34;VIX\u0026#34;, fontsize=10) plt.yticks(fontsize=8) # Format title, layout, grid, and legend plt.title(f\u0026#34;CBOE Volatility Index (VIX), VIX Spikes, Trades, {plot_start_date} - {plot_end_date}\u0026#34;, fontsize=12) plt.tight_layout() plt.grid(True, linestyle=\u0026#39;--\u0026#39;, alpha=0.7) plt.legend(fontsize=9) # Save figure and display plot if export_plot == True: # plt.savefig(f\u0026#34;{index_number}_VIX_Spike_Trades_{plot_start_date}_{plot_end_date}.png\u0026#34;, dpi=300, bbox_inches=\u0026#34;tight\u0026#34;) plt.savefig(f\u0026#34;{index_number}_VIX_Spike_Trades.png\u0026#34;, dpi=300, bbox_inches=\u0026#34;tight\u0026#34;) # Display the plot plt.show() return vix_data strategy_harry_brown_perm_port 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 import pandas as pd def strategy_harry_brown_perm_port( fund_list: str, starting_cash: int, cash_contrib: int, close_prices_df: pd.DataFrame, rebal_month: int, rebal_day: int, rebal_per_high: float, rebal_per_low: float, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Execute the re-balance strategy based on specified criteria. Parameters: ----------- fund_list (str): List of funds for data to be combined from. Funds are strings in the form \u0026#34;BTC-USD\u0026#34;. starting_cash (int): Starting investment balance. cash_contrib (int): Cash contribution to be made daily. close_prices_df (pd.DataFrame): DataFrame containing date and close prices for all funds to be included. rebal_month (int): Month for annual rebalance. rebal_day (int): Day for annual rebalance. rebal_per_high (float): High percentage for rebalance. rebal_per_low (float): Low percentage for rebalance. excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- df (pd.DataFrame): DataFrame containing strategy data for all funds to be included. Also dumps the df to excel for reference later. \u0026#34;\u0026#34;\u0026#34; num_funds = len(fund_list) df = close_prices_df.copy() df.reset_index(inplace = True) # Date to be used for annual rebalance target_month = rebal_month target_day = rebal_day # Create a dataframe with dates from the specific month rebal_date = df[df[\u0026#39;Date\u0026#39;].dt.month == target_month] # Specify the date or the next closest rebal_date = rebal_date[rebal_date[\u0026#39;Date\u0026#39;].dt.day \u0026gt;= target_day] # Group by year and take the first entry for each year rebal_dates_by_year = rebal_date.groupby(rebal_date[\u0026#39;Date\u0026#39;].dt.year).first().reset_index(drop=True) \u0026#39;\u0026#39;\u0026#39; Column order for the dataframe: df[fund + \u0026#34;_BA_Shares\u0026#34;] df[fund + \u0026#34;_BA_$_Invested\u0026#34;] df[fund + \u0026#34;_BA_Port_%\u0026#34;] df[\u0026#39;Total_BA_$_Invested\u0026#39;] df[\u0026#39;Contribution\u0026#39;] df[\u0026#39;Rebalance\u0026#39;] df[fund + \u0026#34;_AA_Shares\u0026#34;] df[fund + \u0026#34;_AA_$_Invested\u0026#34;] df[fund + \u0026#34;_AA_Port_%\u0026#34;] df[\u0026#39;Total_AA_$_Invested\u0026#39;] \u0026#39;\u0026#39;\u0026#39; # Calculate the columns and initial values for before action (BA) shares, $ invested, and port % for fund in fund_list: df[fund + \u0026#34;_BA_Shares\u0026#34;] = starting_cash / num_funds / df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_BA_$_Invested\u0026#34;] = df[fund + \u0026#34;_BA_Shares\u0026#34;] * df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_BA_Port_%\u0026#34;] = 0.25 # Set column values initially df[\u0026#39;Total_BA_$_Invested\u0026#39;] = starting_cash df[\u0026#39;Contribution\u0026#39;] = cash_contrib df[\u0026#39;Rebalance\u0026#39;] = \u0026#34;No\u0026#34; # Set columns and values initially for after action (AA) shares, $ invested, and port % for fund in fund_list: df[fund + \u0026#34;_AA_Shares\u0026#34;] = starting_cash / num_funds / df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_AA_$_Invested\u0026#34;] = df[fund + \u0026#34;_AA_Shares\u0026#34;] * df[fund + \u0026#34;_Close\u0026#34;] df[fund + \u0026#34;_AA_Port_%\u0026#34;] = 0.25 # Set column value for after action (AA) total $ invested df[\u0026#39;Total_AA_$_Invested\u0026#39;] = starting_cash # Iterate through the dataframe and execute the strategy for index, row in df.iterrows(): # Ensure there\u0026#39;s a previous row to reference by checking the index value if index \u0026gt; 0: # Initialize variable Total_BA_Invested = 0 # Calculate before action (BA) shares and $ invested values for fund in fund_list: df.at[index, fund + \u0026#34;_BA_Shares\u0026#34;] = df.at[index - 1, fund + \u0026#34;_AA_Shares\u0026#34;] df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] = df.at[index, fund + \u0026#34;_BA_Shares\u0026#34;] * row[fund + \u0026#34;_Close\u0026#34;] # Sum the asset values to find the total Total_BA_Invested = Total_BA_Invested + df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] # Calculate before action (BA) port % values for fund in fund_list: df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] = df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] / Total_BA_Invested # Set column for before action (BA) total $ invested df.at[index, \u0026#39;Total_BA_$_Invested\u0026#39;] = Total_BA_Invested # Initialize variables rebalance = \u0026#34;No\u0026#34; date = row[\u0026#39;Date\u0026#39;] # Check for a specific date annually # Simple if statement to check if date_to_check is in jan_28_or_after_each_year if date in rebal_dates_by_year[\u0026#39;Date\u0026#39;].values: rebalance = \u0026#34;Yes\u0026#34; else: pass # Check to see if any asset has portfolio percentage of greater than 35% or less than 15% and if so set variable for fund in fund_list: if df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] \u0026gt; rebal_per_high or df.at[index, fund + \u0026#34;_BA_Port_%\u0026#34;] \u0026lt; rebal_per_low: rebalance = \u0026#34;Yes\u0026#34; else: pass # If rebalance is required, rebalance back to 25% for each asset, else just divide contribution evenly across assets if rebalance == \u0026#34;Yes\u0026#34;: df.at[index, \u0026#39;Rebalance\u0026#39;] = rebalance for fund in fund_list: df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] = (Total_BA_Invested + df.at[index, \u0026#39;Contribution\u0026#39;]) * 0.25 else: df.at[index, \u0026#39;Rebalance\u0026#39;] = rebalance for fund in fund_list: df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] = df.at[index, fund + \u0026#34;_BA_$_Invested\u0026#34;] + df.at[index, \u0026#39;Contribution\u0026#39;] * 0.25 # Initialize variable Total_AA_Invested = 0 # Set column values for after action (AA) shares and port % for fund in fund_list: df.at[index, fund + \u0026#34;_AA_Shares\u0026#34;] = df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] / row[fund + \u0026#34;_Close\u0026#34;] # Sum the asset values to find the total Total_AA_Invested = Total_AA_Invested + df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] # Calculate after action (AA) port % values for fund in fund_list: df.at[index, fund + \u0026#34;_AA_Port_%\u0026#34;] = df.at[index, fund + \u0026#34;_AA_$_Invested\u0026#34;] / Total_AA_Invested # Set column for after action (AA) total $ invested df.at[index, \u0026#39;Total_AA_$_Invested\u0026#39;] = Total_AA_Invested # If this is the first row else: pass df[\u0026#39;Return\u0026#39;] = df[\u0026#39;Total_AA_$_Invested\u0026#39;].pct_change() df[\u0026#39;Cumulative_Return\u0026#39;] = (1 + df[\u0026#39;Return\u0026#39;]).cumprod() plan_name = \u0026#39;_\u0026#39;.join(fund_list) # Export to excel if excel_export == True: df.to_excel(f\u0026#34;{plan_name}_Strategy.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df.to_pickle(f\u0026#34;{plan_name}_Strategy.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;Strategy complete for {plan_name}\u0026#34;) else: pass return df summary_stats 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 import pandas as pd import numpy as np def summary_stats( fund_list: list[str], df: pd.DataFrame, period: str, use_calendar_days: bool, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Calculate summary statistics for the given fund list and return data. Parameters: ----------- fund_list (str): List of funds. This is used below in the excel/pickle export but not in the analysis.. Funds are strings in the form \u0026#34;BTC-USD\u0026#34;. df (pd.DataFrame): Dataframe with return data. period (str): Period for which to calculate statistics. Options are \u0026#34;Monthly\u0026#34;, \u0026#34;Weekly\u0026#34;, \u0026#34;Daily\u0026#34;. use_calendar_days (bool): If True, use calendar days for calculations. If False, use trading days. excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- df_stats (pd.DataFrame): pd.DataFrame: DataFrame containing various portfolio statistics. \u0026#34;\u0026#34;\u0026#34; period = period.strip().capitalize() # Map base timeframes period_to_timeframe = { \u0026#34;Monthly\u0026#34;: 12, \u0026#34;Weekly\u0026#34;: 52, \u0026#34;Daily\u0026#34;: 365 if use_calendar_days else 252, } try: timeframe = period_to_timeframe[period] except KeyError: raise ValueError(f\u0026#34;Invalid period: {period}. Must be one of {list(period_to_timeframe.keys())}\u0026#34;) df_stats = pd.DataFrame(df.mean(axis=0) * timeframe) # annualized # df_stats = pd.DataFrame((1 + df.mean(axis=0)) ** timeframe - 1) # annualized, this is this true annualized return but we will simply use the mean df_stats.columns = [\u0026#39;Annualized Mean\u0026#39;] df_stats[\u0026#39;Annualized Volatility\u0026#39;] = df.std() * np.sqrt(timeframe) # annualized df_stats[\u0026#39;Annualized Sharpe Ratio\u0026#39;] = df_stats[\u0026#39;Annualized Mean\u0026#39;] / df_stats[\u0026#39;Annualized Volatility\u0026#39;] df_cagr = (1 + df[\u0026#39;Return\u0026#39;]).cumprod() cagr = (df_cagr.iloc[-1] / 1) ** ( 1 / (len(df_cagr) / timeframe)) - 1 df_stats[\u0026#39;CAGR\u0026#39;] = cagr df_stats[f\u0026#39;{period} Max Return\u0026#39;] = df.max() df_stats[f\u0026#39;{period} Max Return (Date)\u0026#39;] = df.idxmax().values[0] df_stats[f\u0026#39;{period} Min Return\u0026#39;] = df.min() df_stats[f\u0026#39;{period} Min Return (Date)\u0026#39;] = df.idxmin().values[0] wealth_index = 1000 * (1 + df).cumprod() previous_peaks = wealth_index.cummax() drawdowns = (wealth_index - previous_peaks) / previous_peaks df_stats[\u0026#39;Max Drawdown\u0026#39;] = drawdowns.min() df_stats[\u0026#39;Peak\u0026#39;] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns] df_stats[\u0026#39;Bottom\u0026#39;] = drawdowns.idxmin() recovery_date = [] for col in wealth_index.columns: prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max() recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T recovery_date.append(recovery_wealth[recovery_wealth[col] \u0026gt;= prev_max].index.min()) df_stats[\u0026#39;Recovery Date\u0026#39;] = recovery_date plan_name = \u0026#39;_\u0026#39;.join(fund_list) # Export to excel if excel_export == True: df_stats.to_excel(f\u0026#34;{plan_name}_Summary_Stats.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df_stats.to_pickle(f\u0026#34;{plan_name}_Summary_Stats.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;Summary stats complete for {plan_name}\u0026#34;) else: pass return df_stats yf_pull_data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 import os import pandas as pd import yfinance as yf from IPython.display import display def yf_pull_data( base_directory: str, ticker: str, source: str, asset_class: str, excel_export: bool, pickle_export: bool, output_confirmation: bool, ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34; Download daily price data from Yahoo Finance and export it. Parameters: ----------- base_directory : str Root path to store downloaded data. ticker : str Ticker symbol to download. source : str Name of the data source (e.g., \u0026#39;Yahoo\u0026#39;). asset_class : str Asset class name (e.g., \u0026#39;Equities\u0026#39;). excel_export : bool If True, export data to Excel format. pickle_export : bool If True, export data to Pickle format. output_confirmation : bool If True, print confirmation message. Returns: -------- df : pd.DataFrame DataFrame containing the downloaded data. \u0026#34;\u0026#34;\u0026#34; # Download data from YF df = yf.download(ticker) # Drop the column level with the ticker symbol df.columns = df.columns.droplevel(1) # Reset index df = df.reset_index() # Remove the \u0026#34;Price\u0026#34; header from the index df.columns.name = None # Reset date column df[\u0026#39;Date\u0026#39;] = df[\u0026#39;Date\u0026#39;].dt.tz_localize(None) # Set \u0026#39;Date\u0026#39; column as index df = df.set_index(\u0026#39;Date\u0026#39;, drop=True) # Drop data from last day because it\u0026#39;s not accrate until end of day df = df.drop(df.index[-1]) # Create directory directory = f\u0026#34;{base_directory}/{source}/{asset_class}/Daily\u0026#34; os.makedirs(directory, exist_ok=True) # Export to excel if excel_export == True: df.to_excel(f\u0026#34;{directory}/{ticker}.xlsx\u0026#34;, sheet_name=\u0026#34;data\u0026#34;) else: pass # Export to pickle if pickle_export == True: df.to_pickle(f\u0026#34;{directory}/{ticker}.pkl\u0026#34;) else: pass # Output confirmation if output_confirmation == True: print(f\u0026#34;The first and last date of data for {ticker} is: \u0026#34;) display(df[:1]) display(df[-1:]) print(f\u0026#34;Yahoo Finance data complete for {ticker}\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) else: pass return df References None\nCode The jupyter notebook with the functions and all other code is available here. The html export of the jupyter notebook is available here. The pdf export of the jupyter notebook is available here.\n","date":"2025-02-02T00:00:01Z","image":"https://www.jaredszajkowski.com/2025/02/02/reusable-extensible-python-functions-financial-data-analysis/cover.jpg","permalink":"https://www.jaredszajkowski.com/2025/02/02/reusable-extensible-python-functions-financial-data-analysis/","title":"Reusable And Extensible Python Functions For Financial Data Analysis"},{"content":"Post Updates Update 1/12/2025: Updated section for use of requirements.txt file for dependency management. Update 1/30/2025: Added section for version specific python virtual environments.\nPython Module Management As an Arch Linux user, the push is to utilize pacman and related tools to manage dependencies and package updates (including Python modules). In fact, the wiki itself explicitly states this (see 2.1), and the default Arch installation of Python disables python-pip.\nUnfortunately, there are limited resources put into maintaining packages for modules and only the most common and popular modules are maintained, and they are updated promptly as is consistent within the Arch ecosystem.\nCreating A Virtual Environment After recently delving into crypto and the web3 Python module, the Coinbase API, and others, I\u0026rsquo;ve found the need to install Python modules from Pypi, the Python package index. This is the most exhaustive location to find modules, including the latest updates and version history.\nUsing python-pip necessitated the use of virtual environments, which made me reconsider the idea of not maintaining Python modules (or maintaining very few) through pacman at all.\nI chose to place the virtual environments at ~/python-virtual-envs/ and within that directory have one called general and other called wrds. The wrds environment is specific to the Wharton Research Data Services which requires (for some reason) an older package of nympy.\nThe \u0026ldquo;general\u0026rdquo; environment covers everything else. I created it with the usual command:\n$ python -m venv ~/python-virtual-envs/general Once created, it can be activated (either in a terminal or an IDE such as VS Code) by executing the following in the terminal:\n$ source ~/python-virtual-envs/general/bin/activate Creating Version Specific Python Virtual Environments If a specific version of python is required (vs the version installed on the base Arch system), it can be installed as follows:\n$ sudo yay python312 And then follow the requisite prompts to install. Note that I am using yay, with the binary build yay-bin.\nOnce that completes, the virtual environment can be installed as follows:\n$ python3.12 -m venv ~/python-virtual-envs/general_312 The virtual environment can then be activated in a similar manner as any other:\n$ source ~/python-virtual-envs/general_312/bin/activate Using python-pip After the virtual environment is created and activated, modules can be installed by using python-pip, such as:\n$ pip install \u0026lt;package-name\u0026gt; If you want to view all installed modules, run:\n$ pip list Or the outdated modules:\n$ pip list --outdated And updated at a later point in time with:\n$ pip install --upgrade \u0026lt;package-name\u0026gt; Using A requirements.txt File If you have a requirements.txt file present in a git repository/directory, such as:\nYou can install the required dependencies with the following command:\n$ pip install -r requirements.txt pip will then install all the required package and module versions based on the requirements file.\nMaintaining Across Multiple Systems To avoid having to redundantly install modules on different systems, after I make a change to the virtual environment I can zip the entire ~/python-virtual-envs/ directory (or any of the individual directories of the virtual environments) and upload the zip file to Dropbox. This takes only a few minutes, and if I am working on a different system can simply extract the archive and have a completely up-to-date and current virtual environment to work in.\nReferences https://docs.python.org/3/library/venv.html https://pypi.org/ https://note.nkmk.me/en/python-pip-usage/ https://wiki.archlinux.org/title/Python https://github.com/Jguer/yay ","date":"2024-12-02T00:00:01Z","image":"https://www.jaredszajkowski.com/2024/12/02/using-python-virtual-environments/cover.jpg","permalink":"https://www.jaredszajkowski.com/2024/12/02/using-python-virtual-environments/","title":"Using Python Virtual Environments"},{"content":" Introduction Harry Browne was an influencial politician, financial advisor, and author who lived from 1933 to 2006 and published 12 books. Wikipedia has an in-depth biography on him.\nWithin the world of finance and investing, one of his best known works is Fail-Safe Investing: Lifelong Financial Security in 30 Minutes. In it, he introduces the idea of the \u0026ldquo;Permanent Portfolio\u0026rdquo;, an investment strategy that uses only four assets and is very simple to implement.\nIn this post, we will investigate Browne\u0026rsquo;s suggested portfolio, including performance across various market cycles and economic regimes.\nBrowne\u0026rsquo;s Portfolio Requirements In Fail-Safe Investing, under rule #11, Browne lays out the requirements for a \u0026ldquo;bulletproof portfolio\u0026rdquo; that will \u0026ldquo;assure that your wealth will survive any event - including events that would be devastating to any one investment. In other words, this portfolio should protect you no matter what the future brings.\u0026rdquo;\nHis requirements for the portfolio consist of the followng:\nSafety: Protection again any economic future, including \u0026ldquo;inflation, recession, or even depression\u0026rdquo; Stability: Performance should be consistent so that you will not need to make any changes and will not experience significant drawdowns Simplicity: Easy to implement and take very little time to maintain He then describes the four \u0026ldquo;broad movements\u0026rdquo; of the economy:\nProsperity: The economy is growing, business is doing well, interest rates are usually low Inflation: The cost of goods and services is rising Tight money or recession: The money supply is shrinking, economic activity is slowing Deflation: Prices are declining and the value of money is increasing The Permanent Portfolio Browne then matches an asset class to each of the economic conditions above:\nProsperity -\u0026gt; Stocks (due to prosperity) and long term bonds (when interest rates fall) Inflation -\u0026gt; Gold Deflation -\u0026gt; Long term bonds (when interest rates fall) Tight money -\u0026gt; Cash He completes the Permanent Portfolio by stipulating the following:\nStart with a base allocation of 25% to each of the asset classes (stocks, bonds, gold, cash) Rebalance back to the base allocation annually, or when \u0026ldquo;any of the four investments has become worth less than 15%, or more than 35%, of the portfolio\u0026rsquo;s overall value\u0026rdquo;Note: Browne does not specify when the portfolio should be rebalanced; therefore, we will make an assumption of a January 1st rebalance. Data For this exercise, we will use the following asset classes:\nStocks: S\u0026amp;P 500 (SPXT_S\u0026amp;P 500 Total Return Index) Bonds: 10 Year US Treasuries (SPBDU10T_S\u0026amp;P US Treasury Bond 7-10 Year Total Return Index) Gold: Gold Spot Price (XAU_Gold USD Spot) Cash: USD With the exception of cash, all data is sourced from Bloomberg.\nWe could use ETFs, but the available price history for the ETFs is much shorter than the indices above. If we wanted to use ETFs, the following would work:\nStocks: IVV - iShares Core S\u0026amp;P 500 ETF Bonds: IEF - iShares 7-10 Year Treasury Bond ETF Gold: IAU - iShares Gold Trust Cash: USD Python Functions Here are the functions needed for this project:\nbb_clean_data: Takes an Excel export from Bloomberg, removes the miscellaneous headings/rows, and returns a DataFrame. df_info: A simple function to display the information about a DataFrame and the first five rows and last five rows. df_info_markdown: Similar to the df_info function above, except that it coverts the output to markdown. export_track_md_deps: Exports various text outputs to markdown files, which are included in the index.md file created when building the site with Hugo. load_data: Load data from a CSV, Excel, or Pickle file into a pandas DataFrame. pandas_set_decimal_places: Set the number of decimal places displayed for floating-point numbers in pandas. strategy_harry_brown_perm_port: Execute the strategy for the Harry Brown permanent portfolio. summary_stats: Generate summary statistics for a series of returns. Data Overview Load Data As previously mentioned, the data for this exercise comes primarily from Bloomberg. We\u0026rsquo;ll start with loading the data first for bonds:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # Set decimal places pandas_set_decimal_places(3) # Bonds dataframe bb_clean_data( base_directory=DATA_DIR, fund_ticker_name=\u0026#34;SPBDU10T_S\u0026amp;P US Treasury Bond 7-10 Year Total Return Index\u0026#34;, source=\u0026#34;Bloomberg\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) bonds_data = load_data( base_directory=DATA_DIR, ticker=\u0026#34;SPBDU10T_S\u0026amp;P US Treasury Bond 7-10 Year Total Return Index_Clean\u0026#34;, source=\u0026#34;Bloomberg\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, ) bonds_data[\u0026#39;Date\u0026#39;] = pd.to_datetime(bonds_data[\u0026#39;Date\u0026#39;]) bonds_data.set_index(\u0026#39;Date\u0026#39;, inplace = True) bonds_data = bonds_data[(bonds_data.index \u0026gt;= \u0026#39;1990-01-01\u0026#39;) \u0026amp; (bonds_data.index \u0026lt;= \u0026#39;2023-12-31\u0026#39;)] bonds_data.rename(columns={\u0026#39;Close\u0026#39;:\u0026#39;Bonds_Close\u0026#39;}, inplace=True) bonds_data[\u0026#39;Bonds_Daily_Return\u0026#39;] = bonds_data[\u0026#39;Bonds_Close\u0026#39;].pct_change() bonds_data[\u0026#39;Bonds_Total_Return\u0026#39;] = (1 + bonds_data[\u0026#39;Bonds_Daily_Return\u0026#39;]).cumprod() display(bonds_data.head()) The following is the output:\nDate Bonds_Close Bonds_Daily_Return Bonds_Total_Return 1990-01-02 00:00:00 99.972 nan nan 1990-01-03 00:00:00 99.733 -0.002 0.998 1990-01-04 00:00:00 99.813 0.001 0.998 1990-01-05 00:00:00 99.769 -0.000 0.998 1990-01-08 00:00:00 99.681 -0.001 0.997 Then for stocks:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Stocks dataframe bb_clean_data( base_directory=DATA_DIR, fund_ticker_name=\u0026#34;SPXT_S\u0026amp;P 500 Total Return Index\u0026#34;, source=\u0026#34;Bloomberg\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) stocks_data = load_data( base_directory=DATA_DIR, ticker=\u0026#34;SPXT_S\u0026amp;P 500 Total Return Index_Clean\u0026#34;, source=\u0026#34;Bloomberg\u0026#34;, asset_class=\u0026#34;Indices\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, ) stocks_data[\u0026#39;Date\u0026#39;] = pd.to_datetime(stocks_data[\u0026#39;Date\u0026#39;]) stocks_data.set_index(\u0026#39;Date\u0026#39;, inplace = True) stocks_data = stocks_data[(stocks_data.index \u0026gt;= \u0026#39;1990-01-01\u0026#39;) \u0026amp; (stocks_data.index \u0026lt;= \u0026#39;2023-12-31\u0026#39;)] stocks_data.rename(columns={\u0026#39;Close\u0026#39;:\u0026#39;Stocks_Close\u0026#39;}, inplace=True) stocks_data[\u0026#39;Stocks_Daily_Return\u0026#39;] = stocks_data[\u0026#39;Stocks_Close\u0026#39;].pct_change() stocks_data[\u0026#39;Stocks_Total_Return\u0026#39;] = (1 + stocks_data[\u0026#39;Stocks_Daily_Return\u0026#39;]).cumprod() display(stocks_data.head()) The following is the output:\nDate Stocks_Close Stocks_Daily_Return Stocks_Total_Return 1990-01-01 00:00:00 nan nan nan 1990-01-02 00:00:00 386.160 nan nan 1990-01-03 00:00:00 385.170 -0.003 0.997 1990-01-04 00:00:00 382.020 -0.008 0.989 1990-01-05 00:00:00 378.300 -0.010 0.980 And finally, gold:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Gold dataframe bb_clean_data( base_directory=DATA_DIR, fund_ticker_name=\u0026#34;XAU_Gold USD Spot\u0026#34;, source=\u0026#34;Bloomberg\u0026#34;, asset_class=\u0026#34;Commodities\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) gold_data = load_data( base_directory=DATA_DIR, ticker=\u0026#34;XAU_Gold USD Spot_Clean\u0026#34;, source=\u0026#34;Bloomberg\u0026#34;, asset_class=\u0026#34;Commodities\u0026#34;, timeframe=\u0026#34;Daily\u0026#34;, ) gold_data[\u0026#39;Date\u0026#39;] = pd.to_datetime(gold_data[\u0026#39;Date\u0026#39;]) gold_data.set_index(\u0026#39;Date\u0026#39;, inplace = True) gold_data = gold_data[(gold_data.index \u0026gt;= \u0026#39;1990-01-01\u0026#39;) \u0026amp; (gold_data.index \u0026lt;= \u0026#39;2023-12-31\u0026#39;)] gold_data.rename(columns={\u0026#39;Close\u0026#39;:\u0026#39;Gold_Close\u0026#39;}, inplace=True) gold_data[\u0026#39;Gold_Daily_Return\u0026#39;] = gold_data[\u0026#39;Gold_Close\u0026#39;].pct_change() gold_data[\u0026#39;Gold_Total_Return\u0026#39;] = (1 + gold_data[\u0026#39;Gold_Daily_Return\u0026#39;]).cumprod() display(gold_data.head()) The following is the output:\nDate Gold_Close Gold_Daily_Return Gold_Total_Return 1990-01-02 00:00:00 399.000 nan nan 1990-01-03 00:00:00 395.000 -0.010 0.990 1990-01-04 00:00:00 396.500 0.004 0.994 1990-01-05 00:00:00 405.000 0.021 1.015 1990-01-08 00:00:00 404.600 -0.001 1.014 Combine Data We\u0026rsquo;ll now combine the dataframes for the timeseries data from each of the asset classes, as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Merge the stock data and bond data into a single DataFrame using their indices (dates) perm_port = pd.merge(stocks_data[\u0026#39;Stocks_Close\u0026#39;], bonds_data[\u0026#39;Bonds_Close\u0026#39;], left_index=True, right_index=True) # Add gold data to the portfolio DataFrame by merging it with the existing data on indices (dates) perm_port = pd.merge(perm_port, gold_data[\u0026#39;Gold_Close\u0026#39;], left_index=True, right_index=True) # Add a column for cash with a constant value of 1 (assumes the value of cash remains constant at $1 over time) perm_port[\u0026#39;Cash_Close\u0026#39;] = 1 # Remove any rows with missing values (NaN) to ensure clean data for further analysis perm_port.dropna(inplace=True) # Display the finalized portfolio DataFrame display(perm_port) Check For Missing Values We can check for any missing (NaN) values in each column:\n1 2 # Check for any missing values in each column perm_port.isnull().any() DataFrame Info Now, running:\n1 df_info(perm_port) Gives us the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 The columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 8479 entries, 1990-01-02 to 2023-12-29 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Stocks_Close 8479 non-null float64 1 Bonds_Close 8479 non-null float64 2 Gold_Close 8479 non-null float64 3 Cash_Close 8479 non-null int64 dtypes: float64(3), int64(1) memory usage: 331.2 KB The first 5 rows are:\nDate Stocks_Close Bonds_Close Gold_Close Cash_Close 1990-01-02 00:00:00 386.16 99.97 399.00 1.00 1990-01-03 00:00:00 385.17 99.73 395.00 1.00 1990-01-04 00:00:00 382.02 99.81 396.50 1.00 1990-01-05 00:00:00 378.30 99.77 405.00 1.00 1990-01-08 00:00:00 380.04 99.68 404.60 1.00 The last 5 rows are:\nDate Stocks_Close Bonds_Close Gold_Close Cash_Close 2023-12-22 00:00:00 10292.37 604.17 2053.08 1.00 2023-12-26 00:00:00 10335.98 604.55 2067.81 1.00 2023-12-27 00:00:00 10351.60 609.36 2077.49 1.00 2023-12-28 00:00:00 10356.59 606.83 2065.61 1.00 2023-12-29 00:00:00 10327.83 606.18 2062.98 1.00 We can see that we have daily close price data for all 4 asset classes from the beginning of 1990 to the end of 2023.\nExecute Strategy Using an annual rebalance date of January 1, we\u0026rsquo;ll now execute the strategy with the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # List of funds to be used fund_list = [\u0026#39;Stocks\u0026#39;, \u0026#39;Bonds\u0026#39;, \u0026#39;Gold\u0026#39;, \u0026#39;Cash\u0026#39;] # Starting cash contribution starting_cash = 10000 # Monthly cash contribution cash_contrib = 0 strat = strategy_harry_brown_perm_port( fund_list=fund_list, starting_cash=starting_cash, cash_contrib=cash_contrib, close_prices_df=perm_port, rebal_month=1, rebal_day=1, rebal_per_high=0.35, rebal_per_low=0.15, excel_export=True, pickle_export=True, output_confirmation=True, ) strat = strat.set_index(\u0026#39;Date\u0026#39;) This returns a dataframe with the entire strategy.\nRunning:\n1 df_info(strat) Gives us:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 The columns, shape, and data types are: \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; DatetimeIndex: 8479 entries, 1990-01-02 to 2023-12-29 Data columns (total 34 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Stocks_Close 8479 non-null float64 1 Bonds_Close 8479 non-null float64 2 Gold_Close 8479 non-null float64 3 Cash_Close 8479 non-null int64 4 Stocks_BA_Shares 8479 non-null float64 5 Stocks_BA_$_Invested 8479 non-null float64 6 Stocks_BA_Port_% 8479 non-null float64 7 Bonds_BA_Shares 8479 non-null float64 8 Bonds_BA_$_Invested 8479 non-null float64 9 Bonds_BA_Port_% 8479 non-null float64 10 Gold_BA_Shares 8479 non-null float64 11 Gold_BA_$_Invested 8479 non-null float64 12 Gold_BA_Port_% 8479 non-null float64 13 Cash_BA_Shares 8479 non-null float64 14 Cash_BA_$_Invested 8479 non-null float64 15 Cash_BA_Port_% 8479 non-null float64 16 Total_BA_$_Invested 8479 non-null float64 17 Contribution 8479 non-null int64 18 Rebalance 8479 non-null object 19 Stocks_AA_Shares 8479 non-null float64 20 Stocks_AA_$_Invested 8479 non-null float64 21 Stocks_AA_Port_% 8479 non-null float64 22 Bonds_AA_Shares 8479 non-null float64 23 Bonds_AA_$_Invested 8479 non-null float64 24 Bonds_AA_Port_% 8479 non-null float64 25 Gold_AA_Shares 8479 non-null float64 26 Gold_AA_$_Invested 8479 non-null float64 27 Gold_AA_Port_% 8479 non-null float64 28 Cash_AA_Shares 8479 non-null float64 29 Cash_AA_$_Invested 8479 non-null float64 30 Cash_AA_Port_% 8479 non-null float64 31 Total_AA_$_Invested 8479 non-null float64 32 Return 8478 non-null float64 33 Cumulative_Return 8478 non-null float64 dtypes: float64(31), int64(2), object(1) memory usage: 2.3+ MB The first 5 rows are:\nDate Stocks_Close Bonds_Close Gold_Close Cash_Close Stocks_BA_Shares Stocks_BA_$_Invested Stocks_BA_Port_% Bonds_BA_Shares Bonds_BA_$_Invested Bonds_BA_Port_% Gold_BA_Shares Gold_BA_$_Invested Gold_BA_Port_% Cash_BA_Shares Cash_BA_$_Invested Cash_BA_Port_% Total_BA_$_Invested Contribution Rebalance Stocks_AA_Shares Stocks_AA_$_Invested Stocks_AA_Port_% Bonds_AA_Shares Bonds_AA_$_Invested Bonds_AA_Port_% Gold_AA_Shares Gold_AA_$_Invested Gold_AA_Port_% Cash_AA_Shares Cash_AA_$_Invested Cash_AA_Port_% Total_AA_$_Invested Return Cumulative_Return 1990-01-02 00:00:00 386.16 99.97 399.00 1 6.47 2500.00 0.25 25.01 2500.00 0.25 6.27 2500.00 0.25 2500.00 2500.00 0.25 10000.00 0 No 6.47 2500.00 0.25 25.01 2500.00 0.25 6.27 2500.00 0.25 2500.00 2500.00 0.25 10000.00 nan nan 1990-01-03 00:00:00 385.17 99.73 395.00 1 6.47 2493.59 0.25 25.01 2494.02 0.25 6.27 2474.94 0.25 2500.00 2500.00 0.25 9962.55 0 No 6.47 2493.59 0.25 25.01 2494.02 0.25 6.27 2474.94 0.25 2500.00 2500.00 0.25 9962.55 -0.00 1.00 1990-01-04 00:00:00 382.02 99.81 396.50 1 6.47 2473.20 0.25 25.01 2496.02 0.25 6.27 2484.34 0.25 2500.00 2500.00 0.25 9953.56 0 No 6.47 2473.20 0.25 25.01 2496.02 0.25 6.27 2484.34 0.25 2500.00 2500.00 0.25 9953.56 -0.00 1.00 1990-01-05 00:00:00 378.30 99.77 405.00 1 6.47 2449.11 0.25 25.01 2494.92 0.25 6.27 2537.59 0.25 2500.00 2500.00 0.25 9981.63 0 No 6.47 2449.11 0.25 25.01 2494.92 0.25 6.27 2537.59 0.25 2500.00 2500.00 0.25 9981.63 0.00 1.00 1990-01-08 00:00:00 380.04 99.68 404.60 1 6.47 2460.38 0.25 25.01 2492.72 0.25 6.27 2535.09 0.25 2500.00 2500.00 0.25 9988.19 0 No 6.47 2460.38 0.25 25.01 2492.72 0.25 6.27 2535.09 0.25 2500.00 2500.00 0.25 9988.19 0.00 1.00 The last 5 rows are:\nDate Stocks_Close Bonds_Close Gold_Close Cash_Close Stocks_BA_Shares Stocks_BA_$_Invested Stocks_BA_Port_% Bonds_BA_Shares Bonds_BA_$_Invested Bonds_BA_Port_% Gold_BA_Shares Gold_BA_$_Invested Gold_BA_Port_% Cash_BA_Shares Cash_BA_$_Invested Cash_BA_Port_% Total_BA_$_Invested Contribution Rebalance Stocks_AA_Shares Stocks_AA_$_Invested Stocks_AA_Port_% Bonds_AA_Shares Bonds_AA_$_Invested Bonds_AA_Port_% Gold_AA_Shares Gold_AA_$_Invested Gold_AA_Port_% Cash_AA_Shares Cash_AA_$_Invested Cash_AA_Port_% Total_AA_$_Invested Return Cumulative_Return 2023-12-22 00:00:00 10292.37 604.17 2053.08 1 1.81 18595.87 0.29 25.03 15124.46 0.23 8.00 16426.12 0.25 14717.17 14717.17 0.23 64863.62 0 No 1.81 18595.87 0.29 25.03 15124.46 0.23 8.00 16426.12 0.25 14717.17 14717.17 0.23 64863.62 0.00 6.49 2023-12-26 00:00:00 10335.98 604.55 2067.81 1 1.81 18674.66 0.29 25.03 15134.20 0.23 8.00 16543.97 0.25 14717.17 14717.17 0.23 65070.01 0 No 1.81 18674.66 0.29 25.03 15134.20 0.23 8.00 16543.97 0.25 14717.17 14717.17 0.23 65070.01 0.00 6.51 2023-12-27 00:00:00 10351.60 609.36 2077.49 1 1.81 18702.89 0.29 25.03 15254.36 0.23 8.00 16621.42 0.25 14717.17 14717.17 0.23 65295.84 0 No 1.81 18702.89 0.29 25.03 15254.36 0.23 8.00 16621.42 0.25 14717.17 14717.17 0.23 65295.84 0.00 6.53 2023-12-28 00:00:00 10356.59 606.83 2065.61 1 1.81 18711.90 0.29 25.03 15191.10 0.23 8.00 16526.37 0.25 14717.17 14717.17 0.23 65146.54 0 No 1.81 18711.90 0.29 25.03 15191.10 0.23 8.00 16526.37 0.25 14717.17 14717.17 0.23 65146.54 -0.00 6.51 2023-12-29 00:00:00 10327.83 606.18 2062.98 1 1.81 18659.94 0.29 25.03 15175.01 0.23 8.00 16505.33 0.25 14717.17 14717.17 0.23 65057.44 0 No 1.81 18659.94 0.29 25.03 15175.01 0.23 8.00 16505.33 0.25 14717.17 14717.17 0.23 65057.44 -0.00 6.51 From the above, we can see that there are all columns for before/after re-balancing, including the shares, asset values, percentages, etc. for the four different asset classes.\nStrategy Statistics Let\u0026rsquo;s look at the summary statistics for the entire timeframe, as well as several different ranges:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 sum_stats = summary_stats( fund_list=fund_list, df=strat[[\u0026#39;Return\u0026#39;]], period=\u0026#34;Daily\u0026#34;, excel_export=True, pickle_export=True, output_confirmation=True, ) strat_pre_1999 = strat[strat.index \u0026lt; \u0026#39;2000-01-01\u0026#39;] sum_stats_pre_1999 = summary_stats( fund_list=fund_list, df=strat_pre_1999[[\u0026#39;Return\u0026#39;]], period=\u0026#34;Daily\u0026#34;, excel_export=False, pickle_export=False, output_confirmation=True, ) strat_post_1999 = strat[strat.index \u0026gt;= \u0026#39;2000-01-01\u0026#39;] sum_stats_post_1999 = summary_stats( fund_list=fund_list, df=strat_post_1999[[\u0026#39;Return\u0026#39;]], period=\u0026#34;Daily\u0026#34;, excel_export=False, pickle_export=False, output_confirmation=True, ) strat_post_2009 = strat[strat.index \u0026gt;= \u0026#39;2010-01-01\u0026#39;] sum_stats_post_2009 = summary_stats( fund_list=fund_list, df=strat_post_2009[[\u0026#39;Return\u0026#39;]], period=\u0026#34;Daily\u0026#34;, excel_export=False, pickle_export=False, output_confirmation=True, ) And the concat them to make comparing them easier:\n1 2 3 4 5 6 7 8 9 all_sum_stats = pd.concat([sum_stats]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;1990 - 2023\u0026#39;}) all_sum_stats = pd.concat([all_sum_stats, sum_stats_pre_1999]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;Pre 1999\u0026#39;}) all_sum_stats = pd.concat([all_sum_stats, sum_stats_post_1999]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;Post 1999\u0026#39;}) all_sum_stats = pd.concat([all_sum_stats, sum_stats_post_2009]) all_sum_stats = all_sum_stats.rename(index={\u0026#39;Return\u0026#39;: \u0026#39;Post 2009\u0026#39;}) display(all_sum_stats) Which gives us:\nAnnualized Mean Annualized Volatility Annualized Sharpe Ratio CAGR Daily Max Return Daily Max Return (Date) Daily Min Return Daily Min Return (Date) Max Drawdown Peak Bottom Recovery Date 1990 - 2023 0.057 0.060 0.957 0.057 0.029 2020-03-24 00:00:00 -0.030 2020-03-12 00:00:00 -0.154 2008-03-18 00:00:00 2008-11-12 00:00:00 2009-10-06 00:00:00 Pre 1999 0.060 0.050 1.207 0.061 0.022 1999-09-28 00:00:00 -0.018 1993-08-05 00:00:00 -0.062 1998-07-20 00:00:00 1998-08-31 00:00:00 1998-11-05 00:00:00 Post 1999 0.056 0.064 0.883 0.056 0.029 2020-03-24 00:00:00 -0.030 2020-03-12 00:00:00 -0.154 2008-03-18 00:00:00 2008-11-12 00:00:00 2009-10-06 00:00:00 Post 2009 0.056 0.060 0.927 0.056 0.029 2020-03-24 00:00:00 -0.030 2020-03-12 00:00:00 -0.127 2021-12-27 00:00:00 2022-10-20 00:00:00 2023-12-01 00:00:00 Annual Returns Here\u0026rsquo;s the annual returns:\nYear Return 1991 0.102 1992 0.030 1993 0.099 1994 -0.017 1995 0.153 1996 0.049 1997 0.056 1998 0.102 1999 0.039 2000 0.000 2001 -0.005 2002 0.043 2003 0.121 2004 0.051 2005 0.064 2006 0.104 2007 0.117 2008 -0.033 2009 0.107 2010 0.137 2011 0.070 2012 0.068 2013 -0.006 2014 0.052 2015 -0.018 2016 0.052 2017 0.095 2018 -0.012 2019 0.145 2020 0.134 2021 0.057 2022 -0.082 2023 0.109 Since the strategy, summary statistics, and annual returns are all exported as excel files, they can be found at the following locations:\nStocks_Bonds_Gold_Cash_Strategy.xlsx Stocks_Bonds_Gold_Cash_Summary_Stats.xlsx Stocks_Bonds_Gold_Cash_Annual_Returns.xlsx Next we will look at some plots to help visualize the data.\nGenerate Plots Here are the various functions needed for the plots:\nPlot Cumulative Return Plot cumulative return:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def plot_cumulative_return(strat_df): # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, strat_df[\u0026#39;Cumulative_Return\u0026#39;], label = \u0026#39;Strategy Cumulative Return\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;green\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 0.5 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.ylabel(\u0026#39;Cumulative Return\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(0, 7.5) # Set title, etc. plt.title(\u0026#39;Cumulative Return\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;03_Cumulative_Return.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Plot Portfolio Values Plot portfolio values:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def plot_values(strat_df): # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, strat_df[\u0026#39;Total_AA_$_Invested\u0026#39;], label=\u0026#39;Total Portfolio Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;black\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Stocks_AA_$_Invested\u0026#39;], label=\u0026#39;Stocks Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;orange\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Bonds_AA_$_Invested\u0026#39;], label=\u0026#39;Bond Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;yellow\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Gold_AA_$_Invested\u0026#39;], label=\u0026#39;Gold Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;blue\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Cash_AA_$_Invested\u0026#39;], label=\u0026#39;Cash Position Value\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;brown\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 5000 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:,.0f}\u0026#39;.format(x))) # Adding commas to y-axis labels plt.ylabel(\u0026#39;Total Value ($)\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(0, 75000) # Set title, etc. plt.title(\u0026#39;Total Values For Stocks, Bonds, Gold, and Cash Positions and Portfolio\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;04_Portfolio_Values.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Plot Portfolio Drawdown Plot portfolio drawdown:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def plot_drawdown(strat_df): rolling_max = strat_df[\u0026#39;Total_AA_$_Invested\u0026#39;].cummax() drawdown = (strat_df[\u0026#39;Total_AA_$_Invested\u0026#39;] - rolling_max) / rolling_max * 100 # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, drawdown, label=\u0026#39;Drawdown\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;red\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 1 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) # plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:,.0f}\u0026#39;.format(x))) # Adding commas to y-axis labels plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:.0f}\u0026#39;.format(x))) # Adding 0 decimal places to y-axis labels plt.ylabel(\u0026#39;Drawdown (%)\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(-20, 0) # Set title, etc. plt.title(\u0026#39;Portfolio Drawdown\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;05_Portfolio_Drawdown.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Plot Portfolio Asset Weights Plot portfolio asset weights:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def plot_asset_weights(strat_df): # Generate plot plt.figure(figsize=(10, 5), facecolor = \u0026#39;#F5F5F5\u0026#39;) # Plotting data plt.plot(strat_df.index, strat_df[\u0026#39;Stocks_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Stocks Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;orange\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Bonds_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Bonds Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;yellow\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Gold_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Gold Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;blue\u0026#39;, linewidth=1) plt.plot(strat_df.index, strat_df[\u0026#39;Cash_AA_Port_%\u0026#39;] * 100, label=\u0026#39;Cash Portfolio Weight\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;brown\u0026#39;, linewidth=1) # Set X axis # x_tick_spacing = 5 # Specify the interval for x-axis ticks # plt.gca().xaxis.set_major_locator(MultipleLocator(x_tick_spacing)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize = 9) plt.xticks(rotation = 45, fontsize = 7) # plt.xlim(, ) # Set Y axis y_tick_spacing = 2 # Specify the interval for y-axis ticks plt.gca().yaxis.set_major_locator(MultipleLocator(y_tick_spacing)) # plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: \u0026#39;{:,.0f}\u0026#39;.format(x))) # Adding commas to y-axis labels plt.ylabel(\u0026#39;Asset Weight (%)\u0026#39;, fontsize = 9) plt.yticks(fontsize = 7) plt.ylim(14, 36) # Set title, etc. plt.title(\u0026#39;Portfolio Asset Weights For Stocks, Bonds, Gold, and Cash Positions\u0026#39;, fontsize = 12) # Set the grid \u0026amp; legend plt.tight_layout() plt.grid(True) plt.legend(fontsize=8) # Save the figure plt.savefig(\u0026#39;07_Portfolio_Weights.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # Display the plot return plt.show() Execute plots:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 plot_cumulative_return(strat) plot_values(strat) plot_drawdown(strat) plot_asset_weights(strat) # Create dataframe for the annual returns strat_annual_returns = strat[\u0026#39;Cumulative_Return\u0026#39;].resample(\u0026#39;Y\u0026#39;).last().pct_change().dropna() strat_annual_returns_df = strat_annual_returns.to_frame() strat_annual_returns_df[\u0026#39;Year\u0026#39;] = strat_annual_returns_df.index.year # Add a \u0026#39;Year\u0026#39; column with just the year strat_annual_returns_df.reset_index(drop=True, inplace=True) # Reset the index to remove the datetime index # Now the DataFrame will have \u0026#39;Year\u0026#39; and \u0026#39;Cumulative_Return\u0026#39; columns strat_annual_returns_df = strat_annual_returns_df[[\u0026#39;Year\u0026#39;, \u0026#39;Cumulative_Return\u0026#39;]] # Keep only \u0026#39;Year\u0026#39; and \u0026#39;Cumulative_Return\u0026#39; columns strat_annual_returns_df.rename(columns = {\u0026#39;Cumulative_Return\u0026#39;:\u0026#39;Return\u0026#39;}, inplace=True) strat_annual_returns_df.set_index(\u0026#39;Year\u0026#39;, inplace=True) display(strat_annual_returns_df) plan_name = \u0026#39;_\u0026#39;.join(fund_list) file = plan_name + \u0026#34;_Annual_Returns.xlsx\u0026#34; location = file strat_annual_returns_df.to_excel(location, sheet_name=\u0026#39;data\u0026#39;) plot_annual_returns(strat_annual_returns_df) Here are several relevant plots:\nCumulative Return Portfolio Values (Total, Stocks, Bonds, Gold, and Cash) Here we can see the annual rebalancing taking effect with the values of the different asset classes. This can also be seen more clearly below.\nPortfolio Drawdown From this plot, we can see that the maximum drawdown came during the GFC; the drawdown during COVID was (interestingly) less than 10%.\nPortfolio Asset Weights The annual rebalancing appears to work effectively by selling assets that have increased in value and buying assets that have decreased in value over the previous year. Also note that there is only one instance when the weight of an asset fell to 15%. This occured for stocks during the GFC.\nPortfolio Annual Returns It\u0026rsquo;s interesting to see that there really aren\u0026rsquo;t any significant up or down years. Instead, it\u0026rsquo;s a steady climb without much volatility.\nSummary Overall, this is an interesting case study and Browne\u0026rsquo;s idea behind the Permanent Portfolio is certainly compelling. There might be more investigation to be done with respect to the following:\nInvestigate the extent to which the rebalancing date effects the portfolio performance Vary the weights of the asset classes to see if there is a meanful change in the results Experiment with leverage (i.e., simulating 1.2x leverage with a portfolio with weights of 30, 30, 30, 10 for stocks, bonds, gold, cash respectively.) Use ETFs instead of Bloomberg index data, and verify the results are similar. ETF data is much more available than the Bloomberg index. References Fail-Safe Investing: Lifelong Financial Security in 30 Minutes, by Harry Browne Code The jupyter notebook with the functions and all other code is available here. The html export of the jupyter notebook is available here. The pdf export of the jupyter notebook is available here.\n","date":"2024-11-04T00:00:01Z","image":"https://www.jaredszajkowski.com/2024/11/04/harry-browne-permanent-portfolio/cover.jpg","permalink":"https://www.jaredszajkowski.com/2024/11/04/harry-browne-permanent-portfolio/","title":"Does Harry Browne's permanent portfolio withstand the test of time?"},{"content":"Post Updates Update 4/12/2025: Revised script to accomodate a list of excluded directories.\nIntroduction While there are numerous backup solutions available for Linux, many require extensive configuration and maintenance, and restoring from the backup is not always simple. Incremental backups are ideal because they maintain snapshots of the files and allow for access to previous versions of files.\nLinux Journal recently published an article on various backup solutions, and I thought I\u0026rsquo;d provide my incremental backup script that uses rsync and cp.\nIncremental backup script This script provides an incremental backup solution and only requires rsync and cp to be installed on the system.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash # Define the directories to backup and their destination directories source_dir1=\u0026#34;/source1\u0026#34; backup_dir1=\u0026#34;/backup1/\u0026#34; source_dir2=\u0026#34;/source2\u0026#34; backup_dir2=\u0026#34;/backup2/\u0026#34; # Define excluded directories excluded_dir1=\u0026#34;leave/out/\u0026#34; excluded_dir2=\u0026#34;dont/want/\u0026#34; excluded_dir3=\u0026#34;exclude/this/\u0026#34; # Function to run a backup run_backup() { source_dir=$1 backup_dir=$2 # Check if the source directory exists if [ ! -d \u0026#34;$source_dir\u0026#34; ]; then echo \u0026#34;Error: Source directory not found\u0026#34; exit 1 fi # Input year and date echo \u0026#34;What is today\u0026#39;s year:\u0026#34; read backup_year echo \u0026#34;What is today\u0026#39;s date:\u0026#34; read backup_date # Check if the backup directory exists and run backup if [ -d \u0026#34;$backup_dir\u0026#34; ]; then echo \u0026#34;Backup directory found, backing up $source_dir\u0026#34; rsync -av --delete --exclude \u0026#34;$excluded_dir1\u0026#34; --exclude \u0026#34;$excluded_dir2\u0026#34; --exclude \u0026#34;$excluded_dir3\u0026#34; $source_dir $backup_dir/Monthly/ cp -al $backup_dir/Monthly/ $backup_dir/$backup_year/$backup_date/ else echo \u0026#34;Error: Backup directory not found\u0026#34; exit 1 fi } # Run backups run_backup $source_dir1 $backup_dir1 run_backup $source_dir2 $backup_dir2 # Output confirmation echo \u0026#34;Backup complete\u0026#34; Updated Incremental Backup Script Here\u0026rsquo;s the updated script, which now accomodates a list of excluded directories, along with a few other checks for the year and backup date.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 #!/bin/bash # Define the directories to backup and their destination directories source_dir1=\u0026#34;/source1\u0026#34; backup_dir1=\u0026#34;/backup1/\u0026#34; source_dir2=\u0026#34;/source2\u0026#34; backup_dir2=\u0026#34;/backup2/\u0026#34; # Define excluded directories excluded_dirs=( \u0026#34;/leave/out/\u0026#34; \u0026#34;/dont/want/\u0026#34; ) # Function to run a backup run_backup() { local source_dir=\u0026#34;$1\u0026#34; local backup_dir=\u0026#34;$2\u0026#34; # Check if the source directory exists if [ ! -d \u0026#34;$source_dir\u0026#34; ]; then echo \u0026#34;Error: Source directory \u0026#39;$source_dir\u0026#39; not found.\u0026#34; exit 2 fi # Input year and date echo \u0026#34;What is today\u0026#39;s year (YYYY):\u0026#34; read -r backup_year if [[ ! \u0026#34;$backup_year\u0026#34; =~ ^[0-9]{4}$ ]]; then echo \u0026#34;Error: Invalid year entered.\u0026#34; exit 3 fi echo \u0026#34;What is today\u0026#39;s date (YYYY-MM-DD):\u0026#34; read backup_date if [[ ! \u0026#34;$backup_date\u0026#34; =~ ^[0-9]{4}-[0-9]{2}-[0-9]{2}$ ]]; then echo \u0026#34;Error: Invalid date format. Use YYYY-MM-DD.\u0026#34; exit 4 fi # Check if the backup directory exists and run backup if [ -d \u0026#34;$backup_dir\u0026#34; ]; then echo \u0026#34;Backup directory \u0026#39;$backup_dir\u0026#39; found, backing up \u0026#39;$source_dir\u0026#39;...\u0026#34; # Build rsync exclude arguments exclude_args=() for dir in \u0026#34;${excluded_dirs[@]}\u0026#34;; do exclude_args+=(--exclude \u0026#34;$dir\u0026#34;) done rsync -av --delete \u0026#34;${exclude_args[@]}\u0026#34; \u0026#34;$source_dir\u0026#34; \u0026#34;$backup_dir/Monthly/\u0026#34; cp -al \u0026#34;$backup_dir/Monthly/\u0026#34; \u0026#34;$backup_dir/$backup_year/$backup_date/\u0026#34; else echo \u0026#34;Error: Backup directory \u0026#39;$backup_dir\u0026#39; not found.\u0026#34; exit 5 fi } # Run backups run_backup \u0026#34;$source_dir1\u0026#34; \u0026#34;$backup_dir1\u0026#34; run_backup \u0026#34;$source_dir2\u0026#34; \u0026#34;$backup_dir2\u0026#34; # Output confirmation echo \u0026#34;All backups completed successfully.\u0026#34; Let\u0026rsquo;s break this down line by line.\nSource and backup directories First, we need to define the source and backup directories, and any directories from the source that are to be excluded from the backup:\n1 2 3 4 5 6 7 8 9 10 11 12 # Define the directories to backup and their destination directories source_dir1=\u0026#34;/source1\u0026#34; backup_dir1=\u0026#34;/backup1/\u0026#34; source_dir2=\u0026#34;/source2\u0026#34; backup_dir2=\u0026#34;/backup2/\u0026#34; # Define excluded directories excluded_dirs=( \u0026#34;/leave/out/\u0026#34; \u0026#34;/dont/want/\u0026#34; ) You can add as many directories as you want here. The script compiles them before executing the rsync command.\nBackup function Then we have the backup function. This performs the following:\nTakes an input of the source and backup directories (defined above) Checks to see if the source directory exists Prompts for a year Prompts for a date Checks to make sure the backup destination directory exists Executes the backup 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # Function to run a backup run_backup() { local source_dir=\u0026#34;$1\u0026#34; local backup_dir=\u0026#34;$2\u0026#34; # Check if the source directory exists if [ ! -d \u0026#34;$source_dir\u0026#34; ]; then echo \u0026#34;Error: Source directory \u0026#39;$source_dir\u0026#39; not found.\u0026#34; exit 2 fi # Input year and date echo \u0026#34;What is today\u0026#39;s year (YYYY):\u0026#34; read -r backup_year if [[ ! \u0026#34;$backup_year\u0026#34; =~ ^[0-9]{4}$ ]]; then echo \u0026#34;Error: Invalid year entered.\u0026#34; exit 3 fi echo \u0026#34;What is today\u0026#39;s date (YYYY-MM-DD):\u0026#34; read backup_date if [[ ! \u0026#34;$backup_date\u0026#34; =~ ^[0-9]{4}-[0-9]{2}-[0-9]{2}$ ]]; then echo \u0026#34;Error: Invalid date format. Use YYYY-MM-DD.\u0026#34; exit 4 fi # Check if the backup directory exists and run backup if [ -d \u0026#34;$backup_dir\u0026#34; ]; then echo \u0026#34;Backup directory \u0026#39;$backup_dir\u0026#39; found, backing up \u0026#39;$source_dir\u0026#39;...\u0026#34; # Build rsync exclude arguments exclude_args=() for dir in \u0026#34;${excluded_dirs[@]}\u0026#34;; do exclude_args+=(--exclude \u0026#34;$dir\u0026#34;) done rsync -av --delete \u0026#34;${exclude_args[@]}\u0026#34; \u0026#34;$source_dir\u0026#34; \u0026#34;$backup_dir/Monthly/\u0026#34; cp -al \u0026#34;$backup_dir/Monthly/\u0026#34; \u0026#34;$backup_dir/$backup_year/$backup_date/\u0026#34; else echo \u0026#34;Error: Backup directory \u0026#39;$backup_dir\u0026#39; not found.\u0026#34; exit 5 fi } rsync is used to compare the files in the source to the Monthly backup directory and then update or delete files accordingly.\nOnce the files are copied over via rsync, then the cp command is used to link the files in the Monthly directory to the year/date/ diorectory. As the files change in the Monthly directory, then the link also changes. This method saves disk space because files are not copied over and over again. Any files that do not change are simply linked within the filesystem. The links take up a trivial amount of disk space, and the filesystem handles all of the heavy lifting associated with tracking which files are linked and where on the filesystem. There is no database, log, etc. required to track the individual files and/or their versions.\nRunning backups Finally, run the backups and confirm complete:\n1 2 3 4 5 6 # Run backups run_backup \u0026#34;$source_dir1\u0026#34; \u0026#34;$backup_dir1\u0026#34; run_backup \u0026#34;$source_dir2\u0026#34; \u0026#34;$backup_dir2\u0026#34; # Output confirmation echo \u0026#34;All backups completed successfully.\u0026#34; Results This script provides an incremental backup record organized by year and date:\nAccessing older backups is straightforward - simply navigate to the desired directory within the filesystem.\nDeleting old backups Deleting or removing old and out-of-date backups is as simple as deleting the directories. The filesystem links and files that are not linked elsewhere are removed from the filesystem, freeing up the disk space.\nReferences https://rsync.samba.org/ https://github.com/WayneD/rsync https://www.gnu.org/software/coreutils/ https://www.man7.org/linux/man-pages/man1/cp.1.html ","date":"2024-01-12T00:00:01Z","image":"https://www.jaredszajkowski.com/2024/01/12/simple-incremental-bash-backup-script/cover.jpg","permalink":"https://www.jaredszajkowski.com/2024/01/12/simple-incremental-bash-backup-script/","title":"Simple Incremental Bash Backup Script"},{"content":"Introduction In this tutorial, we will write a python function that pulls data from Nasdaq Data Link through the tables API, adds relevant columns that are not present in the raw data, updates columns to allow for ease of use, and leaves the data in a format where it can then be used in time series analysis.\nNasdaq Data Link is a provider of numerous different types of financial data from many different asset classes. It provides API\u0026rsquo;s that allow access from Python, R, Excel, and other methods. It is available to institutional investors as well as individual retail investors.\nNasdaq Data Link Initial Data Retrieval We will use the data for AAPL for this example. This will give us a data set that requires some thought as to how the splits need to be handled as well as the dividends.\nWe\u0026rsquo;ll start with pulling the initial data set, with the first 10 rows shown as follows from the pandas dataframe:\nAnd the last 10 rows:\nFrom left to right, we have the following columns:\nRow number: 0 indexed, gives us the total number of rows/dates of data Ticker: The ticker symbol for our data Date: In the format YYYY-MM-DD Open: Daily open High: Daily high Low: Daily low Close: Daily close Volume: Volume of shares traded Dividend: Dividend paid on that date Split: Split executed on that date Adjusted Open: Daily open price adusted for all splits and dividends Adjusted High: Daily high price adusted for all splits and dividends Adjusted Low: Daily low price adusted for all splits and dividends Adjusted Close: Daily close price adusted for all splits and dividends Adjusted Volume: Daily volume price adusted for all splits Data questions The above information is a good starting point, but what if we are looking for the following answers?\nThe data shows a split value for every day, but we know the stock didn\u0026rsquo;t split every day. What does this represent? What is the total cumulative split ratio? What is the split ratio at different points in time? What is the adjusted share price without including the dividends? This would be needed for any time series analysis. What is the dividend dollar value based on an adjusted share price? What would the share price be if the stock hadn\u0026rsquo;t split? We\u0026rsquo;ll add columns and modify as necessary to answer the above questions and more.\nAssumptions The remainder of this tutorial assumes the following:\nYou have the Nasdaq Data Link library installed You have the pandas library installed You have the OpenPyXL library installed Python function to modify the data The following function will perform the desired modifications:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # This function pulls the data for the specific fund from from Nasdaq Data # Link and adds many missing columns # Imports import nasdaqdatalink import pandas as pd import numpy as np # Add API key for reference to allow access to unrestricted data nasdaqdatalink.ApiConfig.api_key = \u0026#39;your_key\u0026#39; # Function definition def ndl_data_updater(fund): # Command to pull data # If start date and end date are not specified the entire data set is included df = nasdaqdatalink.get_table(\u0026#39;QUOTEMEDIA/PRICES\u0026#39;, ticker = fund, paginate=True) # Sort columns by date ascending df.sort_values(\u0026#39;date\u0026#39;, ascending = True, inplace = True) # Rename date column df.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Date\u0026#39;}, inplace = True) # Set index to date column df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Replace all split values of 1.0 with NaN df[\u0026#39;split\u0026#39;] = df[\u0026#39;split\u0026#39;].replace(1.0, np.nan) # Create a new data frame with split values only df_splits = df.drop(columns = {\u0026#39;ticker\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;, \u0026#39;dividend\u0026#39;, \u0026#39;adj_open\u0026#39;, \u0026#39;adj_high\u0026#39;, \u0026#39;adj_low\u0026#39;, \u0026#39;adj_close\u0026#39;, \u0026#39;adj_volume\u0026#39;}).dropna() # Create a new column for cumulative split df_splits[\u0026#39;Cum_Split\u0026#39;] = df_splits[\u0026#39;split\u0026#39;].cumprod() # Drop original split column before combining dataframes df_splits.drop(columns = {\u0026#39;split\u0026#39;}, inplace = True) # Merge df and df_split dataframes df_comp = pd.merge(df, df_splits, on=\u0026#39;Date\u0026#39;, how=\u0026#39;outer\u0026#39;) # Forward fill for all cumulative split values df_comp[\u0026#39;Cum_Split\u0026#39;].fillna(method = \u0026#39;ffill\u0026#39;, inplace = True) # Replace all split and cumulative split values of NaN with 1.0 to have complete split values df_comp[\u0026#39;split\u0026#39;] = df_comp[\u0026#39;split\u0026#39;].replace(np.nan, 1.0) df_comp[\u0026#39;Cum_Split\u0026#39;] = df_comp[\u0026#39;Cum_Split\u0026#39;].replace(np.nan, 1.0) # Calculate the non adjusted prices based on the splits only df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] = df_comp[\u0026#39;open\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] = df_comp[\u0026#39;high\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] = df_comp[\u0026#39;low\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] = df_comp[\u0026#39;close\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] # Calculate the adjusted prices based on the splits df_comp[\u0026#39;Open\u0026#39;] = df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;High\u0026#39;] = df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Low\u0026#39;] = df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Close\u0026#39;] = df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend\u0026#39;] = df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend_Pct_Orig\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] / df_comp[\u0026#39;close\u0026#39;] df_comp[\u0026#39;Dividend_Pct_Adj\u0026#39;] = df_comp[\u0026#39;Dividend\u0026#39;] / df_comp[\u0026#39;Close\u0026#39;] # Export data to excel file = fund + \u0026#34;_NDL.xlsx\u0026#34; df_comp.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df_comp[-1:]) print(f\u0026#34;NDL data updater complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Let\u0026rsquo;s break this down line by line.\nImports First, we need to import the required libraries:\n1 2 3 4 # Imports import nasdaqdatalink import pandas as pd import numpy as np NDL API Key To gain access to anything beyond the free tier, you will need to provide your access key:\n1 2 # Add API key for reference to allow access to unrestricted data nasdaqdatalink.ApiConfig.api_key = \u0026#39;your_key\u0026#39; Download data as a dataframe Moving on to the function definition, we have the command to pull data from NDL. There are two separate APIs - the time series and the tables. The syntax is different, and some data sets are only available as one or the other. We will use the tables API for this tutorial.\n1 2 3 # Command to pull data # If start date and end date are not specified the entire data set is included df = nasdaqdatalink.get_table(\u0026#39;QUOTEMEDIA/PRICES\u0026#39;, ticker = fund, paginate=True) In the example above, the fund is an input parameter to the function.\nThe 'QUOTEMEDIA/PRICES' is the data source that we are accessing.\nThere are many other arguments that we could pass in the above, including specifying columns, period start date, period end date, and others. Nasdaq as a few examples to get you started:\nhttps://docs.data.nasdaq.com/docs/python-tables\nRunning:\ndf.head(10) Gives us:\nSort columns by date Next, we will sort the columns by date ascending. By default, the dataframe is created with the data sorted by descending date, and we want to change that:\n1 2 # Sort columns by date ascending df.sort_values(\u0026#39;date\u0026#39;, ascending = True, inplace = True) The inplace = True argument specifies that the sort function should take effect on the existing dataframe.\nNow, running:\ndf.head(10) Gives us:\nSetting the date as the index Next, we will rename the date column from \u0026lsquo;date\u0026rsquo; to \u0026lsquo;Date\u0026rsquo;, and set the index to be the Date column:\n1 2 3 4 5 # Rename date column df.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Date\u0026#39;}, inplace = True) # Set index to date column df.set_index(\u0026#39;Date\u0026#39;, inplace = True) Now, running:\ndf.head(10) Gives us:\nCalculating splits The next sections deal with the split column. So far we have only seen a split value of 1.0 in the data, but we\u0026rsquo;ve only looked at the first 10 and last 10 rows. Are there any other values? Let\u0026rsquo;s check by running:\n1 df_not_1_split = df[df[\u0026#39;split\u0026#39;] != 1.0] And checking the first 10 rows:\ndf_not_1_split.head(10) Gives us:\nSo we now know that the stock did in fact split several times. Next, we will replace all of the 1.0 split values - because they are really meaningless - and then create a new dataframe to deal with the splits.\n1 2 # Replace all split values of 1.0 with NaN df[\u0026#39;split\u0026#39;] = df[\u0026#39;split\u0026#39;].replace(1.0, np.nan) This gives us:\nWe will now create a dataframe with only the split values:\n1 2 3 4 # Create a new data frame with split values only df_splits = df.drop(columns = {\u0026#39;ticker\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;, \u0026#39;dividend\u0026#39;, \u0026#39;adj_open\u0026#39;, \u0026#39;adj_high\u0026#39;, \u0026#39;adj_low\u0026#39;, \u0026#39;adj_close\u0026#39;, \u0026#39;adj_volume\u0026#39;}).dropna() Which gives us:\nCreating a column for the cumulative split will provide an accurate perspective on the stock price. We can do that with the following:\n1 2 # Create a new column for cumulative split df_splits[\u0026#39;Cum_Split\u0026#39;] = df_splits[\u0026#39;split\u0026#39;].cumprod() Which gives us:\nWe will then drop the original split column before combining the split data frame with the original data frame, as follows:\n1 2 # Drop original split column before combining dataframes df_splits.drop(columns = {\u0026#39;split\u0026#39;}, inplace = True) Which gives us:\nCombining dataframes Now we will merge the df_split dataframe with the original df dataframe so that the cumulative split column is part of the original dataframe. We will call this data frame df_comp:\n1 2 # Merge df and df_split dataframes df_comp = pd.merge(df, df_splits, on=\u0026#39;Date\u0026#39;, how=\u0026#39;outer\u0026#39;) We are using the merge function of pandas, which includes arguments for the names of both dataframes to be merged, the column to match between the dataframes, and the parameter for the type of merge to be performed. The outer argument specifies that all rows from both dataframes will be included, and any missing values will be filled in with NaN if there is no matching data. This ensures that all data from both dataframes is retained.\nRunning:\ndf_comp.head(10) Gives us:\nForward filling cumulative split values From here, we want to fill in the rest of the split and Cum_Split values. This is done using the forward fill function, which for all cells that have a value of NaN will fill in the previous valid value until another value is encountered. Here\u0026rsquo;s the code:\n1 2 # Forward fill for all cumulative split values df_comp[\u0026#39;Cum_Split\u0026#39;].fillna(method = \u0026#39;ffill\u0026#39;, inplace = True) Running:\ndf_comp.head(10) Gives us:\nAt first glance, it doesn\u0026rsquo;t look like anything changed. That\u0026rsquo;s because there wasn\u0026rsquo;t any ffill action taken on the initial values until pandas encountered a valid value to then forward fill. However, checking the last 10 rows:\ndf_comp.tail(10) Gives us:\nWhich is the result that we were expecting. But, what about the first rows from 12/12/1980 to 6/15/1987? We can fill those split and Cum_Split values with the following code:\n1 2 3 # Replace all split and cumulative split values of NaN with 1.0 to have complete split values df_comp[\u0026#39;split\u0026#39;] = df_comp[\u0026#39;split\u0026#39;].replace(np.nan, 1.0) df_comp[\u0026#39;Cum_Split\u0026#39;] = df_comp[\u0026#39;Cum_Split\u0026#39;].replace(np.nan, 1.0) Now, checking the first 10 rows:\ndf_comp.head(10) Gives us:\nWith this data, we now know for every day in the data set the following pieces of information:\nIf the stock split on that day What the total split ratio is up to and including that day Calculating adjusted and non-adjusted prices From here, we can complete our dataset by calculating the adjusted and non-adjusted prices using the cumulative split ratios from above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Calculate the non adjusted prices based on the splits only df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] = df_comp[\u0026#39;open\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] = df_comp[\u0026#39;high\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] = df_comp[\u0026#39;low\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] = df_comp[\u0026#39;close\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] # Calculate the adjusted prices based on the splits df_comp[\u0026#39;Open\u0026#39;] = df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;High\u0026#39;] = df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Low\u0026#39;] = df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Close\u0026#39;] = df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend\u0026#39;] = df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend_Pct_Orig\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] / df_comp[\u0026#39;close\u0026#39;] df_comp[\u0026#39;Dividend_Pct_Adj\u0026#39;] = df_comp[\u0026#39;Dividend\u0026#39;] / df_comp[\u0026#39;Close\u0026#39;] Included above is the adjusted dividends values. For any time series analysis, not only are the adjusted prices needed, but so are the adusted dividends. Remember, we already have the adjusted total return prices - those come directly from NDL.\nExport data Next, we want to export the data to an excel file, for easy viewing and reference later:\n1 2 3 # Export data to excel file = fund + \u0026#34;_NDL.xlsx\u0026#34; df_comp.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) And verify the output is as expected:\nOutput confirmation Finally, we want to print a confirmation that the process succeeded along withe last date we have for data:\n1 2 3 4 5 # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df_comp[-1:]) print(f\u0026#34;NDL data updater complete for {fund} data\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) And confirming the output:\nReferences https://docs.data.nasdaq.com/docs https://docs.data.nasdaq.com/docs/tables-1 https://docs.data.nasdaq.com/docs/time-series https://docs.data.nasdaq.com/docs/python\n","date":"2023-12-24T00:00:01Z","image":"https://www.jaredszajkowski.com/2023/12/24/nasdaq-data-link-tables-api-data-retrieval/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/12/24/nasdaq-data-link-tables-api-data-retrieval/","title":"Nasdaq Data Link Tables API Data Retrieval"},{"content":"Introduction In this tutorial, we will write a python function that imports an excel export from Bloomberg, removes ancillary rows and columns, and leaves the data in a format where it can then be used in time series analysis.\nExample of a Bloomberg excel export We will use the SPX index data in this example. Exporting the data from Bloomberg using the excel Bloomberg add-on yields data in the following format:\nData modifications The above format isn\u0026rsquo;t horrible, but we want to perform the following modifications:\nRemove the first six rows of the data Convert the 7th row to become column headings Rename column 2 to \u0026ldquo;Close\u0026rdquo; to represent the closing price Remove column 3, as we are not concerned about volume Export to excel and make the name of the excel worksheet \u0026ldquo;data\u0026rdquo; Assumptions The remainder of this tutorial assumes the following:\nYour excel file is named \u0026ldquo;SPX_Index.xlsx\u0026rdquo; The worksheet in the excel file is named \u0026ldquo;Worksheet\u0026rdquo; You have the pandas library installed You have the OpenPyXL library installed Python function to modify the data The following function will perform the modifications mentioned above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # This function takes an excel export from Bloomberg and # removes all excess data leaving date and close columns # Imports import pandas as pd # Function definition def bb_data_updater(fund): # File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Let\u0026rsquo;s break this down line by line.\nImports First, we need to import pandas:\n1 import pandas as pd Import excel data file Then import the excel file as a pandas dataframe:\n1 2 3 4 5 # File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) Running:\ndf.head(10) Gives us:\nSet column headings Next, set the column heading:\n1 2 # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] Now, running:\ndf.head(10) Gives us:\nRemove index heading Next, remove the column heading from the index column:\n1 2 # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) Note: The axis=1 argument here specifies the column index.\nNow, running:\ndf.head(10) Gives us:\nDrop rows Next, we want to remove the first 6 rows that have unneeded data:\n1 2 # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) Note: When dropping rows, the range to drop begins with row 0 and continues up to - but not including - row 6.\nNow, running:\ndf.head(10) Gives us:\nSet index Next, we want to set the date column as the index:\n1 2 # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) Now, running:\ndf.head(10) Gives us:\nDrop the \u0026ldquo;PX_VOLUME\u0026rdquo; column Next, we want to drop the volume column:\n1 2 3 4 5 # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass For some data records, the volume column does not exist. Therefore, we try, and if it fails with a KeyError, then we assume the \u0026ldquo;PX_VOLUME\u0026rdquo; column does not exist, and just pass to move on.\nNow, running:\ndf.head(10) Gives us:\nRename the \u0026ldquo;PX_LAST\u0026rdquo; column Next, we want to rename the \u0026ldquo;PX_LAST\u0026rdquo; column as \u0026ldquo;Close\u0026rdquo;:\n1 2 # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) Now, running:\ndf.head(10) Gives us:\nSort data Next, we want to sort the data starting with the oldest date:\n1 2 # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) Now, running:\ndf.head(10) Gives us:\nExport data Next, we want to export the data to an excel file, for easy viewing and reference later:\n1 2 3 # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) And verify the output is as expected:\nOutput confirmation Finally, we want to print a confirmation that the process succeeded along withe last date we have for data:\n1 2 3 4 5 # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) And confirming the output:\nReferences https://www.bloomberg.com/professional/support/software-updates/\n","date":"2023-11-15T00:00:01Z","image":"https://www.jaredszajkowski.com/2023/11/15/cleaning-bloomberg-excel-export/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/11/15/cleaning-bloomberg-excel-export/","title":"Cleaning A Bloomberg Data Excel Export"},{"content":"Introduction Here are my notes for some of the more commonly used git commands along with initial setup for git in Linux.\nInstallation To begin, install as follows for Arch Linux:\n$ sudo pacman -Sy git Or\n$ yay git Pacman will include all required depencies.\nInitial configuration First, set your name and email address:\n$ git config --global user.name \u0026quot;Firstname Lastname\u0026quot; $ git config --global user.email \u0026quot;email@address.com\u0026quot; Then, set your preferred text editor (if you have one). I use nano:\n$ git config --global core.editor \u0026quot;nano\u0026quot; You can verify the updates with:\n$ git config --global core.editor Alternatively, you can edit the git configuration directly with:\n$ git config --global --edit Store credentials In 2021, GitHub disabled authentication via password and now requires authentication with a token. The following command sets up the credential helper, where it will store your token in ~/.git-credentials:\n$ git config --global credential.helper store After you log in during a git push with your username and token, the username or email address and token will be stored in the above location.\nNote: The token is stored in plain text, so use caution if that is a concern.\nCloning repositories Repositories can be cloned with the following:\n$ git clone https://github.com/\u0026lt;username\u0026gt;/\u0026lt;repository\u0026gt;.git Updating repositories The local record of a repository can be updated with the following command:\n$ cd \u0026lt;repository\u0026gt;/ $ git pull Adding, committing, and pushing Any files or directories that have been added, modified, or removed can be add to the list of changes to be pushed with the following command:\n$ git add . This function stages files that have been modified and deleted but new files that you have not added are not affected:\n$ git commit -a This function commits any staged changes:\n$ git commit -m \u0026quot;message\u0026quot; These arguments can be stacked as follows:\n$ git commit -am \u0026quot;Add your commit message here\u0026quot; Note: Without add, commit will handle any changes to files that have been modified or deleted, but will not incorporate any files that have been created.\nThen finally pushed:\n$ git push If, for some reason, you would like to reset a commit:\n$ git reset These commands can be chained together with the AND operator:\n$ git add . \u0026amp;\u0026amp; git commit -am \u0026quot;Add your commit message here\u0026quot; \u0026amp;\u0026amp; git push Stashing changes If you forget to update a repository before making changes, you can \u0026ldquo;stash\u0026rdquo; those changes and then re-apply them after running git pull.\nFirst, stash the changes:\n$ git stash Then, update the local record of the repository:\n$ git pull Finally, re-apply the changes you previously made:\n$ git stash apply This has proven to be very useful for me when I forget to update a repository before making edits to the code.\nReferences https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup#_first_time https://git-scm.com/book/en/v2/Appendix-C%3A-Git-Commands-Setup-and-Config https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage#_credential_caching https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning https://www.geeksforgeeks.org/difference-between-chaining-operators-in-linux/ ","date":"2023-10-16T00:00:00Z","image":"https://www.jaredszajkowski.com/2023/10/16/git-quick-start-guide/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/10/16/git-quick-start-guide/","title":"Git Quick Start Guide"},{"content":"Introduction If anyone uses Zoom to record or Panopto to host recordings and later wants to access the recordings, here\u0026rsquo;s a simple linux bash script to download the video file and acompanying subtitles. For a while I used zoomdl, but it is no longer under active development, and I began running into various issues about a year ago. I stumbled upon yt-dlp and found it under active development and quite extensive.\nThis tutorial requires you to have a \u0026ldquo;cookies\u0026rdquo; text file, which needs to contain the cookies export in the Netscape HTTP format of the Zoom cookies after logging in.\nInstall cookie editor Install the cookie editor extension. I personnally use it with Microsoft Edge, but there are similar extensions for Chrome, Firefox, etc.\nModify export format Change the preferred cookie export format to Netscape HTTP Cookie File in the extension options. It is necessary to export in this format, otherwise yt-dlp will not be able to read the cookies.txt file correctly.\nLog in to Zoom or Panopto Log in to Zoom or Panopto in your browser. Be sure to remain logged in while exporting the cookies.\nExport cookies The export button is at the top fo the window. It copies the cookies to your clipboard, which then need to be pasted into a text file (I have my saved as cookies.txt), which yt-dlp will then read when it executes.\nInstall yt-dlp In Arch Linux, yt-dlp can be found with:\n$ yay yt-dlp Or:\n$ sudo pacman -Sy yt-dlp Create bash script for Zoom Save the following code to a text file (my bash script file name is yt-dlp-zoom.sh):\n1 2 3 4 5 6 #!/bin/bash echo What is the link? read link yt-dlp --referer \u0026#34;https://zoom.us/\u0026#34; --cookies /path/to/cookies/file/cookies_zoom.txt -o \u0026#34;%(title)s-%(id)s.%(ext)s\u0026#34; --write-subs $link Create bash script for Panopto Save the following code to a text file (my bash script file name is yt-dlp-panopto.sh):\n1 2 3 4 5 6 #!/bin/bash echo What is the link? read link yt-dlp --cookies /path/to/cookies/file/cookies_panopto.txt -o \u0026#34;%(title)s-%(id)s.%(ext)s\u0026#34; --write-subs $link Change permissions Modify the permissions of the bash scripts to allow execution:\n$ chmod +x yt-dlp-zoom.sh $ chmod +x yt-dlp-panopto.sh Execute the scripts Execute the bash script with either ./yt-dlp-zoom.sh or ./yt-dlp-panopto.sh, copy and paste the link into the shell prompt for the video that you would like to save, and it should download the video and the subtitles.\nReferences https://ostechnix.com/yt-dlp-tutorial/ ","date":"2023-10-01T00:00:00Z","image":"https://www.jaredszajkowski.com/2023/10/01/yt-dlp-with-zoom-and-panopto/banner_1.svg","permalink":"https://www.jaredszajkowski.com/2023/10/01/yt-dlp-with-zoom-and-panopto/","title":"Using yt-dlp With Zoom And Panopto"},{"content":"Introduction This is the basic framework that I use to install Arch Linux, with a few changes catered to the Lenovo ThinkPad E15 Gen 2. I have found that this is a decent mid range laptop, excellent linux compatibility, great keyboard, and overall provides a good value.\nGetting started This tutorial assumes the following:\nYou are booting from a USB drive with the Arch install ISO. Wireless or wired network is detected and drivers are configured automatically. You want drive encrytion on your root partition, but not on your boot/efi/swap partitions. Verify UEFI boot mode The following command should show directory without error:\n# ls /sys/firmware/efi/efivars Configure wireless network The following command will drop you into the iwd daemon:\n# iwctl From there:\n# device list # station *device* scan # station *device* get-networks # station *device* connect *SSID* Verify internet connectivity # ping archlinux.org Update system clock # timedatectl set-ntp true # timedatectl status Disks, partition table \u0026amp; partitions The following assumes that your NVME drive is found as /dev/nvme0n1. Partitions will then be /dev/nvme0n1p1 and so on.\nWipe disk List disks:\n# fdisk -l Wipe all file system records:\n# wipefs -a /dev/nvme0n1 Create new partition table Open nvme0n1 with gdisk:\n# gdisk /dev/nvme0n1 Create GPT partition table with option \u0026ldquo;o\u0026rdquo;.\nCreate EFI partition Create new EFI partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #1 Default starting sector +550M Change partition type to EFI System (ef00) Create boot partition Create new boot partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #2 Default starting sector +550M Leave default type of 8300 Create swap partition The old rule of thumb used to be that a swap partition should be the same size as the amount of memory in the system, but given the typical amount of memory in modern systems this is obviously no longer necessary. For my system with 16 or 32 GB of memory, a swap of 8 GB is rarely even used.\nCreate new Swap partition w/ 8GB with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #3 Default starting sector +8G Change to linux swap (8200) Create root partition Create new root partition w/ remaining disk space with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #4 Default starting sector Remaining space Linux LUKS type 8309 And then exit gdisk.\nWrite file systems EFI partition Write file system to new EFI System partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p1 # mkfs.fat -F32 /dev/nvme0n1p1 Boot partition Then boot partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p2 # mkfs.ext2 /dev/nvme0n1p2 Root partition Prepare root partition w/ LUKS:\n# cryptsetup -y -v luksFormat --type luks2 /dev/nvme0n1p4 # cryptsetup luksDump /dev/nvme0n1p4 # cryptsetup open /dev/nvme0n1p4 archcryptroot # mkfs.ext4 /dev/mapper/archcryptroot # mount /dev/mapper/archcryptroot /mnt I use archcryptroot for the name of my encrypted volume, but change as necessary.\nSwap partition Then swap:\n# mkswap /dev/nvme0n1p3 # swapon /dev/nvme0n1p3 Create mount points # mkdir /mnt/boot # mount /dev/nvme0n1p2 /mnt/boot # mkdir /mnt/boot/efi # mount /dev/nvme0n1p1 /mnt/boot/efi System install Install base packages # pacstrap /mnt base base-devel linux linux-firmware grub-efi-x86_64 efibootmgr Generate fstab # genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Enter new system # arch-chroot /mnt /bin/bash Set clock # ln -sf /usr/share/zoneinfo/America/Chicago /etc/localtime # hwclock –systohc Generate locale In /etc/locale.gen uncomment only: en_US.UTF-8 UTF-8\n# locale-gen In /etc/locale.conf, you should only have this line: LANG=en_US.UTF-8\n# nano /etc/locale.conf Set hostname \u0026amp; update hosts # echo linuxmachine \u0026gt; /etc/hostname Update /etc/hosts with the following:\n127.0.0.1 localhost ::1 localhost 127.0.1.1 linuxmachine.localdomain linuxmachine Set root password # passwd Update /etc/mkinitcpio.conf \u0026amp; generate initrd image Edit /etc/mkinitcpio.conf with the following:\nHOOKS=(base udev autodetect modconf block keymap encrypt resume filesystems keyboard fsck) Then run:\n# mkinitcpio -p linux Install grub # grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=ArchLinux Edit /etc/default/grub so it includes a statement like this:\nGRUB_CMDLINE_LINUX=\u0026quot;cryptdevice=/dev/nvme0n1p4:archcryptroot resume=/dev/nvme0n1p3\u0026quot; Generate final grub configuration:\n# grub-mkconfig -o /boot/grub/grub.cfg Exit \u0026amp; reboot # exit # umount -R /mnt # swapoff -a # reboot To be continued.\n","date":"2023-09-29T00:00:01Z","image":"https://www.jaredszajkowski.com/2023/09/29/arch-linux-install/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/09/29/arch-linux-install/","title":"Arch Linux Install"},{"content":"Hello World Welcome to my website. This is meant to serve as a place for me to publish various posts from my explorations into Arch Linux, data science, quant finance, and other topics.\nThe theme has been adopted from the Hugo Theme Stack produced by Jimmy Cai.\nThis is the only theme that I have found that checks all of the following boxes:\nTheme for the static site generator Hugo Includes modules for archives Includes tags and topics/categories Includes built-in search functionality Simple interface that is easily navigable Highly extensible including modules for image galleries, posts, comment capabilities, etc. It is hosted on GitHub pages. I followed the install instructions that the theme author provided, including using GitHub codespace for editing in the cloud. There are only a few details that I ran into that he did not mention.\nDon\u0026rsquo;t forget to run Hugo to build the site. This creates the public directory, which is where the static site files are located. Make sure to update the branch to be gh-pages under Settings -\u0026gt; Pages -\u0026gt; Build and deployment -\u0026gt; Branch in GitHub. Make sure to remove the public directory from the .gitignore file. Otherwise GitHub will ignore the public directory and your site will show the README.md instead of the Hugo site. The site can be updated either through codespace, or locally as long as Hugo and it\u0026rsquo;s required dependencies have been installed.\nUpdating and pushing changes The simple command after making any changes and to push those updates is as follows:\n$ hugo \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -am \u0026quot;Updating site\u0026quot; \u0026amp;\u0026amp; git push This can be put in a bash script to make it easier. Save the following as git-update.sh:\n1 2 3 4 5 6 #!/bin/bash echo What is the commit message? read message hugo \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -am \u0026#34;$message\u0026#34; \u0026amp;\u0026amp; git push Change permissions:\n$ chmod +x git-update.sh And then execute:\n$ ./git-update.sh References Here\u0026rsquo;s the full list of resources I referenced for deploying Hugo with GitHub pages:\nhttps://www.o11ycloud.com/posts/gh_hugo/ https://github.com/CaiJimmy/hugo-theme-stack https://medium.com/@magstherdev/github-pages-hugo-86ae6bcbadd\n","date":"2023-09-26T00:00:00Z","image":"https://www.jaredszajkowski.com/2023/09/26/hello-world/cover.jpg","permalink":"https://www.jaredszajkowski.com/2023/09/26/hello-world/","title":"Hello World"}]