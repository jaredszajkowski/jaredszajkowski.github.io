[{"content":"Introduction In this tutorial, we will write a python function that pulls data from Nasdaq Data Link through the tables API, adds relevant columns that are not present in the raw data, updates columns to allow for ease of use, and leaves the data in a format where it can then be used in time series analysis.\nNasdaq Data Link is a provider of numerous different types of financial data from many different asset classes. It provides API\u0026rsquo;s that allow access from Python, R, Excel, and other methods. It is available to institutional investors as well as individual retail investors.\nNasdaq Data Link Initial Data Retrieval We will use the data for AAPL for this example. This will give us a data set that requires some thought as to how the splits need to be handled as well as the dividends.\nWe\u0026rsquo;ll start with pulling the initial data set, with the first 10 rows shown as follows from the pandas dataframe:\nAnd the last 10 rows:\nFrom left to right, we have the following columns:\nRow number: 0 indexed, gives us the total number of rows/dates of data Ticker: The ticker symbol for our data Date: In the format YYYY-MM-DD Open: Daily open High: Daily high Low: Daily low Close: Daily close Volume: Volume of shares traded Dividend: Dividend paid on that date Split: Split executed on that date Adjusted Open: Daily open price adusted for all splits and dividends Adjusted High: Daily high price adusted for all splits and dividends Adjusted Low: Daily low price adusted for all splits and dividends Adjusted Close: Daily close price adusted for all splits and dividends Adjusted Volume: Daily volume price adusted for all splits Data questions The above information is a good starting point, but what if we are looking for the following answers?\nThe data shows a split value for every day, but we know the stock didn\u0026rsquo;t split every day. What does this represent? What is the total cumulative split ratio? What is the split ratio at different points in time? What is the adjusted share price without including the dividends? This would be needed for any time series analysis. What is the dividend dollar value based on an adjusted share price? What would the share price be if the stock hadn\u0026rsquo;t split? We\u0026rsquo;ll add columns and modify as necessary to answer the above questions and more.\nAssumptions The remainder of this tutorial assumes the following:\nYou have the Nasdaq Data Link library installed You have the pandas library installed You have the OpenPyXL library installed Python function to modify the data The following function will perform the desired modifications:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # This function pulls the data for the specific fund from from Nasdaq Data # Link and adds many missing columns # Imports import nasdaqdatalink import pandas as pd import numpy as np # Add API key for reference to allow access to unrestricted data nasdaqdatalink.ApiConfig.api_key = \u0026#39;your_key\u0026#39; # Function definition def ndl_data_updater(fund): # Command to pull data # If start date and end date are not specified the entire data set is included df = nasdaqdatalink.get_table(\u0026#39;QUOTEMEDIA/PRICES\u0026#39;, ticker = fund, paginate=True) # Sort columns by date ascending df.sort_values(\u0026#39;date\u0026#39;, ascending = True, inplace = True) # Rename date column df.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Date\u0026#39;}, inplace = True) # Set index to date column df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Replace all split values of 1.0 with NaN df[\u0026#39;split\u0026#39;] = df[\u0026#39;split\u0026#39;].replace(1.0, np.nan) # Create a new data frame with split values only df_splits = df.drop(columns = {\u0026#39;ticker\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;, \u0026#39;dividend\u0026#39;, \u0026#39;adj_open\u0026#39;, \u0026#39;adj_high\u0026#39;, \u0026#39;adj_low\u0026#39;, \u0026#39;adj_close\u0026#39;, \u0026#39;adj_volume\u0026#39;}).dropna() # Create a new column for cumulative split df_splits[\u0026#39;Cum_Split\u0026#39;] = df_splits[\u0026#39;split\u0026#39;].cumprod() # Drop original split column before combining dataframes df_splits.drop(columns = {\u0026#39;split\u0026#39;}, inplace = True) # Merge df and df_split dataframes df_comp = pd.merge(df, df_splits, on=\u0026#39;Date\u0026#39;, how=\u0026#39;outer\u0026#39;) # Forward fill for all cumulative split values df_comp[\u0026#39;Cum_Split\u0026#39;].fillna(method = \u0026#39;ffill\u0026#39;, inplace = True) # Replace all split and cumulative split values of NaN with 1.0 to have complete split values df_comp[\u0026#39;split\u0026#39;] = df_comp[\u0026#39;split\u0026#39;].replace(np.nan, 1.0) df_comp[\u0026#39;Cum_Split\u0026#39;] = df_comp[\u0026#39;Cum_Split\u0026#39;].replace(np.nan, 1.0) # Calculate the non adjusted prices based on the splits only df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] = df_comp[\u0026#39;open\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] = df_comp[\u0026#39;high\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] = df_comp[\u0026#39;low\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] = df_comp[\u0026#39;close\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] # Calculate the adjusted prices based on the splits df_comp[\u0026#39;Open\u0026#39;] = df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;High\u0026#39;] = df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Low\u0026#39;] = df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Close\u0026#39;] = df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend\u0026#39;] = df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend_Pct_Orig\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] / df_comp[\u0026#39;close\u0026#39;] df_comp[\u0026#39;Dividend_Pct_Adj\u0026#39;] = df_comp[\u0026#39;Dividend\u0026#39;] / df_comp[\u0026#39;Close\u0026#39;] # Export data to excel file = fund + \u0026#34;_NDL.xlsx\u0026#34; df_comp.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df_comp[-1:]) print(f\u0026#34;NDL data updater complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Let\u0026rsquo;s break this down line by line.\nImports First, we need to import the required libraries:\n1 2 3 4 # Imports import nasdaqdatalink import pandas as pd import numpy as np NDL API Key To gain access to anything beyond the free tier, you will need to provide your access key:\n1 2 # Add API key for reference to allow access to unrestricted data nasdaqdatalink.ApiConfig.api_key = \u0026#39;your_key\u0026#39; Download data as a dataframe Moving on to the function definition, we have the command to pull data from NDL. There are two separate APIs - the time series and the tables. The syntax is different, and some data sets are only available as one or the other. We will use the tables API for this tutorial.\n1 2 3 # Command to pull data # If start date and end date are not specified the entire data set is included df = nasdaqdatalink.get_table(\u0026#39;QUOTEMEDIA/PRICES\u0026#39;, ticker = fund, paginate=True) In the example above, the fund is an input parameter to the function.\nThe 'QUOTEMEDIA/PRICES' is the data source that we are accessing.\nThere are many other arguments that we could pass in the above, including specifying columns, period start date, period end date, and others. Nasdaq as a few examples to get you started:\nhttps://docs.data.nasdaq.com/docs/python-tables\nRunning:\ndf.head(10) Gives us:\nSort columns by date Next, we will sort the columns by date ascending. By default, the dataframe is created with the data sorted by descending date, and we want to change that:\n1 2 # Sort columns by date ascending df.sort_values(\u0026#39;date\u0026#39;, ascending = True, inplace = True) The inplace = True argument specifies that the sort function should take effect on the existing dataframe.\nNow, running:\ndf.head(10) Gives us:\nSetting the date as the index Next, we will rename the date column from \u0026lsquo;date\u0026rsquo; to \u0026lsquo;Date\u0026rsquo;, and set the index to be the Date column:\n1 2 3 4 5 # Rename date column df.rename(columns = {\u0026#39;date\u0026#39;:\u0026#39;Date\u0026#39;}, inplace = True) # Set index to date column df.set_index(\u0026#39;Date\u0026#39;, inplace = True) Now, running:\ndf.head(10) Gives us:\nCalculating splits The next sections deal with the split column. So far we have only seen a split value of 1.0 in the data, but we\u0026rsquo;ve only looked at the first 10 and last 10 rows. Are there any other values? Let\u0026rsquo;s check by running:\n1 df_not_1_split = df[df[\u0026#39;split\u0026#39;] != 1.0] And checking the first 10 rows:\ndf_not_1_split.head(10) Gives us:\nSo we now know that the stock did in fact split several times. Next, we will replace all of the 1.0 split values - because they are really meaningless - and then create a new dataframe to deal with the splits.\n1 2 # Replace all split values of 1.0 with NaN df[\u0026#39;split\u0026#39;] = df[\u0026#39;split\u0026#39;].replace(1.0, np.nan) This gives us:\nWe will now create a dataframe with only the split values:\n1 2 3 4 # Create a new data frame with split values only df_splits = df.drop(columns = {\u0026#39;ticker\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;, \u0026#39;volume\u0026#39;, \u0026#39;dividend\u0026#39;, \u0026#39;adj_open\u0026#39;, \u0026#39;adj_high\u0026#39;, \u0026#39;adj_low\u0026#39;, \u0026#39;adj_close\u0026#39;, \u0026#39;adj_volume\u0026#39;}).dropna() Which gives us:\nCreating a column for the cumulative split will provide an accurate perspective on the stock price. We can do that with the following:\n1 2 # Create a new column for cumulative split df_splits[\u0026#39;Cum_Split\u0026#39;] = df_splits[\u0026#39;split\u0026#39;].cumprod() Which gives us:\nWe will then drop the original split column before combining the split data frame with the original data frame, as follows:\n1 2 # Drop original split column before combining dataframes df_splits.drop(columns = {\u0026#39;split\u0026#39;}, inplace = True) Which gives us:\nCombining dataframes Now we will merge the df_split dataframe with the original df dataframe so that the cumulative split column is part of the original dataframe. We will call this data frame df_comp:\n1 2 # Merge df and df_split dataframes df_comp = pd.merge(df, df_splits, on=\u0026#39;Date\u0026#39;, how=\u0026#39;outer\u0026#39;) We are using the merge function of pandas, which includes arguments for the names of both dataframes to be merged, the column to match between the dataframes, and the parameter for the type of merge to be performed. The outer argument specifies that all rows from both dataframes will be included, and any missing values will be filled in with NaN if there is no matching data. This ensures that all data from both dataframes is retained.\nRunning:\ndf_comp.head(10) Gives us:\nForward filling cumulative split values From here, we want to fill in the rest of the split and Cum_Split values. This is done using the forward fill function, which for all cells that have a value of NaN will fill in the previous valid value until another value is encountered. Here\u0026rsquo;s the code:\n1 2 # Forward fill for all cumulative split values df_comp[\u0026#39;Cum_Split\u0026#39;].fillna(method = \u0026#39;ffill\u0026#39;, inplace = True) Running:\ndf_comp.head(10) Gives us:\nAt first glance, it doesn\u0026rsquo;t look like anything changed. That\u0026rsquo;s because there wasn\u0026rsquo;t any ffill action taken on the initial values until pandas encountered a valid value to then forward fill. However, checking the last 10 rows:\ndf_comp.tail(10) Gives us:\nWhich is the result that we were expecting. But, what about the first rows from 12/12/1980 to 6/15/1987? We can fill those split and Cum_Split values with the following code:\n1 2 3 # Replace all split and cumulative split values of NaN with 1.0 to have complete split values df_comp[\u0026#39;split\u0026#39;] = df_comp[\u0026#39;split\u0026#39;].replace(np.nan, 1.0) df_comp[\u0026#39;Cum_Split\u0026#39;] = df_comp[\u0026#39;Cum_Split\u0026#39;].replace(np.nan, 1.0) Now, checking the first 10 rows:\ndf_comp.head(10) Gives us:\nWith this data, we now know for every day in the data set the following pieces of information:\nIf the stock split on that day What the total split ratio is up to and including that day Calculating adjusted and non-adjusted prices From here, we can complete our dataset by calculating the adjusted and non-adjusted prices using the cumulative split ratios from above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Calculate the non adjusted prices based on the splits only df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] = df_comp[\u0026#39;open\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] = df_comp[\u0026#39;high\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] = df_comp[\u0026#39;low\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] = df_comp[\u0026#39;close\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] * df_comp[\u0026#39;Cum_Split\u0026#39;] # Calculate the adjusted prices based on the splits df_comp[\u0026#39;Open\u0026#39;] = df_comp[\u0026#39;non_adj_open_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;High\u0026#39;] = df_comp[\u0026#39;non_adj_high_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Low\u0026#39;] = df_comp[\u0026#39;non_adj_low_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Close\u0026#39;] = df_comp[\u0026#39;non_adj_close_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend\u0026#39;] = df_comp[\u0026#39;non_adj_dividend_split_only\u0026#39;] / df_comp[\u0026#39;Cum_Split\u0026#39;][-1] df_comp[\u0026#39;Dividend_Pct_Orig\u0026#39;] = df_comp[\u0026#39;dividend\u0026#39;] / df_comp[\u0026#39;close\u0026#39;] df_comp[\u0026#39;Dividend_Pct_Adj\u0026#39;] = df_comp[\u0026#39;Dividend\u0026#39;] / df_comp[\u0026#39;Close\u0026#39;] Included above is the adjusted dividends values. For any time series analysis, not only are the adjusted prices needed, but so are the adusted dividends. Remember, we already have the adjusted total return prices - those come directly from NDL.\nExport data Next, we want to export the data to an excel file, for easy viewing and reference later:\n1 2 3 # Export data to excel file = fund + \u0026#34;_NDL.xlsx\u0026#34; df_comp.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) And verify the output is as expected:\nOutput confirmation Finally, we want to print a confirmation that the process succeeded along withe last date we have for data:\n1 2 3 4 5 # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df_comp[-1:]) print(f\u0026#34;NDL data updater complete for {fund} data\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) And confirming the output:\nReferences https://docs.data.nasdaq.com/docs https://docs.data.nasdaq.com/docs/tables-1 https://docs.data.nasdaq.com/docs/time-series https://docs.data.nasdaq.com/docs/python\n","date":"2023-12-24T00:00:01Z","permalink":"https://www.jaredszajkowski.com/p/nasdaq-data-link-tables-api-data-retrieval/","title":"Nasdaq Data Link Tables API Data Retrieval"},{"content":"Introduction As a follow up to this post about using yt-dlp with Zoom, I found the need to also download videos hosted by Panopto for offline viewing.\nSimilar to the Zoom tutorial, this also requires you to have a \u0026ldquo;cookies\u0026rdquo; text file, which needs to contain the cookies export in the Netscape HTTP format of the Panopto hosting site after logging in.\nHere\u0026rsquo;s the steps to setting this up:\nInstall cookie editor Install the cookie editor extension. I personnally use it with Microsoft Edge, but there are similar extensions for Chrome, Firefox, etc.\nModify export format Change the preferred cookie export format to Netscape HTTP Cookie File in the extension options. It is necessary to export in this format, otherwise yt-dlp will not be able to read the cookies.txt file correctly.\nLog in to Panopto Log in to Panopto in your browser. Be sure to remain logged in while exporting the cookies under step 4.\nExport cookies The export button is at the top fo the window. It copies the cookies to your clipboard, which then need to be pasted into a text file (I have my saved as cookies.txt), which yt-dlp will then read when it executes.\nCreate bash script Save the following code to a text file (my bash script file name is yt-dlp-panopto.sh):\n1 2 3 4 5 6 #!/bin/bash echo What is the link? read link yt-dlp --cookies /path/to/cookies/file/cookies.txt -o \u0026#34;%(title)s-%(id)s.%(ext)s\u0026#34; --write-subs $link Change permissions Modify the permissions of the bash script to allow execution:\n$ chmod +x yt-dlp-panopto.sh Execute the script Execute the bash script with ./yt-dlp-panopto.sh, copy and paste the link to the video that you would like to save, and it should download the video and the subtitles. If there are not any subtitles present in the stream, then it will notify you and only download the video stream.\nReferences References for yt-dlp (and used for above):\nhttps://ostechnix.com/yt-dlp-tutorial/\n","date":"2023-12-10T00:00:00Z","permalink":"https://www.jaredszajkowski.com/p/using-yt-dlp-with-panopto/","title":"Using yt-dlp With Panopto"},{"content":"Introduction In this tutorial, we will write a python function that imports an excel export from Bloomberg, removes ancillary rows and columns, and leaves the data in a format where it can then be used in time series analysis.\nExample of a Bloomberg excel export We will use the SPX index data in this example. Exporting the data from Bloomberg using the excel Bloomberg add-on yields data in the following format:\nData modifications The above format isn\u0026rsquo;t horrible, but we want to perform the following modifications:\nRemove the first six rows of the data Convert the 7th row to become column headings Rename column 2 to \u0026ldquo;Close\u0026rdquo; to represent the closing price Remove column 3, as we are not concerned about volume Export to excel and make the name of the excel worksheet \u0026ldquo;data\u0026rdquo; Assumptions The remainder of this tutorial assumes the following:\nYour excel file is named \u0026ldquo;SPX_Index.xlsx\u0026rdquo; The worksheet in the excel file is named \u0026ldquo;Worksheet\u0026rdquo; You have the pandas library installed You have the OpenPyXL library installed Python function to modify the data The following function will perform the modifications mentioned above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # This function takes an excel export from Bloomberg and # removes all excess data leaving date and close columns # Imports import pandas as pd # Function definition def bb_data_updater(fund): # File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) return print(f\u0026#34;--------------------\u0026#34;) Let\u0026rsquo;s break this down line by line.\nImports First, we need to import pandas:\n1 import pandas as pd Import excel data file Then import the excel file as a pandas dataframe:\n1 2 3 4 5 # File name variable file = fund + \u0026#34;_Index.xlsx\u0026#34; # Import data from file as a pandas dataframe df = pd.read_excel(file, sheet_name = \u0026#39;Worksheet\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) Running:\ndf.head(10) Gives us:\nSet column headings Next, set the column heading:\n1 2 # Set the column headings from row 5 (which is physically row 6) df.columns = df.iloc[5] Now, running:\ndf.head(10) Gives us:\nRemove index heading Next, remove the column heading from the index column:\n1 2 # Set the column heading for the index to be \u0026#34;None\u0026#34; df.rename_axis(None, axis=1, inplace = True) Note: The axis=1 argument here specifies the column index.\nNow, running:\ndf.head(10) Gives us:\nDrop rows Next, we want to remove the first 6 rows that have unneeded data:\n1 2 # Drop the first 6 rows, 0 - 5 df.drop(df.index[0:6], inplace=True) Note: When dropping rows, the range to drop begins with row 0 and continues up to - but not including - row 6.\nNow, running:\ndf.head(10) Gives us:\nSet index Next, we want to set the date column as the index:\n1 2 # Set the date column as the index df.set_index(\u0026#39;Date\u0026#39;, inplace = True) Now, running:\ndf.head(10) Gives us:\nDrop the \u0026ldquo;PX_VOLUME\u0026rdquo; column Next, we want to drop the volume column:\n1 2 3 4 5 # Drop the volume column try: df.drop(columns = {\u0026#39;PX_VOLUME\u0026#39;}, inplace = True) except KeyError: pass For some data records, the volume column does not exist. Therefore, we try, and if it fails with a KeyError, then we assume the \u0026ldquo;PX_VOLUME\u0026rdquo; column does not exist, and just pass to move on.\nNow, running:\ndf.head(10) Gives us:\nRename the \u0026ldquo;PX_LAST\u0026rdquo; column Next, we want to rename the \u0026ldquo;PX_LAST\u0026rdquo; column as \u0026ldquo;Close\u0026rdquo;:\n1 2 # Rename column df.rename(columns = {\u0026#39;PX_LAST\u0026#39;:\u0026#39;Close\u0026#39;}, inplace = True) Now, running:\ndf.head(10) Gives us:\nSort data Next, we want to sort the data starting with the oldest date:\n1 2 # Sort by date df.sort_values(by=[\u0026#39;Date\u0026#39;], inplace = True) Now, running:\ndf.head(10) Gives us:\nExport data Next, we want to export the data to an excel file, for easy viewing and reference later:\n1 2 3 # Export data to excel file = fund + \u0026#34;.xlsx\u0026#34; df.to_excel(file, sheet_name=\u0026#39;data\u0026#39;) And verify the output is as expected:\nOutput confirmation Finally, we want to print a confirmation that the process succeeded along withe last date we have for data:\n1 2 3 4 5 # Output confirmation print(f\u0026#34;The last date of data for {fund} is: \u0026#34;) print(df[-1:]) print(f\u0026#34;Bloomberg data conversion complete for {fund} data\u0026#34;) print(f\u0026#34;--------------------\u0026#34;) And confirming the output:\nReferences https://www.bloomberg.com/professional/support/software-updates/\n","date":"2023-11-15T00:00:01Z","permalink":"https://www.jaredszajkowski.com/p/cleaning-a-bloomberg-data-excel-export/","title":"Cleaning A Bloomberg Data Excel Export"},{"content":"Introduction Here are my notes for some of the more commonly used git commands along with initial setup for git in Linux.\nInstallation To begin, install as follows for Arch Linux:\n# pacman -Syu git Or\n$ yay git Pacman will include all required depencies.\nInitial configuration First, set your name and email address:\n$ git config --global user.name \u0026quot;Firstname Lastname\u0026quot; $ git config --global user.email \u0026quot;email@address.com\u0026quot; Then, set your preferred text editor (if you have one). I use nano:\n$ git config --global core.editor \u0026quot;nano\u0026quot; You can verify the updates with:\n$ git config --global core.editor Alternatively, you can edit the git configuration directly with:\n$ git config --global --edit Store credentials In 2021, GitHub disabled authentication via password and now requires authentication with a token. The following command sets up the credential helper, where it will store your token in ~/.git-credentials:\n$ git config --global credential.helper store After you log in during a git push with your username and token, the username or email address and token will be stored in the above location.\nNote: The token is stored in plain text, so use caution if that is a concern.\nCloning repositories Repositories can be cloned with the following:\n$ git clone https://github.com/\u0026lt;username\u0026gt;/\u0026lt;repository\u0026gt;.git Updating repositories The local record of a repository can be updated with the following command:\n$ cd \u0026lt;repository\u0026gt;/ $ git pull Adding, committing, and pushing Any files or directories that have been added, modified, or removed can be add to the list of changes to be pushed with the following command:\n$ git add . Then committed (staged in preparation to push) with the following command:\n$ git commit -am \u0026quot;Add your commit message here\u0026quot; Note: Without add, commit will handle any changes to files that have been modified or deleted, but will not incorporate any files that have been created.\nThen finally pushed:\n$ git push If, for some reason, you would like to reset a commit:\n$ git reset These commands can be chained together with the AND operator:\n$ git add . \u0026amp;\u0026amp; git commit -am \u0026quot;Add your commit message here\u0026quot; \u0026amp;\u0026amp; git push Stashing changes If you forget to update a repository before making changes, you can \u0026ldquo;stash\u0026rdquo; those changes and then re-apply them after running git pull.\nFirst, stash the changes:\n$ git stash Then, update the local record of the repository:\n$ git pull Finally, re-apply the changes you previously made:\n$ git stash apply This has proven to be very useful for me when I forget to update a repository before making edits to the code.\nReferences References for git (and used for above):\nhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup#_first_time https://git-scm.com/book/en/v2/Appendix-C%3A-Git-Commands-Setup-and-Config https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage#_credential_caching https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning https://www.geeksforgeeks.org/difference-between-chaining-operators-in-linux/\n","date":"2023-10-16T00:00:00Z","permalink":"https://www.jaredszajkowski.com/p/git-quick-start-guide/","title":"Git Quick Start Guide"},{"content":"Introduction If anyone uses Zoom to record and later wants to access recordings of video calls, here\u0026rsquo;s a simple linux bash script to download the video file and acompanying subtitles. For a long time I used zoomdl, but it is no longer under active development, and I began running into various issues about a year ago.\nThis tutorial requires you to have a \u0026ldquo;cookies\u0026rdquo; text file, which needs to contain the cookies export in the Netscape HTTP format of the Zoom cookies after logging in.\nHere\u0026rsquo;s the steps to setting this up:\nInstall cookie editor Install the cookie editor extension. I personnally use it with Microsoft Edge, but there are similar extensions for Chrome, Firefox, etc.\nModify export format Change the preferred cookie export format to Netscape HTTP Cookie File in the extension options. It is necessary to export in this format, otherwise yt-dlp will not be able to read the cookies.txt file correctly.\nLog in to Zoom Log in to Zoom in your browser. Be sure to remain logged in while exporting the cookies under step 4.\nExport cookies The export button is at the top fo the window. It copies the cookies to your clipboard, which then need to be pasted into a text file (I have my saved as cookies.txt), which yt-dlp will then read when it executes.\nCreate bash script Save the following code to a text file (my bash script file name is yt-dlp-zoom.sh):\n1 2 3 4 5 6 #!/bin/bash echo What is the link? read link yt-dlp --referer \u0026#34;https://zoom.us/\u0026#34; --cookies /path/to/cookies/file/cookies.txt -o \u0026#34;%(title)s-%(id)s.%(ext)s\u0026#34; --write-subs $link Change permissions Modify the permissions of the bash script to allow execution:\n$ chmod +x yt-dlp-zoom.sh Execute the script Execute the bash script with ./yt-dlp-zoom.sh, copy and paste the link to the video that you would like to save, and it should download the video and the subtitles.\nReferences References for yt-dlp (and used for above):\nhttps://ostechnix.com/yt-dlp-tutorial/\n","date":"2023-10-01T00:00:00Z","permalink":"https://www.jaredszajkowski.com/p/using-yt-dlp-with-zoom/","title":"Using yt-dlp With Zoom"},{"content":"Introduction This is the basic framework that I use to install Arch Linux, with a few changes catered to the Lenovo ThinkPad E15 Gen 2. I have found that this is a decent mid range laptop, excellent linux compatibility, great keyboard, and overall provides a good value.\nGetting started This tutorial assumes the following:\nYou are booting from a USB drive with the Arch install ISO. Wireless or wired network is detected and drivers are configured automatically. You want drive encrytion on your root partition, but not on your boot/efi/swap partitions. Verify UEFI boot mode The following command should show directory without error:\n# ls /sys/firmware/efi/efivars Configure wireless network The following command will drop you into the iwd daemon:\n# iwctl From there:\n# device list # station *device* scan # station *device* get-networks # station *device* connect *SSID* Verify internet connectivity # ping archlinux.org Update system clock # timedatectl set-ntp true # timedatectl status Disks, partition table \u0026amp; partitions The following assumes that your NVME drive is found as /dev/nvme0n1. Partitions will then be /dev/nvme0n1p1 and so on.\nWipe disk List disks:\n# fdisk -l Wipe all file system records:\n# wipefs -a /dev/nvme0n1 Create new partition table Open nvme0n1 with gdisk:\n# gdisk /dev/nvme0n1 Create GPT partition table with option \u0026ldquo;o\u0026rdquo;.\nCreate EFI partition Create new EFI partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #1 Default starting sector +550M Change partition type to EFI System (ef00) Create boot partition Create new boot partition w/ 550mb with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #2 Default starting sector +550M Leave default type of 8300 Create swap partition The old rule of thumb used to be that a swap partition should be the same size as the amount of memory in the system, but given the typical amount of memory in modern systems this is obviously no longer necessary. For my system with 16 or 32 GB of memory, a swap of 8 GB is rarely even used.\nCreate new Swap partition w/ 8GB with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #3 Default starting sector +8G Change to linux swap (8200) Create root partition Create new root partition w/ remaining disk space with option \u0026ldquo;n\u0026rdquo;, using the following parameters:\nPartition #4 Default starting sector Remaining space Linux LUKS type 8309 And then exit gdisk.\nWrite file systems EFI partition Write file system to new EFI System partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p1 # mkfs.fat -F32 /dev/nvme0n1p1 Boot partition Then boot partition:\n# cat /dev/zero \u0026gt; /dev/nvme0n1p2 # mkfs.ext2 /dev/nvme0n1p2 Root partition Prepare root partition w/ LUKS:\n# cryptsetup -y -v luksFormat --type luks2 /dev/nvme0n1p4 # cryptsetup luksDump /dev/nvme0n1p4 # cryptsetup open /dev/nvme0n1p4 archcryptroot # mkfs.ext4 /dev/mapper/archcryptroot # mount /dev/mapper/archcryptroot /mnt I use archcryptroot for the name of my encrypted volume, but change as necessary.\nSwap partition Then swap:\n# mkswap /dev/nvme0n1p3 # swapon /dev/nvme0n1p3 Create mount points # mkdir /mnt/boot # mount /dev/nvme0n1p2 /mnt/boot # mkdir /mnt/boot/efi # mount /dev/nvme0n1p1 /mnt/boot/efi System install Install base packages # pacstrap /mnt base base-devel linux linux-firmware grub-efi-x86_64 efibootmgr Generate fstab # genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Enter new system # arch-chroot /mnt /bin/bash Set clock # ln -sf /usr/share/zoneinfo/America/Chicago /etc/localtime # hwclock â€“systohc Generate locale In /etc/locale.gen uncomment only: en_US.UTF-8 UTF-8\n# locale-gen In /etc/locale.conf, you should only have this line: LANG=en_US.UTF-8\n# nano /etc/locale.conf Set hostname \u0026amp; update hosts # echo linuxmachine \u0026gt; /etc/hostname Update /etc/hosts with the following:\n127.0.0.1 localhost ::1 localhost 127.0.1.1 linuxmachine.localdomain linuxmachine Set root password # passwd Update /etc/mkinitcpio.conf \u0026amp; generate initrd image Edit /etc/mkinitcpio.conf with the following:\nHOOKS=(base udev autodetect modconf block keymap encrypt resume filesystems keyboard fsck) Then run:\n# mkinitcpio -p linux Install grub # grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=ArchLinux Edit /etc/default/grub so it includes a statement like this:\nGRUB_CMDLINE_LINUX=\u0026quot;cryptdevice=/dev/nvme0n1p4:archcryptroot resume=/dev/nvme0n1p3\u0026quot; Generate final grub configuration:\n# grub-mkconfig -o /boot/grub/grub.cfg Exit \u0026amp; reboot # exit # umount -R /mnt # swapoff -a # reboot To be continued.\n","date":"2023-09-29T00:00:01Z","permalink":"https://www.jaredszajkowski.com/p/arch-linux-install/","title":"Arch Linux Install"},{"content":"Hello World Welcome to my website. This is meant to serve as a place for me to publish various posts from my explorations into Arch Linux, data science, quant finance, and other topics.\nThe theme has been adopted from the Hugo Theme Stack produced by Jimmy Cai.\nThis is the only theme that I have found that checks all of the following boxes:\nTheme for the static site generator Hugo Includes modules for archives Includes tags and topics/categories Includes built-in search functionality Simple interface that is easily navigable Highly extensible including modules for image galleries, posts, comment capabilities, etc. It is hosted on GitHub pages. I followed the install instructions that the theme author provided, including using GitHub codespace for editing in the cloud. There are only a few details that I ran into that he did not mention.\nDon\u0026rsquo;t forget to run Hugo to build the site. This creates the public directory, which is where the static site files are located. Make sure to update the branch to be gh-pages under Settings -\u0026gt; Pages -\u0026gt; Build and deployment -\u0026gt; Branch in GitHub. Make sure to remove the public directory from the .gitignore file. Otherwise GitHub will ignore the public directory and your site will show the README.md instead of the Hugo site. The site can be updated either through codespace, or locally as long as Hugo and it\u0026rsquo;s required dependencies have been installed.\nUpdating and pushing changes The simple command after making any changes and to push those updates is as follows:\n$ hugo \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -am \u0026quot;Updating site\u0026quot; \u0026amp;\u0026amp; git push This can be put in a bash script to make it easier. Save the following as git-update.sh:\n1 2 3 4 5 6 #!/bin/bash echo What is the commit message? read message hugo \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -am \u0026#34;$message\u0026#34; \u0026amp;\u0026amp; git push Change permissions:\n$ chmod +x git-update.sh And then execute:\n$ ./git-update.sh References Here\u0026rsquo;s the full list of resources I referenced for deploying Hugo with GitHub pages:\nhttps://www.o11ycloud.com/posts/gh_hugo/ https://github.com/CaiJimmy/hugo-theme-stack https://medium.com/@magstherdev/github-pages-hugo-86ae6bcbadd\n","date":"2023-09-26T00:00:00Z","permalink":"https://www.jaredszajkowski.com/p/hello-world/","title":"Hello World"}]